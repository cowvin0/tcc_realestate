---
title: "Uso de Aprendizado de M√°quina na Modelagem Preditiva dos Valores de Im√≥veis na Cidade de Jo√£o Pessoa"
crossref:
  custom:
    - kind: float
      key: algo
      reference-prefix: "Algoritmo"
      caption-prefix: "Algoritmo"
      latex-env: algo
      latex-list-of-description: Algoritmo
institute: "Universidade Federal da Para√≠ba"
author: "Aluno: Gabriel de Jesus Pereira <br> Orientador: Pedro Rafael Diniz Marinho"
bibliography: ../includes/bib.bib
embed-resources: true
format:
  revealjs:
    #theme: [solarized, custom.scss]
    theme: solarized
    width: 1920
    height: 1280
    logo: ../includes/logode.png
    footer: 'Departamento de Estat√≠stica - UFPB'
    transition: slide
    background-transition: fade
    preview-links: auto
    slide-number: true
    scrollable: true
    controls: true
    code-tools: true
    auto-stretch: true
    code-link: true
execute:
  refresh: true
  warning: false
  error: false
  eval: true
  echo: false
editor:
  markdown:
    wrap: 72
lang: pt
---

# Introdu√ß√£o

## Formula√ß√£o do mercado imobili√°rio brasileiro

- Urbaniza√ß√£o no Brasil: de 31% em 1940 para 85,1% em 2014

- √äxodo rural, industrializa√ß√£o e crescimento desordenado das metr√≥poles

  + In√≠cio da industrializa√ß√£o no Governo de Get√∫lio Vargas (d√©cada de 1930)
  + Governo de Juscelino Kubitschek implementou o Plano de Metas (d√©cada de 1950)
    + Incluia investimentos estatais em agricultura, sa√∫de, educa√ß√£o, energia, transporte, minera√ß√£o e constru√ß√£o civil.

- Cria√ß√£o do Sistema Financeiro da Habita√ß√£o (SFH) ‚Äì Lei n¬∫ 4.380/1964

  + Implementou a corre√ß√£o monet√°ria
  + Cria√ß√£o do Banco Nacional da Habita√ß√£o (BNH)
    + Visava incentivar o mercado imobili√°rio, atrair o setor privado e regulamentar o financiamento do SFH, incluindo garantias, prazos e taxas [@assumpccao2011credito]
  + Cria√ß√£o das Sociedades de Cr√©dito Imobili√°rio (SCI)
    + Subordinadas ao BNH, atuavam como agentes financeiros exclusivos no financiamento de constru√ß√£o, venda ou aquisi√ß√£o de bens destinados a habita√ß√£o
  + O FGTS foi criado nesse per√≠odo e, junto com a caderneta de poupan√ßa, tornou-se a principal fonte de financiamento habitacional no Brasil

## Crises econ√¥micas e impacto no setor habitacional

- A partir de 1980, o SFH passou a ser impactado pelo aumento da infla√ß√£o e pelas medidas adotadas pelos governos para cont√™-la

- Em 1985, as presta√ß√µes foram reajustadas em 112%, enquanto os saldos devedores subiram 246% com a infla√ß√£o acumulada

- O Plano Cruzado (1986) converteu o valor das presta√ß√µes com base na m√©dia dos 12 meses anteriores e congelou os reajustes pelos 12 meses seguintes
  + A medida atingiu todos os contratos, reduzindo as presta√ß√µes em cerca de 40%, mas, a longo prazo, os deixou mais caros

- Em 1987 e 1989, as presta√ß√µes foram congeladas temporariamente pelo Plano Bresser e pelo Plano Ver√£o, respectivamente

- O Plano Collor I (1990), foi o que mais prejudicou o SFH
  + Bloqueou todos os ativos financeiros e 60% do saldo das cadernetas de poupan√ßa. Dos 40% restantes, cerca da metade foi sacada pelos depositantes, reduzindo o saldo das cadernetas de US$ 30 bilh√µes para aproximadamente entre US$ 7 e US$ 8 bilh√µes.

- A crise econ√¥mica entre as d√©cadas de 1980 e 1990 elevou a inadimpl√™ncia no SFH de 9% em 1994 para 30% em 2005.

## Moderniza√ß√£o do sistema habitacional

- A partir da experi√™ncia com o SFH, o Sistema de Financiamento Imobili√°rio (SFI) foi criado em 1997, pela Lei n¬∫ 9.514, para modernizar o modelo de financiamento habitacional no Brasil

\vfill

- O SFI capta recursos diretamente do mercado por meio de opera√ß√µes realizadas por institui√ß√µes financeiras autorizadas, como bancos, sociedades de cr√©dito e companhias hipotec√°rias.
  + As entidades autorizadas podem aplicar os recursos por meio de novos instrumentos introduzidos pelo SFI, como o Certificado de Receb√≠veis Imobili√°rios (CRI), a Letra de Cr√©dito Imobili√°rio (LCI) e a C√©dula de Cr√©dito Imobili√°rio (CCI).

\vfill

- A seguran√ßa dos contratos passou a ser garantida pela introdu√ß√£o da aliena√ß√£o fidunci√°ria

\vfill

- Com a securitiza√ß√£o dos cr√©ditos imobili√°rios e o fortalecimento da seguran√ßa jur√≠dica dos contratos como principais fundamentos, o SFI representou a moderniza√ß√£o efetiva do mercado imobili√°rio brasileiro.

## Possibilidades e desafios

- Possibilidades
  + Valoriza√ß√£o do mercado imobili√°rio de Jo√£o Pessoa
    + Destaque nacional em 2024, com valoriza√ß√£o acumulada de 16,13% at√© novembro
      + Atratividade da cidade: qualidade de vida, turismo, investimentos urbanos
  + Aplica√ß√£o de modelos preditivos
    + Apoio a decis√µes no mercado: compradores, vendedores, investidores, setor p√∫blico
    + Utiliza√ß√£o para estimativas tribut√°rias, como ITBI
    + Potencial uso por imobili√°rias e plataformas online para precifica√ß√£o autom√°tica

  + Visualiza√ß√µes interativas e mapas
    + Compreens√£o geogr√°fica das din√¢micas de pre√ßo por bairro

- Desafios
  + Escassez e descentraliza√ß√£o dos dados
    + Falta de bases p√∫blicas atualizadas e padronizadas sobre im√≥veis
    + Necessidade de utilizar web scraping para coleta em sites especializados

  + Dados com informa√ß√µes incompletas ou inconsistentes
    + Aus√™ncia de vari√°veis relevantes: idade do im√≥vel, padr√£o construtivo, estado de conserva√ß√£o
    + Varia√ß√µes na estrutura dos sites ao longo do tempo

## Objetivo

- **Objetivo Geral**
  + Realizar a an√°lise e modelagem dos valores dos im√≥veis em Jo√£o Pessoa, utilizando t√©cnicas de aprendizado de m√°quina para compreender os fatores que mais influenciam essas estimativas

- **Objetivos Espec√≠ficos**
  + Desenvolver modelos preditivos de valor de im√≥veis com base em dados coletados via web scraping
  + Construir uma aplica√ß√£o interativa para estimar pre√ßos com base nas caracter√≠sticas informadas pelo usu√°rio
  + Criar visualiza√ß√µes que facilitem a interpreta√ß√£o dos resultados e auxiliem na tomada de decis√µes
  + Avaliar o impacto e a import√¢ncia das vari√°veis por meio de t√©cnicas interpret√°veis (SHAP, ICE, PDP)
  + Comparar o desempenho de diferentes algoritmos de aprendizado de m√°quina aplicados ao problema

# Algoritmos de Aprendizado de M√°quina

## √Årvores de decis√£o

<br>

::: columns
::: {.column width="50%"}

- As √°rvores de decis√£o podem ser utilizadas tanto para regress√£o quanto para classifica√ß√£o

- Uma das grandes vantagens √© a interpretabilidade do modelo, em que √© poss√≠vel visualizar claramente as regras de decis√£o geradas

- O processo de constru√ß√£o das √°rvores se baseia no particionamento recursivo do espa√ßo dos preditores.
 + Cada particionamento √© chamado de n√≥ e o resultado final √© chamado de folha ou n√≥ terminal.

- O espa√ßo dos preditores √© dividido em $J$ regi√µes distintas e disjuntas denotadas por $R_{1}, R_{2}, \cdots, R_{J}$.
  + As regi√µes t√™m formato de caixa.

:::

::: {.column width="50%"}
![Exemplo de estrutura de √°rvore de decis√£o. A √°rvore tem cinco folhas e quatro n√≥s internos.](../../../../Documents/mermaid-tree.png){width="70%"}
:::
:::

- A vari√°vel resposta √© modelada como sendo uma constante $c_j$ em cada regi√£o $R_j$:

::: {layout-ncol=2}
$$
f\left(x\right) = \sum^{J}_{j=1}c_jI\left(x \in R_j \right),
$$

$$
I_{R_j}(x) =
\begin{cases}
    1,& \text{se } x \in R_j \\
    0,& \text{se } x \notin R_j
\end{cases}\text{.}
$$
:::

## √Årvores de decis√£o

O estimador para a constante $c_j$ √© encontrado pelo m√©todo de m√≠nimos quadrados:

$$
\sum_{x_i \in R_{j}} \left[y_i - f\left(x_i\right)\right]^2 \text{.}
$${#eq-minimo_q}

Como $f\left(x_i\right)$ est√° sendo avaliado em um ponto espec√≠fico $x_i$ e as regi√µes s√£o disjuntas, tem-se que $f\left(x_i\right) = c_j$. Dessa forma, a @eq-minimo_q se reduz a:

$$
\sum_{x_i \in R_j}\left[y_i - c_j\right]^2
$$

Derivando em rela√ß√£o a $c_j$ e igualando a zero:

$$
\frac{\partial}{\partial c_j} \sum_{x_i \in R_j}\left(y_i - c_j\right)^2 = -2 \sum_{x_i \in R_j}\left(y_i - c_j\right) = 0
$$

Assim, chega-se ao estimador para $c_j$:

$$
\hat{c}_j = \frac{1}{N_j}\sum_{x_i \in R_j} y_i
$$

Ou seja, a predi√ß√£o ser√° a m√©dia das observa√ß√µes da resposta que est√£o contidas em $R_j$.

## √Årvores de decis√£o

- Considerar todas as poss√≠veis parti√ß√µes do espa√ßo dos preditores √© invi√°vel devido ao alto custo computacional.

- Dessa forma, utiliza-se a abordagem de divis√£o bin√°ria recursiva.

- O processo inicia com a escolha de uma vari√°vel independente $X_j$ e um ponto de corte $s$ que proporcione a maior redu√ß√£o poss√≠vel na soma dos quadrados dos res√≠duos. Isso define dois subconjuntos:

$$
R_{1}\left(j, s\right) = \{X|X_j \leq s\} \text{ e } R_{2}\left(j, s\right) = \{X|X_j > s\} \text{, }
$$

- O objetivo √© minimizar:

$$
\min_{j, s}\left[\min_{c_1} \sum_{x_i \in R_1\left(j, s\right)} \left(y_i - c_{1}\right)^2 + \min_{c_2} \sum_{x_i \in R_2\left(j, s\right)} \left(y_i - c_{2}\right)^2\right]\text{,}
$$
em que $c_1$ e $c_2$ s√£o as m√©dias das respostas nas regi√µes $R_1(j, s)$ e $R_2(j, s)$, respectivamente.

- Ap√≥s encontrar a melhor divis√£o, os dados s√£o particionados nas duas regi√µes e o processo √© repetido de forma recursiva para todas as sub-regi√µes.

## √Årvores de decis√£o

- O tamanho da √°rvore atua como um controle da complexidade do modelo:
  + √Årvores muito grandes tendem ao sobreajuste, com bom desempenho no treino, mas fraca generaliza√ß√£o.
  + √Årvores muito pequenas podem levar ao subajuste, por n√£o capturarem padr√µes relevantes nos pr√≥prios dados de treino.

- Para lidar com isso, adota-se a estrat√©gia de crescer inicialmente uma √°rvore grande $T_0$, interrompendo as divis√µes apenas quando um n√∫mero m√≠nimo de observa√ß√µes por n√≥ for alcan√ßado.

- Em seguida, realiza-se a poda da √°rvore com base no crit√©rio de custo-complexidade.

- Para o processo de poda, define-se uma √°rvore qualquer $T \subset T_{0}$ obtida a partir da poda de $T_{0}$, isto √©, colapsando o seus n√≥s internos. Assim, define-se o crit√©rio de custo-complexidade:

$$
C_{\alpha}\left(T\right) = \sum^{|T|}_{j=1}N_jQ_j\left(T\right) + \alpha|T|
$$
em que $|T|$ √© o n√∫mero de folhas da √°rvore, $Q\left(T\right)$ √© a impureza do n√≥ terminal $j$, $\alpha$ √© o par√¢metro que equilibra o tamanho da √°rvore e a adequa√ß√£o aos dados.


- O objetivo √© encontrar, para cada valor de $\alpha$, a sub√°rvore $T_\alpha \subset T_0$ que minimiza $C_{\alpha}(T)$.
  + Quando $\alpha = 0$, resulta na pr√≥pria √°rvore $T_{0}$ e valores grandes de $\alpha$ resultam em √°rvores menores

## √Årvores de decis√£o

- A busca pela sub√°rvore $T_{\alpha}$ √© feita por meio do colapso sucessivo dos n√≥s internos que causam o menor aumento em $\sum_{j} N_j Q_j\left(T\right)$, at√© restar uma √°rvore com um √∫nico n√≥.

- Esse processo gera uma sequ√™ncia de sub√°rvores, dentre as quais existe, para cada valor de $\alpha$, uma √∫nica sub√°rvore que minimiza $C_{\alpha}\left(T\right)$.

- A estima√ß√£o de $\alpha$ pode ser feita por valida√ß√£o cruzada, resultando na √°rvore final $T_{\hat{\alpha}}$.

- No caso da regress√£o, $Q_j\left(T\right)$ pode representar o erro quadr√°tico m√©dio; j√° na classifica√ß√£o, utilizam-se m√©tricas como o √≠ndice de Gini ou a entropia cruzada.

- Para classifica√ß√£o, a predi√ß√£o no n√≥ terminal $j$ corresponde √† classe majorit√°ria, cuja propor√ß√£o √© dada por:

$$
\hat p_{jk} = \frac{1}{N_j} \sum_{x_i \in R_j}I\left(y_i = k\right)\text{, }
$$

- Assim, a classe atribu√≠da ao n√≥ terminal $j$ ser√°:

$$
k\left(j\right) = \arg \max_{k} \hat{p}_{jk}
$$

## M√©todos Ensemble

<br>

- Os m√©todos ensemble consistem em combinar m√∫ltiplos estimadores base para construir um modelo preditivo mais robusto e preciso.

\vfill

- Segundo @hastie2009elements, o aprendizado em conjunto envolve duas etapas principais:
  + Desenvolver uma popula√ß√£o de estimadores base a partir dos dados de treinamento.
  + Combinar esses estimadores para formar um √∫nico modelo preditivo.

\vfill

- Essa abordagem √© especialmente √∫til para melhorar o desempenho de modelos simples, como √°rvores de decis√£o, ao combin√°-las em conjunto.

\vfill

- Entre os principais m√©todos ensemble, destacam-se: Bagging, Random Forest, Boosting, Gradient Boosting e Stacking.

## Bagging

- O algoritmo Bagging (Bootstrap Aggregating) foi introduzido por @breiman1996bagging.

- Sua ideia central √© gerar um estimador agregado a partir de m√∫ltiplas vers√µes de um preditor, treinadas com amostras bootstrap do conjunto de treinamento.

- O principal objetivo do Bagging √© reduzir a vari√¢ncia de modelos, como as √°rvores de decis√£o, que tendem a variar muito quando treinadas isoladamente.

- Ao combinar diversas √°rvores treinadas em subconjuntos distintos, o Bagging melhora a estabilidade e o desempenho preditivo do modelo final.

- No caso de regress√£o, a predi√ß√£o do Bagging √© obtida pela m√©dia das predi√ß√µes individuais de cada √°rvore.

- Formalmente, seja $\mathcal{L}$ o conjunto de treinamento. A partir dele, geram-se $B$ amostras bootstrap $\mathcal{L}^{(b)}$, cada uma usada para treinar uma √°rvore de regress√£o $f(x, \mathcal{L}^{(b)})$.

- A predi√ß√£o final agregada √© dada por:

$$
f_{B}\left(x\right) = \frac{1}{B} \sum_{b = 1}^B f \left(x, \mathcal{L}^{\left(B\right)}\right)\text{,}
$$

onde $f_B(x)$ representa a predi√ß√£o m√©dia das $B$ √°rvores.

## Bagging

- Embora o Bagging reduza a vari√¢ncia e melhore o desempenho preditivo de √°rvores de regress√£o, isso ocorre √† custa de menor interpretabilidade.

- Uma alternativa √© estimar a import√¢ncia dos preditores com base na redu√ß√£o m√©dia do erro (ex.: erro quadr√°tico m√©dio) provocada por cada vari√°vel ao longo das $B$ √°rvores. Um valor elevado na redu√ß√£o total m√©dia do erro quadr√°tico m√©dio, calculado com base nas divis√µes realizadas por um determinado preditor em todas as ùêµ √°rvores, indica que o preditor √© importante.

- No entanto, as √°rvores geradas pelo Bagging tendem a ser muito semelhantes, o que limita seus ganhos:
  + Se forem independentes e identicamente distribu√≠das (i.i.d.), a vari√¢ncia da m√©dia das predi√ß√µes √© $\frac{1}{B}\sigma^2$.
  + Se houver correla√ß√£o positiva $\rho$ entre as √°rvores, a vari√¢ncia se torna:
$$
\rho\sigma^2 + \frac{1 - \rho}{B}\sigma^2
$$
  + Mesmo com $B \to \infty$, o termo $\rho \sigma^2$ permanece, limitando a redu√ß√£o de vari√¢ncia.

- Essa correla√ß√£o ocorre porque as mesmas vari√°veis tendem a ser escolhidas repetidamente nas divis√µes, por causarem maior redu√ß√£o no erro.

## Random Forest

<br>

- O algoritmo Random Forest √© uma extens√£o do Bagging aplicado a √°rvores de decis√£o, com uma modifica√ß√£o: reduzir a correla√ß√£o entre as √°rvores.

\vfill

- Isso √© feito por meio da sele√ß√£o aleat√≥ria de $m$ preditores, dentre os $p$ dispon√≠veis, como candidatos para cada divis√£o.
    + Quando $m = p$, o algoritmo se comporta como o Bagging.

\vfill

- Essa aleatoriedade procura resolver o problema do Bagging, que tende a gerar √°rvores muito semelhantes.

\vfill

- Em m√©dia, uma fra√ß√£o $1 - m/p$ das divis√µes sequer incluir√° o preditor mais forte como candidato, o que aumenta a chance de sele√ß√£o de outros preditores.

\vfill

- Assim como o Bagging, o Random Forest n√£o sofre de sobreajuste com o aumento do n√∫mero de √°rvores $B$, bastando escolher um valor suficientemente grande para estabilizar o erro.

## Boosting

- No Boosting, n√£o √© necess√°rio utilizar amostras bootstrap.

- Assim como outros m√©todos ensemble, o Boosting pode ser aplicado a diferentes modelos, mas √© mais conhecido pelo uso com √°rvores de regress√£o.

- Diferente do Bagging, que constr√≥i as √°rvores de forma paralela, o Boosting as constr√≥i sequencialmente.

- O objetivo √© incorporar informa√ß√µes e corrigir os erros cometidos pelas √°rvores anteriores.

- O processo come√ßa ajustando uma √°rvore aos valores observados da vari√°vel dependente; em seguida:
  + Calculam-se os res√≠duos da √°rvore anterior.
  + A nova √°rvore √© ajustada para predizer esses res√≠duos.
  + A predi√ß√£o √© ent√£o adicionada ao estimador atual para atualizar os res√≠duos.

- Como cada √°rvore depende das anteriores, √°rvores menores s√£o suficientes [@james2013introduction].

-  O aprendizado no Boosting √© mais lento, mas tende a gerar modelos mais precisos.
  + O processo √© controlado por um hiperpar√¢metro $\lambda$, chamado taxa de aprendizado.
    + Valores pequenos de $\lambda$ exigem um n√∫mero maior de √°rvores $B$, mas melhoram a generaliza√ß√£o.
    + Diferente do Bagging e Random Forest, o Boosting pode sofrer sobreajuste se $B$ for excessivamente grande.

- A profundidade das √°rvores, controlada pelo n√∫mero de divis√µes $d$, tamb√©m pode ser considerado um hiperpar√¢metro para ajuste:
  + Para $d = 1$, ajusta-se um modelo aditivo, pois cada √°rvore cont√©m apenas um n√≥.

## Gradient Boosting

## Stacked generalization

- O Stacked Generalization (Stacking) √© um m√©todo ensemble que combina as predi√ß√µes de v√°rios modelos para treinar um novo estimador, com o objetivo de melhorar a precis√£o das predi√ß√µes.

- A ideia central √© atribuir pesos √†s predi√ß√µes dos modelos, dando mais import√¢ncia √†queles com melhor desempenho, enquanto se evita atribuir altos pesos a modelos excessivamente complexos.

- Matematicamente, o Stacking ajusta o modelo $m = 1, \cdots, M$ ao conjunto de treinamento com a $i$-√©sima observa√ß√£o removida, definindo predi√ß√µes $\hat f_{m}^{-i}\left(x\right)$.

- Os pesos s√£o estimados por m√≠nimos quadrados, resolvendo:

$$
\hat{w}^{st} = \arg \min_{w} \sum^{N}_{i = 1} \left[y_i - \sum^{M}_{m = 1} w_m \hat{f}^{-i}_m\left(x_i\right)\right]^2\text{.}
$$

- A predi√ß√£o final √© dada por:

$$
\sum_{m}\hat{w}^{\text{st}}_{m}\hat{f}_{m}\left(x\right)
$$

- O uso da  valida√ß√£o cruzada leave-one-out na constru√ß√£o das predi√ß√µes $\hat{f}^{-i}_{m}\left(x\right)$ reduz o risco de sobreajuste, impedindo que modelos muito complexos tenham pesos muito altos.

# Metodologia

## Obten√ß√£o dos dados

::: columns
::: {.column width="50%"}
- A obten√ß√£o de dados para o mercado imobili√°rio representa um desafio, devido √† escassez de bases p√∫blicas organizadas.

- Como alternativa, optou-se pela extra√ß√£o direta de informa√ß√µes de sites especializados utilizando a t√©cnica conhecida como web scraping, com a coleta realizada em dois momentos distintos.

- Em particular, as informa√ß√µes foram coletadas do Zap Im√≥veis.
:::

::: {.column width="50%"}
![](Zap_im%C3%B3veis_2021.svg.png){width="80%"}
:::
:::

- O Zap Im√≥veis √© uma das principais plataformas online de compra, venda e aluguel de im√≥veis no Brasil. Fundado em mar√ßo de 2000, inicialmente recebeu o nome de Planeta Im√≥vel.

- Possui uma base extensa de an√∫ncios em diferentes cidades, incluindo Jo√£o Pessoa, abrangendo im√≥veis novos e usados, residenciais e comerciais.

- Sua ampla cobertura e variedade de informa√ß√µes o tornam uma fonte relevante para estudos de mercado e an√°lise imobili√°ria.

- Entre as informa√ß√µes dispon√≠veis para coleta, destacam-se o pre√ßo do im√≥vel, a √°rea, o n√∫mero de quartos, o n√∫mero de vagas de garagem, o endere√ßo, o tipo de im√≥vel, o valor do condom√≠nio e caracter√≠sticas adicionais, como piscina, academia, spa, entre outras.

## Web scraping

- O web scraping √© uma t√©cnica utilizada para extrair informa√ß√µes de sites da internet, armazenando-as em arquivos ou sistemas de banco de dados para fins de an√°lise, desenvolvimento de aplica√ß√µes ou acesso a informa√ß√µes de dif√≠cil obten√ß√£o.

- A coleta dos dados √© realizada por meio do Hypertext Transfer Protocol (HTTP),
  + O HTTP √© o protocolo que gerencia a comunica√ß√£o cliente-servidor na internet, baseado em requisi√ß√µes que especificam a√ß√µes sobre recursos definidos.

- Os m√©todos mais utilizados em web scraping s√£o o GET e o POST:
  + O m√©todo GET requisita uma representa√ß√£o do recurso especificado, sendo utilizado principalmente para visualizar dados.
  + O m√©todo POST √© usado para enviar dados ao servidor, geralmente para serem processados, como no envio de informa√ß√µes via formul√°rios HTML (ex.: login ou cadastro).

- O processo de web scraping normalmente se inicia com uma requisi√ß√£o GET para obter o conte√∫do de uma p√°gina web.
  + A resposta, geralmente em HTML, √© ent√£o analisada para extrair os dados de interesse.

## Principais desafios do web scraping

- Mudan√ßas na estrutura dos sites:
  + Pequenas altera√ß√µes no c√≥digo HTML ou na organiza√ß√£o das p√°ginas podem quebrar os scripts de extra√ß√£o, exigindo manuten√ß√µes frequentes.

- Conte√∫do din√¢mico:
  + Muitos sites modernos utilizam **JavaScript** para carregar informa√ß√µes de forma ass√≠ncrona, o que dificulta a captura dos dados apenas com requisi√ß√µes HTTP b√°sicas, tornando necess√°ria a utiliza√ß√£o de ferramentas como (Selenium ou Playwright).

- Limita√ß√µes de acesso e bloqueios:
  + Sites podem impor restri√ß√µes, como o bloqueio de IPs, o que exige a utiliza√ß√£o de t√©cnicas de rotacionamento de IP para contornar essas limita√ß√µes.

- Aspectos √©ticos e legais:
  + Nem todos os sites autorizam o scraping de seus dados. √â fundamental respeitar pol√≠ticas de uso, robots.txt, e considerar as implica√ß√µes legais, especialmente em rela√ß√£o a direitos autorais e prote√ß√£o de dados.

## Ferramentas utilizadas para web scraping em R

- `rvest`
  +

- `xml2`
  +

- `httr`
  +

## Ferramentas utilizadas para web scraping em Python

- `Scrapy`
  +

- `Playwright`
  +

## Dados obtidos a partir do web scraping

- Entre os dados obtidos, destacam-se:
  + Pre√ßo de venda: valor anunciado para o im√≥vel.
  + √Årea privativa: metragem em metros quadrados.
  + N√∫mero de quartos, su√≠tes e banheiros: caracter√≠sticas estruturais importantes para avalia√ß√£o.
  + N√∫mero de vagas de garagem: relevante para a valoriza√ß√£o do im√≥vel.
  + Endere√ßo e bairro: localiza√ß√µes aproximadas, utilizadas posteriormente para infer√™ncia geogr√°fica.
  + Tipo de im√≥vel: apartamento, casa, cobertura, entre outros.
  + Valor do condom√≠nio: custo adicional mensal associado ao im√≥vel, quando dispon√≠vel.
  + Caracter√≠sticas adicionais: presen√ßa de piscina, academia, spa, varanda, entre outros itens que podem influenciar o pre√ßo.
  + Descri√ß√£o textual: informa√ß√µes complementares fornecidas nos an√∫ncios.
  + Data de coleta: para registro do momento da extra√ß√£o, importante para controle temporal dos dados.

## Dados utilizados para cria√ß√£o de aplica√ß√£o web

::: columns
::: {.column width="60%"}
- O Filipeia - Mapas da Cidade √© uma plataforma online que disponibiliza dados geogr√°ficos e cartogr√°ficos sobre a cidade de Jo√£o Pessoa.

- Criado com o objetivo de apoiar pesquisadores, profissionais e a popula√ß√£o em geral, o site re√∫ne mapas atualizados dos bairros, ruas, equipamentos p√∫blicos e outras divis√µes territoriais do munic√≠pio.

- Entre os dados dispon√≠veis na plataforma e que foram utilizados no projeto, destacam-se: ciclovias, faixas exclusivas de √¥nibus, corredores de transporte p√∫blico, √°reas rurais, limites de bairros, al√©m da localiza√ß√£o de parques, pra√ßas e rios.
:::

::: {.column width="40%"}
![](filipeiaLogo.png)
:::
:::

::: columns
::: {.column width="40%"}
![](inep.jpg)
:::

::: {.column width="60%"}
- As informa√ß√µes sobre escolas p√∫blicas foram obtidas junto ao Instituto Nacional de Estudos e Pesquisas Educacionais An√≠sio Teixeira (INEP).

- A partir dos dados do INEP, foi poss√≠vel incorporar √† aplica√ß√£o a localiza√ß√£o das escolas p√∫blicas de Jo√£o Pessoa, enriquecendo a an√°lise espacial e oferecendo mais uma camada de informa√ß√£o relevante para o projeto.
:::
:::

# Constru√ß√£o do modelo

## Divis√£o entre Treinamento e Teste e Escolha das Vari√°veis Preditivas

- Para avaliar o desempenho dos modelos, o conjunto de dados foi dividido em conjuntos de treinamento e teste da seguinte forma:
  + A fim de manter a propor√ß√£o dos intervalos de pre√ßos dos im√≥veis em ambos os conjuntos, a divis√£o foi realizada por meio da estratifica√ß√£o da vari√°vel dependente em cinco faixas: primeiro intervalo: de R$ 43.914 at√© R$ 200.000; segundo intervalo: de R$ 200.000 at√© R$ 400.000; terceiro intervalo: de R$ 400.000 at√© R$ 600.000; quarto intervalo: de R$ 600.000 at√© R$ 800.000; quinto intervalo: acima de R$ 800.000 at√© o m√°ximo de R$ 7.000.000.
  + Dessa forma, 20% dos dados foram reservados para o conjunto de teste e 80% para o treinamento dos modelos.
- Para o ajuste dos modelos, foram selecionadas vari√°veis consideradas relevantes para o estudo:
  + Pre√ßo e √°rea m√©dia do aluguel no bairro;
  + √Årea do im√≥vel, medida em $m^2$;
  + N√∫mero de quartos, banheiros e vagas de garagem;
  + Tipo de im√≥vel, podendo ser: apartamento, casa, casa de condom√≠nio, flat, terreno comercial ou terreno de condom√≠nio;
  + Caracter√≠sticas adicionais do im√≥vel: √°rea de servi√ßo, academia, elevador, espa√ßo gourmet, piscina, playground, portaria 24 horas, quadra de esportes, sal√£o de festas, sauna, spa e varanda gourmet.

## Divis√£o entre Treinamento e Teste e Escolha das Vari√°veis Preditivas

- Al√©m das vari√°veis coletadas por meio de web scraping, foram criadas novas vari√°veis derivadas de combina√ß√µes entre vari√°veis que apresentavam correla√ß√£o relevante:
  + Quantidade total de c√¥modos do im√≥vel;
  + Produto entre as coordenadas geogr√°ficas (latitude e longitude) e o pre√ßo m√©dio do aluguel no bairro;
  + Raz√£o entre o n√∫mero de quartos e a √°rea do im√≥vel;
  + Produto entre o n√∫mero de quartos e a √°rea m√©dia do aluguel no bairro.
- A an√°lise de correla√ß√£o entre as vari√°veis foi realizada utilizando o coeficiente de Spearman [@spearman1961proof]:
  + A correla√ß√£o de Spearman √© uma medida n√£o param√©trica, aplicada aos postos das vari√°veis, e √© definida pela express√£o:
$$
r_{s} = \rho_{rg_{X}, rg_{Y}} = \frac{cov\left(rg_{X}, rg_{Y}\right)}{\sigma_{rg_X}\sigma_{rg_Y}} \text{,}
$$
  + em que:
    + $\rho$ √© o coeficiente de Pearson aplicado aos postos das vari√°veis,
    + $cov\left(rg_{X}, rg_{Y}\right)$ √© a covari√¢ncia entre os postos,
    + $\sigma_{rg_X}$ e $\sigma_{rg_Y}$ s√£o os desvios padr√£o dos postos de $X$ e $Y$.

## Etapas de pr√©-processamento

- **M√©todo de imputa√ß√£o de valores ausentes:**

  + Foi utilizado o KNNImputer, que estima valores ausentes com base na m√©dia dos $k$ vizinhos mais pr√≥ximos no espa√ßo dos dados observados. O m√©todo √© expresso pela seguinte f√≥rmula:
$$
\hat{y} = \frac{1}{k}\sum_{x_i \in N_k\left(x\right)}y_i\text{,}
$$
  + em que $\hat{y}$ √© o valor imputado, $k$ √© o n√∫mero de vizinhos considerados, $N_k\left(x\right)$ representa o conjunto dos $k$ vizinhos mais pr√≥ximos de $x$.
  + A imputa√ß√£o funciona da seguinte maneira:
    + Para cada observa√ß√£o com valores ausentes, o algoritmo identifica as $k$ observa√ß√µes mais pr√≥ximas, considerando apenas as vari√°veis com valores dispon√≠veis;
      + Para fazer a identifica√ß√£o dos $k$ vizinhos mais pr√≥ximos, foi utilizado a dist√¢ncia euclidiana:
      $$
      d\left(x, x^{'}\right) = \sqrt{\sum_{j \in S}\left(x_j - x^{'}_j\right)^2}
      $$
        + em que $x$ e $x^{'}$ s√£o duas observa√ß√µes e $S$ representa o conjunto de vari√°veis dispon√≠veis (n√£o ausentes) nas duas observa√ß√µes.
    + Em seguida, calcula a m√©dia dos valores observados dos $k$ vizinhos para estimar o valor ausente;

## Etapas de pr√©-processamento

- **Transforma√ß√£o das vari√°veis num√©ricas:**
  + Para estabilizar a vari√¢ncia e tornar a distribui√ß√£o das vari√°veis independentes mais pr√≥ximas de uma normal, foram aplicadas duas transforma√ß√µes:
    + Foi aplicada a transforma√ß√£o de $\log\left(1+x\right)$ nas vari√°veis na maioria das vari√°veis num√©ricas, com exce√ß√£o da vari√°vel do produto entre as coordenadas geogr√°ficas e o valor do aluguel
    + A vari√°vel do produto entre as coordenadas geogr√°ficas e o valor do aluguel foi transformada com a transforma√ß√£o de Yeo-Johnson [@yeo], definida da seguinte forma:
    $$
    \psi(\lambda, x) = \begin{cases}
        [(1 + x)^\lambda - 1] / \lambda  &  \lambda \neq 0, \; x \ge 0 \\
        \ln(1 + x)                       &  \lambda = 0, \; x \ge 0 \\
        [(1 - x)^{2 - \lambda} - 1] / (\lambda - 2) \quad & \lambda \neq 2, \; x < 0 \\
        -\ln(1 - x)                     &   \lambda = 2, \; x < 0
    \end{cases} \text{,}
    $$
      + em que $\lambda$ √© estimado por m√°xima verossimilhan√ßa. A transforma√ß√£o $\log(1 + x)$ √© um caso particular da transforma√ß√£o de Yeo-Johnson quando $\lambda = 0$ e $x \ge 0$.
  + As vari√°veis de pre√ßo e √°rea m√©dia de aluguel n√£o foram transformadas, pois apresentavam piora na performance dos modelos quando eram transformadas.

## Etapas de pr√©-processamento

- **Transforma√ß√£o de vari√°veis categ√≥ricas:**
  + Para a vari√°vel tipo de im√≥vel, foi aplicada a t√©cnica de codifica√ß√£o one-hot: cada categoria foi transformada em uma nova coluna bin√°ria.
    + Por exemplo, o tipo "casa" originou uma nova coluna, onde o valor 1 indica que o im√≥vel √© uma casa, e 0 indica outra categoria.
- **Padroniza√ß√£o das vari√°veis num√©ricas:**
  + As vari√°veis num√©ricas, com exce√ß√£o das vari√°veis de valor e √°rea m√©dia do aluguel, foram padronizadas utilizando a seguinte f√≥rmula:
$$
z = \frac{x - \mu}{\sigma}
$$
  + em que:
    + $\mu$ representa a m√©dia da vari√°vel;
    + $\sigma$ representa o desvio padr√£o da vari√°vel.

## M√©tricas para avalia√ß√£o dos modelos

<br>

Para avaliar o desempenho dos modelos, foram utilizadas tr√™s m√©tricas principais:

::: {layout-ncol=3}
$$
\text{RMSE} = \sqrt{\dfrac{1}{n} \sum_{i = 0}^n (y_i - \hat y_i)^2}
$$

$$
R^2 = 1 - \dfrac{SS_{\text{res√≠duos}}}{SS_{\text{total}}}
$$

$$
\text{MAPE} = \frac{1}{n} \sum_{i=0}^n \left|1 - \frac{y_i}{\hat y_i}\right|\text{,}
$$
:::

<br>

- RMSE (Root Mean Squared Error):
  + Mede a raiz quadrada do erro quadr√°tico m√©dio. Penaliza mais fortemente grandes desvios e √© √∫til para identificar a magnitude t√≠pica dos erros do modelo.

- $R^2$ (Coeficiente de Determina√ß√£o):
  + Representa a propor√ß√£o da variabilidade da vari√°vel dependente que √© explicada pelo modelo. Um valor pr√≥ximo de 1 indica boa capacidade de explica√ß√£o.

- MAPE (Mean Absolute Percentage Error):
  + Mede o erro percentual absoluto m√©dio entre as previs√µes e os valores observados. √â √∫til para interpretar o erro de forma relativa ao valor real.

## Valida√ß√£o cruzada e otimiza√ß√£o de hiperpar√¢metros

::: {columns}
::: {.column width="50%"}

![](cv_test_imagem.png){fig-align="center"}
:::

::: {.column width="50%"}
- Para estimar o erro dos modelos e auxiliar na otimiza√ß√£o do n√∫mero de vizinhos mais pr√≥ximos, bem como dos demais hiperpar√¢metros, foi utilizada a t√©cnica de valida√ß√£o cruzada K-Fold.

- O K-Fold busca estimar o erro de generaliza√ß√£o, definido como $Err = E\left[L\left(Y, \hat{f}\left(X\right)\right)\right]$, por meio do seguinte processo:
  +  Em cada itera√ß√£o $1, \cdots, K$, o conjunto de dados √© particionado em $K-1$ folds para treinamento, reservando o fold restante para teste do modelo e c√°lculo da m√©trica de erro.
:::
:::

- Ao final das $K$ itera√ß√µes, o erro de generaliza√ß√£o pelo K-Fold √© calculado como a m√©dia dos erros obtidos nas predi√ß√µes feitas por modelos treinados sem o fold correspondente a cada observa√ß√£o:
$$
CV\left(\hat f\right) = \frac{1}{N}\sum_{i = 1}^N L\left(y_i, \hat{f}^{-k\left(i\right)}\left(x_i\right)\right)\text{.}
$$
em que $N$ √© o n√∫mero de observa√ß√µes, $L$ √© a fun√ß√£o de perda, $\hat{f}^{-k(i)}$ √© o modelo treinado sem o fold de $x_i$, e $k(i)$ indica o fold ao qual a observa√ß√£o pertence.

## Valida√ß√£o cruzada e otimiza√ß√£o de hiperpar√¢metros

- A otimiza√ß√£o dos hiperpar√¢metros foi realizada por meio da otimiza√ß√£o bayesiana.

- Nesse m√©todo, as pr√≥ximas tentativas s√£o ajustadas com base nos resultados anteriores, utilizando uma fun√ß√£o probabil√≠stica $P\left(c \mid \lambda\right)$, que modela a rela√ß√£o entre os hiperpar√¢metros $\lambda$ e o desempenho $c$.
    + A partir dessa fun√ß√£o, estimam-se a performance esperada $\hat{c}(\lambda)$ e a incerteza associada $\hat{\sigma}(\lambda)$ para cada configura√ß√£o $\lambda$.

- A escolha dos pr√≥ximos pontos √© feita por uma fun√ß√£o de aquisi√ß√£o, que equilibra a explora√ß√£o de regi√µes ainda n√£o exploradas e pr√≥ximas aos melhores resultados obtidos.

- Neste trabalho, utilizou-se o modelo Tree-Structured Parzen Estimator (TPE) para a fun√ß√£o probabil√≠stica:
    + O TPE constr√≥i duas fun√ß√µes de densidade, $l(\lambda)$ e $g(\lambda)$, que modelam a distribui√ß√£o dos hiperpar√¢metros com base na m√©trica de desempenho $y$.
    + A probabilidade condicional $P(\lambda \mid y)$ √© definida como:
    $$
    P(\lambda|y) =
    \begin{cases}
        l(\lambda)\text{, } & \text{se } y < y^* \\
        g(\lambda)\text{, } & \text{se } y \ge y^*
    \end{cases}\text{,}
    $$
em que $y^*$ √© um limiar e √© definido como sendo um quantil dos valores observados de y, de forma que:
$$
p\left(y < y^{*}\right) = \gamma
$$

## Valida√ß√£o cruzada e otimiza√ß√£o de hiperpar√¢metros

- No algoritmo TPE, a fun√ß√£o de aquisi√ß√£o utilizada √© o Expected Improvement (EI), que representa a expectativa de melhoria em rela√ß√£o a um limiar $y^{*}$, dado um modelo $M$ que mapeia $f: \Lambda \rightarrow \mathbb{R}^N$. Formalmente, o EI √© definido por:

$$
EI_{y^*}\left(\lambda\right) = \int_{-\infty}^{\infty} \max\left(y^* - y, 0\right)p_{M}\left(y|\lambda\right)dy\text{.}
$$

-  Ap√≥s a otimiza√ß√£o espec√≠fica para o algoritmo TPE, o Expected Improvement pode ser expresso como:

$$
EI_{y^*}\left(\lambda\right) = \frac{\gamma y^* l\left(\lambda\right) - l\left(\lambda\right) \int_{-\infty}^{y^*} yp\left(y\right)dy}{\gamma l\left(\lambda\right) + \left(1 - \gamma\right)g\left(\lambda\right)} \propto \left[\gamma + \frac{g\left(\lambda\right)}{l\left(\lambda\right)} \left(1 - \gamma\right)\right]^{-1}\text{.}
$$

- A partir da express√£o anterior, observa-se que maximizar o Expected Improvement no TPE √© equivalente a maximizar a raz√£o $l\left(\lambda\right)/g\left(\lambda\right)$.

## Import√¢ncia dos hiperpar√¢metros

# Interpreta√ß√£o dos algoritmos de aprendizagem de m√°quina

## Individual Conditional Expectation (ICE)

- O ICE (Individual Conditional Expectation) √© um m√©todo que permite analisar o efeito isolado de uma covari√°vel sobre a predi√ß√£o, mantendo as demais vari√°veis constantes.

- No ICE, √© considerado um grid ${x_{S_i}, x_{C_i}}_{i=1}^{N}$ de valores no dom√≠nio da vari√°vel de interesse $x_S$, enquanto as demais covari√°veis $x_C$ permanecem fixas.

- A partir das predi√ß√µes $\hat{f}$ geradas ao longo dessa varia√ß√£o, constr√≥i-se um gr√°fico que apresenta:
  + No eixo das ordenadas: os valores estimados ($\hat{f}$),
  + No eixo das abscissas: os valores do grid da vari√°vel $x_S$.

- Para facilitar a interpreta√ß√£o, os valores foram centralizados utilizando o valor m√≠nimo $x^{*}$ de $x_S$, feito da seguinte forma:

$$
\hat{f}_{cent}^{\left(i\right)} = \hat{f}^{\left(i\right)} - \mathbf{1} \hat{f}\left(x^*, \mathbf x_{C_i}\right)\text{,}
$$
em que $x^*$ √© selecionado como o m√≠nimo ou o m√°ximo de $x_S$‚Äã, $\hat f$‚Äã √© o modelo ajustado, e $\mathbf 1$ √© um vetor de uns.

- Tamb√©m foi utilizada a linha do Partial Dependence Plot (PDP), que corresponde √† m√©dia de todas as curvas individuais geradas pelo ICE.

<!-- ## Local interpretable model-agnostic explanations (LIME) -->

## Shapley Additive Explanations (SHAP)

-  O SHAP (Shapley Additive Explanations) √© um m√©todo que busca explicar as predi√ß√µes individuais de um modelo, atribuindo a cada vari√°vel uma contribui√ß√£o para o valor predito‚Äã

- Baseia-se nos valores de Shapley da teoria dos jogos de coaliz√£o, introduzidos por @shapley1953value, que representam a contribui√ß√£o m√©dia de cada vari√°vel considerando as poss√≠veis combina√ß√µes (coaliz√µes) de vari√°veis‚Äã.

- F√≥rmula dos Valores de Shapley:
  + Os valores de Shapley s√£o dados por:
$$
\phi_i\left(x\right) = \sum_{Q \subseteq S | \{i\}} \frac{|Q|!\left(|S| - |Q| - 1\right)!}{|S|!} \left(\Delta_{Q\cup \{i\}}\left(x\right) - \Delta_{Q}\left(x\right)\right)\text{,}
$$
  + em $Q$ √© um subconjunto de vari√°veis, $S$ o conjunto completo, e o $\Delta_{Q \cup \{i\}}\left(x\right) - \Delta_{Q}\left(x\right)$ corresponde √† contribui√ß√£o marginal da vari√°vel $i$ ao ser adicionada ao subconjunto $Q$.
- Modelo Aditivo:
  + O SHAP representa a explica√ß√£o como um modelo linear aditivo:
$$
g\left(\mathbf{z^{'}}\right) = \phi_0 + \sum_{j = 1}^{M} \phi_{j} z_{j}^{'}\text{,}
$$
em que $z'_j$ representa o vetor de coaliz√£o, indicando a presen√ßa ou aus√™ncia da vari√°vel $j$.


# Resultados

## An√°lise explorat√≥ria

![](valores_ausentes.png)

# Refer√™ncias

::: {#refs}
:::
