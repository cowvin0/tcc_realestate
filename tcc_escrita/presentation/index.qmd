---
title: "Uso de Aprendizado de M√°quina na Modelagem Preditiva dos Valores de Im√≥veis na Cidade de Jo√£o Pessoa"
crossref:
  custom:
    - kind: float
      key: algo
      reference-prefix: "Algoritmo"
      caption-prefix: "Algoritmo"
      latex-env: algo
      latex-list-of-description: Algoritmo
institute: "Universidade Federal da Para√≠ba"
author: "Aluno: Gabriel de Jesus Pereira <br> Orientador: Pedro Rafael Diniz Marinho"
bibliography: ../includes/bib.bib
embed-resources: true
format:
  revealjs:
    #theme: [solarized, custom.scss]
    theme: solarized
    width: 1920
    height: 1280
    logo: ../includes/logode.png
    footer: 'Departamento de Estat√≠stica - UFPB'
    transition: slide
    background-transition: fade
    preview-links: auto
    slide-number: true
    scrollable: true
    controls: true
    code-tools: true
    auto-stretch: true
    code-link: true
execute:
  refresh: true
  warning: false
  error: false
  eval: true
  echo: false
editor:
  markdown:
    wrap: 72
lang: pt
---

# Introdu√ß√£o

## Formula√ß√£o do mercado imobili√°rio brasileiro

<br>

- Urbaniza√ß√£o no Brasil: de 31% em 1940 para 85,1% em 2014

- √äxodo rural, industrializa√ß√£o e crescimento desordenado das metr√≥poles

  + In√≠cio da industrializa√ß√£o no Governo de Get√∫lio Vargas (d√©cada de 1930)
  + Governo de Juscelino Kubitschek implementou o Plano de Metas (d√©cada de 1950)
    + Incluia investimentos estatais em agricultura, sa√∫de, educa√ß√£o, energia, transporte, minera√ß√£o e constru√ß√£o civil.

- Cria√ß√£o do Sistema Financeiro da Habita√ß√£o (SFH) ‚Äì Lei n¬∫ 4.380/1964

  + Implementou a corre√ß√£o monet√°ria
  + Cria√ß√£o do Banco Nacional da Habita√ß√£o (BNH)
    + Visava incentivar o mercado imobili√°rio, atrair o setor privado e regulamentar o financiamento do SFH, incluindo garantias, prazos e taxas [@assumpccao2011credito]
  + Cria√ß√£o das Sociedades de Cr√©dito Imobili√°rio (SCI)
    + Subordinadas ao BNH, atuavam como agentes financeiros exclusivos no financiamento de constru√ß√£o, venda ou aquisi√ß√£o de bens destinados a habita√ß√£o
  + O FGTS foi criado nesse per√≠odo e, junto com a caderneta de poupan√ßa, tornou-se a principal fonte de financiamento habitacional no Brasil

## Crises econ√¥micas e impacto no setor habitacional

<br>

- A partir de 1980, o SFH passou a ser impactado pelo aumento da infla√ß√£o e pelas medidas adotadas pelos governos para cont√™-la

- Em 1985, as presta√ß√µes foram reajustadas em 112%, enquanto os saldos devedores subiram 246% com a infla√ß√£o acumulada

- O Plano Cruzado (1986) converteu o valor das presta√ß√µes com base na m√©dia dos 12 meses anteriores e congelou os reajustes pelos 12 meses seguintes
  + A medida atingiu todos os contratos, reduzindo as presta√ß√µes em cerca de 40%, mas, a longo prazo, os deixou mais caros

- Em 1987 e 1989, as presta√ß√µes foram congeladas temporariamente pelo Plano Bresser e pelo Plano Ver√£o, respectivamente

- O Plano Collor I (1990), foi o que mais prejudicou o SFH
  + Bloqueou todos os ativos financeiros e 60% do saldo das cadernetas de poupan√ßa. Dos 40% restantes, cerca da metade foi sacada pelos depositantes, reduzindo o saldo das cadernetas de US$ 30 bilh√µes para aproximadamente entre US$ 7 e US$ 8 bilh√µes.

- A crise econ√¥mica entre as d√©cadas de 1980 e 1990 elevou a inadimpl√™ncia no SFH de 9% em 1994 para 30% em 2005.

## Moderniza√ß√£o do sistema habitacional

<br>

- A partir da experi√™ncia com o SFH, o Sistema de Financiamento Imobili√°rio (SFI) foi criado em 1997, pela Lei n¬∫ 9.514, para modernizar o modelo de financiamento habitacional no Brasil

\vfill

- O SFI capta recursos diretamente do mercado por meio de opera√ß√µes realizadas por institui√ß√µes financeiras autorizadas, como bancos, sociedades de cr√©dito e companhias hipotec√°rias.
  + As entidades autorizadas podem aplicar os recursos por meio de novos instrumentos introduzidos pelo SFI, como o Certificado de Receb√≠veis Imobili√°rios (CRI), a Letra de Cr√©dito Imobili√°rio (LCI) e a C√©dula de Cr√©dito Imobili√°rio (CCI).

\vfill

- A seguran√ßa dos contratos passou a ser garantida pela introdu√ß√£o da aliena√ß√£o fidunci√°ria

\vfill

- Com a securitiza√ß√£o dos cr√©ditos imobili√°rios e o fortalecimento da seguran√ßa jur√≠dica dos contratos como principais fundamentos, o SFI representou a moderniza√ß√£o efetiva do mercado imobili√°rio brasileiro.

## Possibilidades e desafios

<br>

- Possibilidades
  + Valoriza√ß√£o do mercado imobili√°rio de Jo√£o Pessoa
    + Destaque nacional em 2024, com valoriza√ß√£o acumulada de 16,13% at√© novembro
      + Atratividade da cidade: qualidade de vida, turismo, investimentos urbanos
  + Aplica√ß√£o de modelos preditivos
    + Apoio a decis√µes no mercado: compradores, vendedores, investidores, setor p√∫blico
    + Utiliza√ß√£o para estimativas tribut√°rias, como ITBI
    + Potencial uso por imobili√°rias e plataformas online para precifica√ß√£o autom√°tica

  + Visualiza√ß√µes interativas e mapas
    + Compreens√£o geogr√°fica das din√¢micas de pre√ßo por bairro

- Desafios
  + Escassez e descentraliza√ß√£o dos dados
    + Falta de bases p√∫blicas atualizadas e padronizadas sobre im√≥veis
    + Necessidade de utilizar abordagens pouco usuais para coleta de dados

  + Dados com informa√ß√µes incompletas ou inconsistentes
    + Aus√™ncia de vari√°veis relevantes: idade do im√≥vel, estado de conserva√ß√£o

## Objetivo

<br>

- **Objetivo Geral**
  + Realizar a an√°lise e modelagem dos valores dos im√≥veis em Jo√£o Pessoa, utilizando t√©cnicas de aprendizado de m√°quina para compreender os fatores que mais influenciam essas estimativas

- **Objetivos Espec√≠ficos**
  + Desenvolver modelos preditivos de valor de im√≥veis com base em dados coletados via web scraping
  + Construir uma aplica√ß√£o interativa para estimar pre√ßos com base nas caracter√≠sticas informadas pelo usu√°rio
  + Criar visualiza√ß√µes que facilitem a interpreta√ß√£o dos resultados e auxiliem na tomada de decis√µes
  + Avaliar o impacto e a import√¢ncia das vari√°veis por meio de t√©cnicas interpret√°veis (SHAP, ICE, PDP)
  + Comparar o desempenho de diferentes algoritmos de aprendizado de m√°quina aplicados ao problema

# Algoritmos de Aprendizado de M√°quina

## √Årvores de regress√£o

<br>

::: columns
::: {.column width="50%"}

<!-- - As √°rvores de decis√£o podem ser utilizadas tanto para regress√£o quanto para classifica√ß√£o -->

- Uma das grandes vantagens √© a interpretabilidade do modelo, em que √© poss√≠vel visualizar claramente as regras de decis√£o geradas

- O processo de constru√ß√£o das √°rvores se baseia no particionamento recursivo do espa√ßo dos preditores.
 + Cada particionamento √© chamado de n√≥ e o resultado final √© chamado de folha ou n√≥ terminal.

- O espa√ßo dos preditores √© dividido em $J$ regi√µes distintas e disjuntas denotadas por $R_{1}, R_{2}, \cdots, R_{J}$.
  <!-- + As regi√µes t√™m formato de caixa. -->

:::

::: {.column width="50%"}
![Exemplo de estrutura de √°rvore de decis√£o. A √°rvore tem cinco folhas e quatro n√≥s internos.](../../../../Documents/mermaid-tree.png){width="70%"}
:::
:::

- A vari√°vel resposta √© modelada como sendo uma constante $c_j$ em cada regi√£o $R_j$:

::: {layout-ncol=2}
$$
f\left(x\right) = \sum^{J}_{j=1}c_jI\left(x \in R_j \right),
$$

$$
I_{R_j}(x) =
\begin{cases}
    1,& \text{se } x \in R_j \\
    0,& \text{se } x \notin R_j
\end{cases}\text{.}
$$
:::

## √Årvores de regress√£o

<br>

O estimador para a constante $c_j$ √© encontrado pelo m√©todo de m√≠nimos quadrados:

$$
\sum_{x_i \in R_{j}} \left[y_i - f\left(x_i\right)\right]^2 \text{.}
$${#eq-minimo_q}

Como $f\left(x_i\right)$ est√° sendo avaliado em um ponto espec√≠fico $x_i$ e as regi√µes s√£o disjuntas, tem-se que $f\left(x_i\right) = c_j$. Dessa forma, a @eq-minimo_q se reduz a:

$$
\sum_{x_i \in R_j}\left[y_i - c_j\right]^2\text{.}
$$

Derivando em rela√ß√£o a $c_j$ e igualando a zero:

$$
\frac{\partial}{\partial c_j} \sum_{x_i \in R_j}\left(y_i - c_j\right)^2 = -2 \sum_{x_i \in R_j}\left(y_i - c_j\right) = 0\text{.}
$$

Assim, chega-se ao estimador para $c_j$:

$$
\hat{c}_j = \frac{1}{N_j}\sum_{x_i \in R_j} y_i\text{.}
$$

<!-- Ou seja, a predi√ß√£o ser√° a m√©dia das observa√ß√µes da resposta que est√£o contidas em $R_j$. -->

## √Årvores de regress√£o

<br>

- Considerar todas as poss√≠veis parti√ß√µes do espa√ßo dos preditores √© invi√°vel devido ao alto custo computacional.

- Dessa forma, utiliza-se a abordagem de divis√£o bin√°ria recursiva.

- O processo inicia com a escolha de uma vari√°vel independente $X_j$ e um ponto de corte $s$ que proporcione a maior redu√ß√£o poss√≠vel na soma dos quadrados dos res√≠duos. Isso define dois subconjuntos:

$$
R_{1}\left(j, s\right) = \{X|X_j \leq s\} \text{ e } R_{2}\left(j, s\right) = \{X|X_j > s\} \text{.}
$$

- O objetivo √© minimizar:

$$
\min_{j, s}\left[\min_{c_1} \sum_{x_i \in R_1\left(j, s\right)} \left(y_i - c_{1}\right)^2 + \min_{c_2} \sum_{x_i \in R_2\left(j, s\right)} \left(y_i - c_{2}\right)^2\right]\text{,}
$$
em que $c_1$ e $c_2$ s√£o as m√©dias das respostas nas regi√µes $R_1(j, s)$ e $R_2(j, s)$, respectivamente.

- Ap√≥s encontrar a melhor divis√£o, os dados s√£o particionados nas duas regi√µes e o processo √© repetido de forma recursiva para todas as sub-regi√µes.

## √Årvores de regress√£o

<br>

- O tamanho da √°rvore atua como um controle da complexidade do modelo:
  + √Årvores muito grandes tendem ao sobreajuste, com bom desempenho no treino, mas fraca generaliza√ß√£o.
  + √Årvores muito pequenas podem levar ao subajuste, por n√£o capturarem padr√µes relevantes nos pr√≥prios dados de treino.

- Para lidar com isso, adota-se a estrat√©gia de crescer inicialmente uma √°rvore grande $T_0$, interrompendo as divis√µes apenas quando um n√∫mero m√≠nimo de observa√ß√µes por n√≥ for alcan√ßado.

- Em seguida, realiza-se a poda da √°rvore com base no crit√©rio de custo-complexidade.

- Para o processo de poda, define-se uma √°rvore qualquer $T \subset T_{0}$ obtida a partir da poda de $T_{0}$, isto √©, colapsando o seus n√≥s internos. Assim, define-se o crit√©rio de custo-complexidade:

$$
C_{\alpha}\left(T\right) = \sum^{|T|}_{j=1}N_jQ_j\left(T\right) + \alpha|T|\text{,}
$$
em que $|T|$ √© o n√∫mero de folhas da √°rvore, $Q_{j}\left(T\right)$ √© a impureza do n√≥ terminal $j$, $\alpha$ √© o par√¢metro que equilibra o tamanho da √°rvore e a adequa√ß√£o aos dados.


- O objetivo √© encontrar, para cada valor de $\alpha$, a sub√°rvore $T_\alpha \subset T_0$ que minimiza $C_{\alpha}(T)$.
  + Quando $\alpha = 0$, resulta na pr√≥pria √°rvore $T_{0}$ e valores grandes de $\alpha$ resultam em √°rvores menores

## √Årvores de regress√£o

<br>

- A busca pela sub√°rvore $T_{\alpha}$ √© feita por meio do colapso sucessivo dos n√≥s internos que causam o menor aumento em $\sum_{j} N_j Q_j\left(T\right)$, at√© restar uma √°rvore com um √∫nico n√≥.

- Esse processo gera uma sequ√™ncia de sub√°rvores, dentre as quais existe, para cada valor de $\alpha$, uma √∫nica sub√°rvore que minimiza $C_{\alpha}\left(T\right)$.

- A estima√ß√£o de $\alpha$ pode ser feita por valida√ß√£o cruzada, resultando na √°rvore final $T_{\hat{\alpha}}$.

- No caso da regress√£o, $Q_j\left(T\right)$ pode representar o erro quadr√°tico m√©dio.

## M√©todos Ensemble

<br>

- Os m√©todos ensemble consistem em combinar m√∫ltiplos estimadores base para construir um modelo preditivo mais robusto e preciso.

\vfill

- Segundo @hastie2009elements, o aprendizado em conjunto envolve duas etapas principais:
  + Desenvolver uma popula√ß√£o de estimadores base a partir dos dados de treinamento.
  + Combinar esses estimadores para formar um √∫nico modelo preditivo.

\vfill

- Essa abordagem √© especialmente √∫til para melhorar o desempenho de modelos simples, como √°rvores de regress√£o, ao combin√°-las em conjunto.

\vfill

- Entre os principais m√©todos ensemble, destacam-se: Bagging, Random Forest, Boosting, Gradient Boosting e Stacking.

## Bagging

<br>

- O algoritmo Bagging (Bootstrap Aggregating) foi introduzido por @breiman1996bagging.

- Sua ideia central √© gerar um estimador agregado a partir de m√∫ltiplas vers√µes de um preditor, treinadas com amostras bootstrap do conjunto de treinamento.

- O principal objetivo do Bagging √© reduzir a vari√¢ncia de modelos, como as √°rvores de regress√£o, que tendem a variar muito quando treinadas isoladamente.

- Ao combinar diversas √°rvores treinadas em subconjuntos distintos, o Bagging melhora a estabilidade e o desempenho preditivo do modelo final.

- No caso de regress√£o, a predi√ß√£o do Bagging √© obtida pela m√©dia das predi√ß√µes individuais de cada √°rvore.

- Formalmente, seja $\mathcal{L}$ o conjunto de treinamento. A partir dele, geram-se $B$ amostras bootstrap $\mathcal{L}^{(b)}$, cada uma usada para treinar uma √°rvore de regress√£o $f(x, \mathcal{L}^{(b)})$.

- A predi√ß√£o final agregada √© dada por:

$$
f_{B}\left(x\right) = \frac{1}{B} \sum_{b = 1}^B f \left(x, \mathcal{L}^{\left(B\right)}\right)\text{,}
$$

onde $f_B(x)$ representa a predi√ß√£o m√©dia das $B$ √°rvores.

## Bagging

<br>

- Embora o Bagging reduza a vari√¢ncia e melhore o desempenho preditivo de √°rvores de regress√£o, isso ocorre √† custa de menor interpretabilidade.

<!-- - Uma alternativa √© estimar a import√¢ncia dos preditores com base na redu√ß√£o m√©dia do erro (ex.: erro quadr√°tico m√©dio) provocada por cada vari√°vel ao longo das $B$ √°rvores. Um valor elevado na redu√ß√£o total m√©dia do erro quadr√°tico m√©dio, calculado com base nas divis√µes realizadas por um determinado preditor em todas as ùêµ √°rvores, indica que o preditor √© importante. -->

- No entanto, as √°rvores geradas pelo Bagging tendem a ser muito semelhantes, o que limita seus ganhos:
  + Se forem independentes e identicamente distribu√≠das (i.i.d.), a vari√¢ncia da m√©dia das predi√ß√µes √© $\frac{1}{B}\sigma^2$.
  + No entanto, se as √°rvores forem apenas identicamente distribu√≠das (i.d), com correla√ß√£o $\rho$, mas n√£o independentes
$$
\rho\sigma^2 + \frac{1 - \rho}{B}\sigma^2
$$
  + Mesmo com $B \to \infty$, o termo $\rho \sigma^2$ permanece, limitando a redu√ß√£o de vari√¢ncia.

- Essa correla√ß√£o ocorre porque as mesmas vari√°veis tendem a ser escolhidas repetidamente nas divis√µes, por causarem maior redu√ß√£o no erro.

## Random Forest

<br>

- O algoritmo Random Forest √© uma extens√£o do Bagging aplicado a √°rvores de regress√£o, com uma modifica√ß√£o: reduzir a correla√ß√£o entre as √°rvores.

\vfill

- Isso √© feito por meio da sele√ß√£o aleat√≥ria de $m$ preditores, dentre os $p$ dispon√≠veis, como candidatos para cada divis√£o.
    + Quando $m = p$, o algoritmo se comporta como o Bagging.

\vfill

- Essa aleatoriedade procura resolver o problema do Bagging, que tende a gerar √°rvores muito semelhantes.

\vfill

- Em m√©dia, uma fra√ß√£o $1 - m/p$ das divis√µes sequer incluir√° o preditor mais forte como candidato, o que aumenta a chance de sele√ß√£o de outros preditores.

\vfill

- Assim como o Bagging, o Random Forest n√£o sofre de sobreajuste com o aumento do n√∫mero de √°rvores $B$, bastando escolher um valor suficientemente grande para estabilizar o erro.

## {background-iframe="https://pedro-rafael.shinyapps.io/shiny_apps/#section-bagging-vs-random-forest"}


## Boosting Trees

<br>

- No Boosting, n√£o √© necess√°rio utilizar amostras bootstrap. Al√©m disso, diferente do Bagging e Random Forest que constroem as √°rvores de forma paralela, o Boosting as constr√≥i sequencialmente.

- O objetivo √© incorporar informa√ß√µes e corrigir os erros cometidos pelas √°rvores anteriores.

- **Passo 1 - Inicializa√ß√£o: **
   + Defina: $\hat{f}\left(x\right) = 0 \text{ e } r_i = y_i$ para todo $i$ no conjunto de treinamento
- **Passo 2 - Cria√ß√£o de $B$ √°rvores (para $b=1$ at√© $B$):**
  + Ajuste uma √°rvore $\hat{f}^{b}$ com $d$ divis√µes aos res√≠duos $\left(X, r\right)$
  + Atualize $\hat{f}$ e os res√≠duos $r_i$ adicionando uma vers√£o com o hiperpar√¢metro de taxa de aprendizado $\lambda$:
  $$
  \hat{f}\left(x\right) \gets \hat{f}\left(x\right) + \lambda \hat{f}^b\left(x\right) \text{, } r_i \gets r_i - \lambda\hat{f}^b\left(x_i\right)
  $$
- **Passo 3 - Retorno do modelo final**
  $$
  \hat{f}\left(x\right) = \sum^{B}_{b=1}\lambda\hat{f}^{b}\left(x\right)
  $$

## Boosting Trees

<br>

- Os hiperpar√¢metros $\lambda$ (taxa de aprendizado), $B$ (n√∫mero de √°rvores) e $d$ (n√∫mero de divis√µes por √°rvore) podem ser ajustados por meio de valida√ß√£o cruzada.

- O processo de aprendizado do Boosting √© mais lento, o que geralmente resulta em
modelos mais precisos, embora mais custosos computacionalmente.

- O processo de aprendizado √© controlado pelo hiperpar√¢metro $\lambda$. Valores menores de $\lambda$ exigem um n√∫mero maior de √°rvores $B$ e, diferentemente do Bagging e da Random Forest, o Boosting pode sofrer sobreajuste caso sejam adicionadas muitas √°rvores ao modelo.

- √â comum utilizar √°rvores com menor profundidade ($d$), j√° que cada nova √°rvore depende das √°rvores j√° constru√≠das [@james2013introduction].

## Gradient Tree Boosting

<br>

- A ideia central √© construir um conjunto de √°rvores de regress√£o baseado no gradiente da fun√ß√£o de perda $L\left(y_i, f\left(x\right)\right)$, minimizando-a de forma sequencial.

**Passo 1 - Inicializa√ß√£o: ** O algoritmo inicia ajustando uma √°rvore aos dados observados:

- O algoritmo inicia ajustante uma √°rvore aos dados observados, sendo as vari√°veis independentes representadas por $\gamma$ nesse primeiro passo:

$$
f_0\left(x\right) = \arg \min_{\gamma} \sum_{i = 1}^N L\left(y_i, \gamma \right)
$$

**Passo 2 ‚Äî Itera√ß√µes (para $m = 1$ at√© $M$):**

- Para cada itera√ß√£o $m$, o algoritmo realiza os seguintes passos:
  - a) Para cada observa√ß√£o $i = 1, 2, \dots, N$, calcula-se o gradiente negativo da fun√ß√£o de perda (pseudo-res√≠duo):
  $$
  {r}_{im} = -\left[\frac{\partial L\left(y_i, f\left(x_i \right)\right)}{\partial f\left(x_i\right)}\right]_{f = f_{m - 1}}
  $$
    + O $r_{im}$ indica a dire√ß√£o de maior redu√ß√£o da fun√ß√£o de perda.

## Gradient Tree Boosting

<br>

**Cont. (Passo 2)**

  - b) Ajusta uma √°rvore aos pseudo-res√≠duos:
    + Uma √°rvore de regress√£o √© ajustada aos valores $r_{im}$, resultando em regi√µes terminais $R_{jm}$, com $j = 1, 2, \dots, J_m$.
  - c) C√°lculo do valor terminal $\gamma_{jm}$:
    + Para cada regi√£o terminal $R_{jm}$‚Äã, busca-se o valor $\gamma_{jm}$‚Äã que minimiza a fun√ß√£o de perda ao atualizar as predi√ß√µes do modelo anterior:
  $$
  \gamma_{jm} = \arg \min_{\gamma} \sum_{x_i \in R_{jm}} L\left(y_i, f_{m - 1}\left(x_i\right) + \gamma\right)
  $$
      + **Observa√ß√£o:** no caso de $L\left(y_i, f\left(x\right)\right)$ ser o erro quadr√°tico m√©dio, $\gamma_{jm}$ corresponde √† m√©dia dos pseudo-res√≠duos na regi√£o $R_{jm}$.

  - b) Atualize $f_m\left(x\right) = f_{m - 1}\left(x\right) + \lambda \sum^{J_m}_{j = 1} \gamma_{jm} I\left(x \in R_{jm}\right)$, em que $\lambda$ √© a taxa de aprendizado.

- **Passo 3: ** Retorne $\hat{f}\left(x\right) = f_M\left(x\right)$.

## Outras implementa√ß√µes de Gradient Boosting

- Algumas implementa√ß√µes de Gradient Boosting, como o **XGBoost** e o **LightGBM**, utilizados neste trabalho, t√™m como foco principal a redu√ß√£o do custo computacional e a incorpora√ß√£o de t√©cnicas de regulariza√ß√£o para melhorar o desempenho e a generaliza√ß√£o dos modelos.

- Para tornar o algoritmo mais eficiente, destaca-se o Histogram-Based Algorithm, que busca os pontos de corte com menor custo computacional. Esse m√©todo discretiza os valores cont√≠nuos das vari√°veis em intervalo, semelhantes aos de um histograma, permitindo que as divis√µes nas √°rvores sejam feitas com base nesses intervalos, em vez de avaliar cada ponto individualmente. Essa abordagem reduz significativamente o custo computacional.

- Al√©m da maior efici√™ncia computacional, t√©cnicas de regulariza√ß√£o L1 ou L2 podem ser incorporadas, a partir da seguinte fun√ß√£o:
$$
\mathcal{L}\left(\phi\right) = \sum_{i}L\left(y_{i}, f\left(x_i\right)\right) + \sum_{m}\Omega\left(f_{m}\left(x\right)\right)\text{, onde }\Omega\left(f\right) = \gamma T + \frac{1}{2}\lambda\|\omega\|^{2}
$$
  + Em que:
    + $T$ representa a quantidade de folhas da √°rvore;
    + $\|\omega\|^{2}$ √© a soma dos quadrados dos pesos atribu√≠dos √†s folhas;
    + $\gamma$ e $\lambda$ s√£o par√¢metros de regulariza√ß√£o.
  + **Observa√ß√£o: ** $\frac{1}{2}\sum_{m}\lambda \|\omega\|^2$ representa a regulariza√ß√£o L2, enquanto $\gamma$ atua como uma penaliza√ß√£o pela complexidade da √°rvore.

## Stacked generalization


- O Stacked Generalization (Stacking) √© um m√©todo ensemble que combina as predi√ß√µes de v√°rios modelos para treinar um novo estimador, com o objetivo de melhorar a precis√£o das predi√ß√µes.

- A ideia central √© atribuir pesos √†s predi√ß√µes dos modelos, dando mais import√¢ncia √†queles com melhor desempenho, enquanto se evita atribuir altos pesos a modelos excessivamente complexos.

- Matematicamente, o Stacking ajusta o modelo $m = 1, \cdots, M$ ao conjunto de treinamento com a $i$-√©sima observa√ß√£o removida, definindo predi√ß√µes $\hat f_{m}^{-i}\left(x\right)$.

- Os pesos s√£o estimados por m√≠nimos quadrados, resolvendo:

$$
\hat{w}^{st} = \arg \min_{w} \sum^{N}_{i = 1} \left[y_i - \sum^{M}_{m = 1} w_m \hat{f}^{-i}_m\left(x_i\right)\right]^2\text{.}
$$

- A predi√ß√£o final √© dada por:

$$
\sum_{m}\hat{w}^{\text{st}}_{m}\hat{f}_{m}\left(x\right)
$$

- O uso da  valida√ß√£o cruzada leave-one-out na constru√ß√£o das predi√ß√µes $\hat{f}^{-i}_{m}\left(x\right)$ reduz o risco de sobreajuste, impedindo que modelos muito complexos tenham pesos muito altos.

# Metodologia

## Ferramentas computacionais

::: r-stack
![](R_logo.svg){.fragment width="400" height="400"}

![](tidygeocoder_hex.png){.fragment width="400" height="400"}

![](logo.png){.fragment width="400" height="400"}

![](Selenium.svg){.fragment width="600" height="600"}

![](python_logo.svg){.fragment width="900" height="900"}

![](numpy_logo.svg){.fragment width="900" height="800"}

![](pandas_logo.svg){.fragment width="900" height="900"}

![](seaborn_logo.svg){.fragment width="400" height="400"}

![](Matplotlib_icon.svg){.fragment width="400" height="400"}

![](optuna-logo.png){.fragment width="700" height="400"}

![](Scikit_learn_logo_small.svg){.fragment width="400" height="400"}

![](Scrapy--Streamline-Simple-Icons.svg){.fragment width="550" height="550"}

![](Playwright_Logo.svg){.fragment width="700" height="700"}

![](scrapeops-logo.svg){.fragment width="500" height="500"}

![](plotly_logo.png){.fragment width="700" height="300"}

![](Postgresql_elephant.svg){.fragment width="500" height="500"}

![](docker-logo-blue.svg){.fragment width="700" height="700"}
:::

## Obten√ß√£o dos dados

<br>

::: columns
::: {.column width="50%"}
- A obten√ß√£o de dados para o mercado imobili√°rio representa um desafio, devido √† escassez de bases p√∫blicas organizadas.

- Como alternativa, optou-se pela extra√ß√£o direta de informa√ß√µes de sites especializados utilizando a t√©cnica conhecida como web scraping, com a coleta realizada em dois momentos distintos.

- Em particular, as informa√ß√µes foram coletadas do Zap Im√≥veis.
:::

::: {.column width="50%"}
![Zap Im√≥veis, plataforma de oferta de im√≥veis.](Zap_im%C3%B3veis_2021.svg.png){#fig-zap width="80%" fig-align="center"}
:::
:::

- O Zap Im√≥veis √© uma das principais plataformas online de compra, venda e aluguel de im√≥veis no Brasil. Fundado em mar√ßo de 2000, inicialmente recebeu o nome de Planeta Im√≥vel.

- Possui uma base extensa de an√∫ncios em diferentes cidades, incluindo Jo√£o Pessoa, abrangendo im√≥veis novos e usados, residenciais e comerciais.

- Sua ampla cobertura e variedade de informa√ß√µes o tornam uma fonte relevante para estudos de mercado e an√°lise imobili√°ria.

- Entre as informa√ß√µes dispon√≠veis para coleta, destacam-se o pre√ßo do im√≥vel, a √°rea, o n√∫mero de quartos, o n√∫mero de vagas de garagem, o endere√ßo, o tipo de im√≥vel, o valor do condom√≠nio e caracter√≠sticas adicionais, como piscina, academia, spa, entre outras.

## Web scraping

<br>

- O web scraping √© uma t√©cnica utilizada para extrair informa√ß√µes de sites da internet, armazenando-as em arquivos ou sistemas de banco de dados para fins de an√°lise, desenvolvimento de aplica√ß√µes ou acesso a informa√ß√µes de dif√≠cil obten√ß√£o.

- A coleta dos dados √© realizada por meio do Hypertext Transfer Protocol (HTTP),
  + O HTTP √© o protocolo que gerencia a comunica√ß√£o cliente-servidor na internet, baseado em requisi√ß√µes que especificam a√ß√µes sobre recursos definidos.

- Os m√©todos mais utilizados em web scraping s√£o o GET e o POST:
  + O m√©todo GET requisita uma representa√ß√£o do recurso especificado, sendo utilizado principalmente para visualizar dados.
  + O m√©todo POST √© usado para enviar dados ao servidor, geralmente para serem processados, como no envio de informa√ß√µes via formul√°rios HTML (ex.: login ou cadastro).

- O processo de web scraping normalmente se inicia com uma requisi√ß√£o GET para obter o conte√∫do de uma p√°gina web.
  + A resposta, geralmente em HTML, √© ent√£o analisada para extrair os dados de interesse.

## Principais desafios do web scraping

<br>

- Mudan√ßas na estrutura dos sites:
  + Pequenas altera√ß√µes no c√≥digo HTML ou na organiza√ß√£o das p√°ginas podem quebrar os scripts de extra√ß√£o, exigindo manuten√ß√µes frequentes.

- Conte√∫do din√¢mico:
  + Muitos sites modernos utilizam **JavaScript** para carregar informa√ß√µes de forma ass√≠ncrona, o que dificulta a captura dos dados apenas com requisi√ß√µes HTTP b√°sicas, tornando necess√°ria a utiliza√ß√£o de ferramentas como (Selenium ou Playwright).

- Limita√ß√µes de acesso e bloqueios:
  + Sites podem impor restri√ß√µes, como o bloqueio de IPs, o que exige a utiliza√ß√£o de t√©cnicas de rotacionamento de IP para contornar essas limita√ß√µes.

- Aspectos √©ticos e legais:
  + Nem todos os sites autorizam o scraping de seus dados. √â fundamental respeitar pol√≠ticas de uso, robots.txt, e considerar as implica√ß√µes legais, especialmente em rela√ß√£o a direitos autorais e prote√ß√£o de dados.

## Exemplo de raspagem com rotacionamento de user-agent

![](scraping_scrapeops.mp4)

## Exemplo de scroll autom√°tico com JavaScript para gerar conte√∫do da p√°gina

::: {.incremental}
![](mobi_scroll.mp4)

```javascript
(async () => {
    const scrollStep = 10;
    const delay = 16;
    let currentPosition = 0;

    function animateScroll() {
        const pageHeight = Math.max(
            document.body.scrollHeight, document.documentElement.scrollHeight,
            document.body.offsetHeight, document.documentElement.offsetHeight,
            document.body.clientHeight, document.documentElement.clientHeight
            );

        if (currentPosition < pageHeight) {
            currentPosition += scrollStep;
            if (currentPosition > pageHeight) {
                currentPosition = pageHeight;
            }
            window.scrollTo(0, currentPosition);
            requestAnimationFrame(animateScroll);
            }
        }
    animateScroll();
    })();
```

:::


## Dados obtidos a partir do web scraping

<br>

- Entre os dados obtidos, destacam-se:
  + Valor do im√≥vel: vari√°vel dependente que ser√° modelada e constitui o principal foco de an√°lise deste trabalho;
  + Valor m√©dio do aluguel no bairro: valor m√©dio do aluguel dos im√≥veis no bairro, em $m^3$;
  + √Årea: √°rea total do im√≥vel, medida em $m^2$;
  + √Årea m√©dia do aluguel no bairro: √°rea m√©dia dos im√≥veis alugados no bairro, em $m^2$;
  + Condom√≠nio: valor mensal pago pelo condom√≠nio do im√≥vel;
  + IPTU: imposto cobrado sobre im√≥veis urbanos;
  + Banheiros: quantidade de banheiros dispon√≠veis na propriedade;
  + Vagas de estacionamento: n√∫mero total de vagas de estacionamento dispon√≠veis;
  + Quartos: quantidade de quartos no im√≥vel;
  +  Latitude: posi√ß√£o horizontal, medida em fra√ß√µes decimais de graus;
  + Longitude: posi√ß√£o vertical, tamb√©m medida em fra√ß√µes decimais de graus, assim como a latitude;
  + Tipo do im√≥vel: sete categorias foram consideradas: apartamentos, casas, casas de condom√≠nio, flats e terrenos de lotes comerciais e de condom√≠nio;

## Dados obtidos a partir do web scraping

<br>

- Endere√ßo: nome do endere√ßo onde o im√≥vel est√° localizado;
- Vari√°veis dicot√¥micas: indicam a presen√ßa (1) ou aus√™ncia (0) de determinadas caracter√≠sticas no im√≥vel, como √°rea de servi√ßo, academia, elevador, espa√ßo gourmet, piscina, playground, portaria 24 horas, quadra de esportes, sal√£o de festas, sauna, spa e varanda gourmet.

- A base de dados final conta com **31.781** observa√ß√µes.

## Dados utilizados para cria√ß√£o de aplica√ß√£o web

<br>

::: columns
::: {.column width="60%"}
- O Filipeia - Mapas da Cidade √© uma plataforma online que disponibiliza dados geogr√°ficos e cartogr√°ficos sobre a cidade de Jo√£o Pessoa.

- Criado com o objetivo de apoiar pesquisadores, profissionais e a popula√ß√£o em geral, o site re√∫ne mapas atualizados dos bairros, ruas, equipamentos p√∫blicos e outras divis√µes territoriais do munic√≠pio.

- Entre os dados dispon√≠veis na plataforma e que foram utilizados no projeto, destacam-se: ciclovias, faixas exclusivas de √¥nibus, corredores de transporte p√∫blico, √°reas rurais, limites de bairros, al√©m da localiza√ß√£o de parques, pra√ßas e rios.
:::

::: {.column width="40%"}
![Filipeia - Mapas da Cidade.](filipeiaLogo.png){#fig-filipeia fig-align="center"}
:::
:::

::: columns
::: {.column width="40%"}
![Instituto Nacional de Estudos e Pesquisas Educacionais An√≠sio Teixeira (INEP).](inep.jpg){#fig-inep fig-align="center"}
:::

::: {.column width="60%"}
- As informa√ß√µes sobre escolas p√∫blicas foram obtidas junto ao Instituto Nacional de Estudos e Pesquisas Educacionais An√≠sio Teixeira (INEP).

- A partir dos dados do INEP, foi poss√≠vel incorporar √† aplica√ß√£o a localiza√ß√£o das escolas p√∫blicas de Jo√£o Pessoa, enriquecendo a an√°lise espacial e oferecendo mais uma camada de informa√ß√£o relevante para o projeto.
:::
:::

# Constru√ß√£o do modelo

## Divis√£o entre Treinamento e Teste e Escolha das Vari√°veis Preditivas

- Para avaliar o desempenho dos modelos, o conjunto de dados foi dividido em conjuntos de treinamento e teste da seguinte forma:
  + A fim de manter a propor√ß√£o dos intervalos de pre√ßos dos im√≥veis em ambos os conjuntos, a divis√£o foi realizada por meio da estratifica√ß√£o da vari√°vel dependente em cinco faixas: primeiro intervalo: de R$ 43.914 at√© R$ 200.000; segundo intervalo: de R$ 200.000 at√© R$ 400.000; terceiro intervalo: de R$ 400.000 at√© R$ 600.000; quarto intervalo: de R$ 600.000 at√© R$ 800.000; quinto intervalo: acima de R$ 800.000 at√© o m√°ximo de R$ 7.000.000.
  + Dessa forma, 20% dos dados foram reservados para o conjunto de teste e 80% para o treinamento dos modelos.
- Para o ajuste dos modelos, foram selecionadas vari√°veis consideradas relevantes para o estudo:
  + Pre√ßo e √°rea m√©dia do aluguel no bairro;
  + √Årea do im√≥vel, medida em $m^2$;
  + N√∫mero de quartos, banheiros e vagas de garagem;
  + Tipo de im√≥vel, podendo ser: apartamento, casa, casa de condom√≠nio, flat, terreno comercial ou terreno de condom√≠nio;
  + Caracter√≠sticas adicionais do im√≥vel: √°rea de servi√ßo, academia, elevador, espa√ßo gourmet, piscina, playground, portaria 24 horas, quadra de esportes, sal√£o de festas, sauna, spa e varanda gourmet.

## Divis√£o entre Treinamento e Teste e Escolha das Vari√°veis Preditivas

- Al√©m das vari√°veis coletadas por meio de web scraping, foram criadas novas vari√°veis derivadas de combina√ß√µes entre vari√°veis que apresentavam correla√ß√£o relevante:
  + Quantidade total de c√¥modos do im√≥vel;
  + Produto entre as coordenadas geogr√°ficas e o pre√ßo m√©dio do aluguel no bairro;
  + Raz√£o entre o n√∫mero de quartos e a √°rea do im√≥vel;
  + Produto entre o n√∫mero de quartos e a √°rea m√©dia do aluguel no bairro.
- A an√°lise de correla√ß√£o entre as vari√°veis foi realizada utilizando o coeficiente de Spearman [@spearman1961proof]:
  + A correla√ß√£o de Spearman √© uma medida n√£o param√©trica, aplicada aos postos das vari√°veis, e √© definida pela express√£o:
$$
r_{s} = \rho_{rg_{X}, rg_{Y}} = \frac{cov\left(rg_{X}, rg_{Y}\right)}{\sigma_{rg_X}\sigma_{rg_Y}} \text{,}
$$
  + em que:
    + $\rho$ √© o coeficiente de Pearson aplicado aos postos das vari√°veis,
    + $cov\left(rg_{X}, rg_{Y}\right)$ √© a covari√¢ncia entre os postos,
    + $\sigma_{rg_X}$ e $\sigma_{rg_Y}$ s√£o os desvios padr√£o dos postos de $X$ e $Y$.

## Etapas de pr√©-processamento

- **M√©todo de imputa√ß√£o de valores ausentes:**

  + Foi utilizado o KNNImputer, que estima valores ausentes com base na m√©dia dos $k$ vizinhos mais pr√≥ximos no espa√ßo dos dados observados. O m√©todo √© expresso pela seguinte f√≥rmula:
$$
\hat{y} = \frac{1}{k}\sum_{x_i \in N_k\left(x\right)}y_i\text{,}
$$
  + em que $\hat{y}$ √© o valor imputado, $k$ √© o n√∫mero de vizinhos considerados, $N_k\left(x\right)$ representa o conjunto dos $k$ vizinhos mais pr√≥ximos de $x$.
  + A imputa√ß√£o funciona da seguinte maneira:
    + Para cada observa√ß√£o com valores ausentes, o algoritmo identifica as $k$ observa√ß√µes mais pr√≥ximas, considerando apenas as vari√°veis com valores dispon√≠veis;
      + Para fazer a identifica√ß√£o dos $k$ vizinhos mais pr√≥ximos, foi utilizado a dist√¢ncia euclidiana:
      $$
      d\left(x, x^{'}\right) = \sqrt{\sum_{j \in S}\left(x_j - x^{'}_j\right)^2}
      $$
        + em que $x$ e $x^{'}$ s√£o duas observa√ß√µes e $S$ representa o conjunto de vari√°veis dispon√≠veis (n√£o ausentes) nas duas observa√ß√µes.
    + Em seguida, calcula a m√©dia dos valores observados dos $k$ vizinhos para estimar o valor ausente;

## Etapas de pr√©-processamento

<br>

- **Transforma√ß√£o das vari√°veis num√©ricas:**
  + Para estabilizar a vari√¢ncia e tornar a distribui√ß√£o das vari√°veis independentes mais pr√≥ximas de uma normal, foram aplicadas duas transforma√ß√µes:
    + Foi aplicada a transforma√ß√£o de $\log\left(1+x\right)$ nas vari√°veis na maioria das vari√°veis num√©ricas, com exce√ß√£o da vari√°vel do produto entre as coordenadas geogr√°ficas, valor e √°rea m√©dia do aluguel no bairro.
    + A vari√°vel do produto entre as coordenadas geogr√°ficas e o valor do aluguel foi transformada com a transforma√ß√£o de Yeo-Johnson [@yeo], definida da seguinte forma:
    $$
    \psi(\lambda, x) = \begin{cases}
        [(1 + x)^\lambda - 1] / \lambda  &  \lambda \neq 0, \; x \ge 0 \\
        \ln(1 + x)                       &  \lambda = 0, \; x \ge 0 \\
        [(1 - x)^{2 - \lambda} - 1] / (\lambda - 2) \quad & \lambda \neq 2, \; x < 0 \\
        -\ln(1 - x)                     &   \lambda = 2, \; x < 0
    \end{cases} \text{,}
    $$
      + em que $\lambda$ √© estimado por m√°xima verossimilhan√ßa. A transforma√ß√£o $\log(1 + x)$ √© um caso particular da transforma√ß√£o de Yeo-Johnson quando $\lambda = 0$ e $x \ge 0$.
  + As vari√°veis de pre√ßo e √°rea m√©dia de aluguel n√£o foram transformadas, pois apresentavam piora na performance dos modelos quando eram transformadas.

## Etapas de pr√©-processamento

<br>

- **Transforma√ß√£o de vari√°veis categ√≥ricas:**
  + Para a vari√°vel tipo de im√≥vel, foi aplicada a t√©cnica de codifica√ß√£o one-hot: cada categoria foi transformada em uma nova coluna bin√°ria.
    + Por exemplo, o tipo "casa" originou uma nova coluna, onde o valor 1 indica que o im√≥vel √© uma casa, e 0 indica outra categoria.
- **Padroniza√ß√£o das vari√°veis num√©ricas:**
  + As vari√°veis num√©ricas, com exce√ß√£o das vari√°veis de valor e √°rea m√©dia do aluguel, foram padronizadas utilizando a seguinte f√≥rmula:
$$
z = \frac{x - \mu}{\sigma}
$$
  + em que:
    + $\mu$ representa a m√©dia da vari√°vel;
    + $\sigma$ representa o desvio padr√£o da vari√°vel.

## M√©tricas para avalia√ß√£o dos modelos

<br>

Para avaliar o desempenho dos modelos, foram utilizadas tr√™s m√©tricas principais:

::: {layout-ncol=3}
$$
\text{RMSE} = \sqrt{\dfrac{1}{n} \sum_{i = 0}^n (y_i - \hat y_i)^2}
$$

$$
R^2 = 1 - \dfrac{SS_{\text{res√≠duos}}}{SS_{\text{total}}}
$$

$$
\text{MAPE} = \frac{1}{n} \sum_{i=0}^n \left|1 - \frac{y_i}{\hat y_i}\right|\text{,}
$$
:::

<br>

- RMSE (Root Mean Squared Error):
  + Mede a raiz quadrada do erro quadr√°tico m√©dio. Penaliza mais fortemente grandes desvios e √© √∫til para identificar a magnitude t√≠pica dos erros do modelo.

- $R^2$ (Coeficiente de Determina√ß√£o):
  + Representa a propor√ß√£o da variabilidade da vari√°vel dependente que √© explicada pelo modelo. Um valor pr√≥ximo de 1 indica boa capacidade de explica√ß√£o.

- MAPE (Mean Absolute Percentage Error):
  + Mede o erro percentual absoluto m√©dio entre as previs√µes e os valores observados. √â √∫til para interpretar o erro de forma relativa ao valor real.

## Valida√ß√£o cruzada e otimiza√ß√£o de hiperpar√¢metros

::: {columns}
::: {.column width="50%"}

![](cv_test_imagem.png){fig-align="center"}
:::

::: {.column width="50%"}
- Para estimar o erro dos modelos e auxiliar na otimiza√ß√£o do n√∫mero de vizinhos mais pr√≥ximos, bem como dos demais hiperpar√¢metros, foi utilizada a t√©cnica de valida√ß√£o cruzada K-Fold.

- O K-Fold busca estimar o erro de generaliza√ß√£o, definido como $Err = E\left[L\left(Y, \hat{f}\left(X\right)\right)\right]$, por meio do seguinte processo:
  +  Em cada itera√ß√£o $1, \cdots, K$, o conjunto de dados √© particionado em $K-1$ folds para treinamento, reservando o fold restante para teste do modelo e c√°lculo da m√©trica de erro.
:::
:::

- Ao final das $K$ itera√ß√µes, o erro de generaliza√ß√£o pelo K-Fold √© calculado como a m√©dia dos erros obtidos nas predi√ß√µes feitas por modelos treinados sem o fold correspondente a cada observa√ß√£o:
$$
CV\left(\hat f\right) = \frac{1}{N}\sum_{i = 1}^N L\left(y_i, \hat{f}^{-k\left(i\right)}\left(x_i\right)\right)\text{.}
$$
em que $N$ √© o n√∫mero de observa√ß√µes, $L$ √© a fun√ß√£o de perda, $\hat{f}^{-k(i)}$ √© o modelo treinado sem o fold de $x_i$, e $k(i)$ indica o fold ao qual a observa√ß√£o pertence.

## Valida√ß√£o cruzada e otimiza√ß√£o de hiperpar√¢metros

- A otimiza√ß√£o dos hiperpar√¢metros foi realizada por meio da otimiza√ß√£o bayesiana.

- Nesse m√©todo, as pr√≥ximas tentativas s√£o ajustadas com base nos resultados anteriores, utilizando uma fun√ß√£o probabil√≠stica $P\left(c \mid \lambda\right)$, que modela a rela√ß√£o entre os hiperpar√¢metros $\lambda$ e o desempenho $c$.
    + A partir dessa fun√ß√£o, estimam-se a performance esperada $\hat{c}(\lambda)$ e a incerteza associada $\hat{\sigma}(\lambda)$ para cada configura√ß√£o $\lambda$.

- A escolha dos pr√≥ximos pontos √© feita por uma fun√ß√£o de aquisi√ß√£o, que equilibra a explora√ß√£o de regi√µes ainda n√£o exploradas e pr√≥ximas aos melhores resultados obtidos.

- Neste trabalho, utilizou-se o modelo Tree-Structured Parzen Estimator (TPE) para a fun√ß√£o probabil√≠stica:
    + O TPE constr√≥i duas fun√ß√µes de densidade, $l(\lambda)$ e $g(\lambda)$, que modelam a distribui√ß√£o dos hiperpar√¢metros com base na m√©trica de desempenho $y$.
    + A probabilidade condicional $P(\lambda \mid y)$ √© definida como:
    $$
    P(\lambda|y) =
    \begin{cases}
        l(\lambda)\text{, } & \text{se } y < y^* \\
        g(\lambda)\text{, } & \text{se } y \ge y^*
    \end{cases}\text{,}
    $$
em que $y^*$ √© um limiar e √© definido como sendo um quantil dos valores observados de y, de forma que:
$$
p\left(y < y^{*}\right) = \gamma
$$

## Valida√ß√£o cruzada e otimiza√ß√£o de hiperpar√¢metros

<br>

- No algoritmo TPE, a fun√ß√£o de aquisi√ß√£o utilizada √© o Expected Improvement (EI), que representa a expectativa de melhoria em rela√ß√£o a um limiar $y^{*}$, dado um modelo $M$ que mapeia $f: \Lambda \rightarrow \mathbb{R}^N$. Formalmente, o EI √© definido por:

$$
EI_{y^*}\left(\lambda\right) = \int_{-\infty}^{\infty} \max\left(y^* - y, 0\right)p_{M}\left(y|\lambda\right)dy\text{.}
$$

-  Para o algoritmo de TPE, o Expected Improvement pode √© expresso como:

$$
EI_{y^*}\left(\lambda\right) = \frac{\gamma y^* l\left(\lambda\right) - l\left(\lambda\right) \int_{-\infty}^{y^*} yp\left(y\right)dy}{\gamma l\left(\lambda\right) + \left(1 - \gamma\right)g\left(\lambda\right)} \propto \left[\gamma + \frac{g\left(\lambda\right)}{l\left(\lambda\right)} \left(1 - \gamma\right)\right]^{-1}\text{.}
$$

- A partir da express√£o anterior, observa-se que maximizar o Expected Improvement no TPE √© equivalente a maximizar a raz√£o $l\left(\lambda\right)/g\left(\lambda\right)$.

## Import√¢ncia dos hiperpar√¢metros

- fANOVA (Functional Analysis of Variance)
  + Realiza a decomposi√ß√£o da vari√¢ncia explicada pela fun√ß√£o objetivo, atribuindo a contribui√ß√£o individual de cada hiperpar√¢metro (ou conjunto de hiperpar√¢metros) para o desempenho do modelo.

- Matematicamente, o fANOVA decomp√µe uma fun√ß√£o
  $$
  \hat y: \Theta_{1} \times \dots \times \Theta_{n} \rightarrow \mathbb{R}
  $$
  em componentes aditivos definidos sobre subconjuntos dos hiperpar√¢metros:
  $$
  \hat{y} = \sum_{U \subseteq N} \hat{f}_{U} \left(\boldsymbol{\theta}_{U}\right)\text{,}
  $$
  em que onde $N \{1, \cdots, n\}$ representa o conjunto de todos os hiperpar√¢metros de um algoritmo $A$, e
  $$
  \boldsymbol{\theta}_U = \left<\theta_{u_{1}}, \dots, \theta_{u_{m}}\right> \text{, com } \theta_{u_i} \in \Theta_{u_i}
  $$
  √© o vetor correspondente a uma configura√ß√£o parcial de hiperpar√¢metros do subconjunto $U = \{u_{1}, \cdots, u_{m}\} \subseteq N$, ou seja, um subconjunto dos √≠ndices dos hiperpar√¢metros.
  + As componentes da fun√ß√£o $\hat{f}_{U}\left(\boldsymbol{\theta_U}\right)$ s√£o definidas da seguinte forma:
  $$
  \hat{f}_{U} \left(\boldsymbol{\theta}_{U}\right)=
  \begin{cases}
    \frac{1}{\|\Theta\|} \int \hat{y} \left(\boldsymbol{\theta}\right) d\boldsymbol{\theta}, & \text{se } U= \emptyset \\
    \hat{a}_{U}\left(\boldsymbol{\theta}_U\right) - \sum_{W \subseteq U} \hat{f}_{W}\left(\boldsymbol{\theta_{W}}\right), & \text{caso contr√°rio}
  \end{cases}\text{,}
  $$

## Import√¢ncia dos hiperpar√¢metros

<br>

- A fun√ß√£o $\hat{a}_{U}\left(\boldsymbol{\theta}_U\right)$ representa uma estimativa da performance marginal do algoritmo $A$ em rela√ß√£o ao subconjunto $U$ dos hiperpar√¢metros, sendo definida por:
  $$
  \hat{a}_{U}\left(\boldsymbol{\theta}_U\right) = \frac{1}{\|\boldsymbol{\Theta}_{T}\|} \int \hat{y}\left(\boldsymbol{\theta}_{N|U}\right)d \boldsymbol{\theta}_{T}
  $$

- Quando $U = \emptyset$:
  + $\hat{f}_{\emptyset}$ corresponde √† m√©dia de $\hat y$‚Äã sobre todo o espa√ßo de hiperpar√¢metros. Representa o valor m√©dio da fun√ß√£o objetivo considerando todas as configura√ß√µes poss√≠veis.
- Efeitos principais $\left(|U| = 1\right)$:
  + As fun√ß√µes un√°rias $\hat{f}_{\{j\}}\left(\boldsymbol{\theta}_{\{j\}}\right)$ quantificam o impacto isolado de cada hiperpar√¢metro em $\hat{y}$‚Äã, considerando a m√©dia do desempenho em todas as poss√≠veis combina√ß√µes dos demais hiperpar√¢metros.
- Efeitos de intera√ß√£o $\left(‚à£U‚à£>1\right)$:
  + As fun√ß√µes $\hat{f}_{U}\left(\boldsymbol{\theta_U}\right)$, com $|U|>1$, representam efeitos de intera√ß√µes entre os hiperpar√¢metros.

## Import√¢ncia dos hiperpar√¢metros

<br>

- A vari√¢ncia da fun√ß√£o objetivo $\hat{y}$ sobre o dom√≠nio dos hiperpar√¢metros $\Theta$ √© definido por:
$$
\mathbb{V} = \frac{1}{\|\boldsymbol{\Theta}\|} \int\left(\hat{y}\left(\boldsymbol{\theta}\right) - \hat{f}_{\emptyset}\right)^2 d \boldsymbol{\theta}\text{,}
$$
e a fANOVA decomp√µe essa vari√¢ncia em contribui√ß√µes associadas a todos os subconjuntos de hiperpar√¢metros:

$$
\mathbb{V} = \sum_{U \subset N}\mathbb{V}_{U} \text{, em que } \mathbb{V}_U = \frac{1}{\|\boldsymbol{\Theta}_{U}\|} \int \hat{f}_U\left(\boldsymbol{\theta}_U\right)^2 d\boldsymbol{\theta}_U\text{.}
$$

- A import√¢ncia relativa de cada efeito principal ou de intera√ß√£o, representado por $\hat{f}_U$‚Äã, pode ent√£o ser quantificada pela fra√ß√£o da vari√¢ncia total que ele explica:
$$
\mathbb{F}_{U} = \mathbb{V}_{U} / \mathbb{V}\text{.}
$$

- Para viabilizar computacionalmente a decomposi√ß√£o da vari√¢ncia, implementa√ß√µes pr√°ticas - como a utilizada na biblioteca Optuna - recorrem a modelos de Random Forest para estimar a fun√ß√£o $\hat{y}$.

# Interpreta√ß√£o dos algoritmos de aprendizagem de m√°quina

## Individual Conditional Expectation (ICE)

<br>

- O ICE (Individual Conditional Expectation) √© um m√©todo que permite analisar o efeito isolado de uma covari√°vel sobre a predi√ß√£o, mantendo as demais vari√°veis constantes.

- No ICE, √© considerado um grid $\{x_{S_i}, x_{C_i}\}_{i=1}^{N}$ de valores no dom√≠nio da vari√°vel de interesse $x_S$, enquanto as demais covari√°veis $x_C$ permanecem fixas.

- A partir das predi√ß√µes $\hat{f}$ geradas ao longo dessa varia√ß√£o, constr√≥i-se um gr√°fico que apresenta:
  + No eixo das ordenadas: os valores estimados ($\hat{f}$),
  + No eixo das abscissas: os valores do grid da vari√°vel $x_S$.

- Para facilitar a interpreta√ß√£o, os valores foram centralizados utilizando o valor m√≠nimo $x^{*}$ de $x_S$, feito da seguinte forma:

$$
\hat{f}_{cent}^{\left(i\right)} = \hat{f}^{\left(i\right)} - \mathbf{1} \hat{f}\left(x^*, \mathbf x_{C_i}\right)\text{,}
$$
em que $x^*$ √© selecionado como o m√≠nimo ou o m√°ximo de $x_S$‚Äã, $\hat f$‚Äã √© o modelo ajustado, e $\mathbf 1$ √© um vetor de uns.

- Tamb√©m foi utilizada a linha do Partial Dependence Plot (PDP), que corresponde √† m√©dia de todas as curvas individuais geradas pelo ICE.

## Local interpretable model-agnostic explanations (LIME)

<br>

- Objetivo: Explicar previs√µes de qualquer modelo complexo, utilizando um modelo localmente interpret√°vel.

- Ideia central:
  + Aproximar o comportamento do modelo complexo $f$ por meio de um modelo explicativo $g$ (por exemplo, regress√£o linear ou √°rvore de decis√£o), somente na vizinhan√ßa local de uma inst√¢ncia espec√≠fica $x$.

- Passos b√°sicos:
  + Amostras sint√©ticas pr√≥ximas de $x$.
  + Obter as predi√ß√µes do modelo complexo $f$ sobre essas amostras.
  + Ajustar o modelo interpret√°vel $g$ sobre esse conjunto sint√©tico, ponderando as amostras de acordo com a dist√¢ncia a x (medida de localidade $\pi_x$).
  + Minimizar a fun√ß√£o de perda $L\left(f, g, \pi_x\right)$, mantendo a complexidade de g $\left(\Omega\left(g\right)\right)$ baixa.
    + Quando $g$ √© uma √°rvore de decis√£o, a penalidade $\Omega\left(g\right)$ pode, por exemplo, ser definida pela profundidade da √°rvore.

- Formula√ß√£o matem√°tica da explica√ß√£o gerada pelo LIME:
$$
\xi\left(x\right) = \arg \min_{g \in G} L\left(f, g, \pi_x\right) + \Omega\left(g\right)\text{.}
$$


## Shapley Additive Explanations (SHAP)

<br>

-  O SHAP (Shapley Additive Explanations) √© um m√©todo que busca explicar as predi√ß√µes individuais de um modelo, atribuindo a cada vari√°vel uma contribui√ß√£o para o valor predito‚Äã

- Baseia-se nos valores de Shapley da teoria dos jogos de coaliz√£o, introduzidos por @shapley1953value, que representam a contribui√ß√£o m√©dia de cada vari√°vel considerando as poss√≠veis combina√ß√µes (coaliz√µes) de covari√°veis.

- F√≥rmula dos Valores de Shapley:
  + Os valores de Shapley s√£o dados por:
$$
\phi_i\left(x\right) = \sum_{Q \subseteq S | \{i\}} \frac{|Q|!\left(|S| - |Q| - 1\right)!}{|S|!} \left(\Delta_{Q\cup \{i\}}\left(x\right) - \Delta_{Q}\left(x\right)\right)\text{,}
$$
  + em $Q$ √© um subconjunto de vari√°veis, $S$ o conjunto completo, e o $\Delta_{Q \cup \{i\}}\left(x\right) - \Delta_{Q}\left(x\right)$ corresponde √† contribui√ß√£o marginal da vari√°vel $i$ ao ser adicionada ao subconjunto $Q$.

## Shapley Additive Explanations (SHAP)

<br>

- Modelo Aditivo:
  + O SHAP representa a explica√ß√£o como um modelo linear aditivo:
$$
g\left(\mathbf{z^{'}}\right) = \phi_0 + \sum_{j = 1}^{M} \phi_{j} z_{j}^{'}\text{,}
$$
em que $\mathbf{z^{'}} = \left(z_{1}^{'}, \cdots, z^{'}_{M}\right)^T \in \{0, 1\}^{M}$, representando a presen√ßa $\left(z^{'}_j = 1\right)$ ou aus√™ncia $\left(z^{'}_j = 0\right)$ de cada covari√°vel, $M$ √© o tamanho m√°ximo da coaliz√£o e $\phi_j$ denota os valores Shapley.

## Gr√°ficos de agrega√ß√£o com SHAP

<br>

- **Import√¢ncia das covari√°veis com SHAP:**
  + A ideia central do SHAP para medir a import√¢ncia das vari√°veis independentes √© que vari√°veis com maiores valores absolutos de Shapley contribuem mais significativamente para as predi√ß√µes.
  + Assim, a import√¢ncia de uma vari√°vel $j$ √© definida como a m√©dia dos valores absolutos de Shapley ao longo de todas as observa√ß√µes:
  $$
  I_j = \frac{1}{n}\sum^{n}_{i = 1} |\phi_j^{\left(i\right)}|
  $$

- **Gr√°fico de resumo:**
  + O gr√°fico de resumo combina a ideia da import√¢ncia das vari√°veis com SHAP e os valores Shapley.
    - No eixo das abscissas s√£o inclu√≠dos os valores Shapley
    - No eixo das ordenadas s√£o inclu√≠dos as vari√°veis, que s√£o ordenadas por ordem de import√¢ncia
    - A cor representa os valores das vari√°veis de baixo para alto

## Gr√°ficos de agrega√ß√£o com SHAP

<br>

- **Gr√°fico de depend√™ncia:**
  + Os valores de Shapley de uma determinada covari√°vel s√£o representados em fun√ß√£o dos valores observados dessa vari√°vel.
  + A rela√ß√£o √© dada pelo conjunto de pares:
  $$
  \{\left(x^{\left(i\right)}_j, \phi^{\left(i\right)}_j\right)\}^{n}_{i=1}
  $$
  + Esse gr√°fico permite visualizar como os diferentes valores da vari√°vel influenciam a predi√ß√£o, mantendo todas as demais vari√°veis constantes.

# Resultados

## An√°lise explorat√≥ria

::: {columns}
::: {.column width="50%"}
- As vari√°veis com dados ausentes foram: banheiro, condom√≠nio, IPTU, quarto, vaga, valor e √°rea de aluguel.

- Condom√≠nio e IPTU apresentaram mais de 60% de valores ausentes e, por isso, foram exclu√≠das da modelagem.

- As demais vari√°veis com valores faltantes foram imputadas por meio do m√©todo KNN, com 17 vizinhos.
:::

::: {.column width="50%"}
![Propor√ß√£o de valores ausentes por vari√°veis.](valores_ausentes.png){#fig-aus fig-align="center"}
:::
:::

::: {columns}

::: {.column width="50%"}
![Varia√ß√£o da m√©dia do valor do $m^2$ dos im√≥veis de Jo√£o Pessoa.](map_valor_m2.svg){#fig-mapa fig-align="center"}
:::

::: {.column width="50%"}
- Alguns bairros apresentaram poucos ou nenhum im√≥vel registrado, como indicado pelas √°reas em cinza no mapa.

- O bairro Barra de Gramame, por exemplo, possui apenas um im√≥vel registrado.

- Os maiores valores m√©dios por metro quadrado foram observados em Cabo Branco (R$ 10.000+), seguido por Tamba√∫ (R$ 8.951,45), Jardim Oceania (R$ 7.879,60) e Altiplano Cabo Branco (R$ 7.218,70).

:::
:::

## An√°lise explorat√≥ria

<br>

::: {columns}

::: {.column width="50%"}
- A maioria dos im√≥veis possuem 2 ou nenhuma vaga de garagem

- No caso da vari√°vel de banheiro, a maioria dos tipos de im√≥veis possuem 1 a 3 banheiros

- A maioria dos im√≥veis possuem uma concentra√ß√£o entre 1 ou 2 quartos

- A maioria das vari√°veis num√©ricas apresenta distribui√ß√£o assim√©trica √† direita, ou seja, com cauda longa em dire√ß√£o a valores mais altos.
:::

::: {.column width="50%"}
![Distribui√ß√£o das vari√°veis num√©ricas.](violin_plot.png){#fig-violin_plot fig-align="center"}
:::

:::

::: {columns}

::: {.column width="50%"}
![Compara√ß√£o entre distribui√ß√£o dos valores dos im√≥veis antes e depois da
transforma√ß√£o logar√≠tmica.](densidade.png){#fig-dens_plot fig-align="center"}
:::

::: {.column width="50%"}
- A distribui√ß√£o original dos valores dos im√≥veis apresenta assimetria positiva, com uma concentra√ß√£o de im√≥veis de valor mais baixo e uma cauda longa √† direita representando poucos im√≥veis com valores extremamente altos.

-  O gr√°fico √† direita demonstra a estabiliza√ß√£o da vari√¢ncia da distribui√ß√£o dos valores de im√≥veis ap√≥s uma transforma√ß√£o $\log\left(1 + x\right)$.
:::

:::

## An√°lise explorat√≥ria

<br>

::: {columns}

::: {.column width="50%"}
![Gr√°fico de correla√ß√£o de Spearman das vari√°veis independentes.](corr.png){fig-align="center"}
:::

::: {.column width="50%"}
- O valor de im√≥vel apresenta maior correla√ß√£o com as vari√°veis de √°rea do im√≥vel e n√∫mero de vagas de estacionamento.

- O valor de im√≥vel tamb√©m tem alta correla√ß√£o com valor m√©dio do aluguel por bairro, n√∫mero de quartos e banheiros, latitude e longitude.
:::

:::

- Com base no gr√°fico de correla√ß√£o, foram criadas novas vari√°veis derivadas da intera√ß√£o entre vari√°veis com maior correla√ß√£o:
  + produto entre o n√∫mero de quartos do im√≥vel e a √°rea m√©dia do aluguel no bairro;
  + a raz√£o entre o n√∫mero de quartos e a √°rea do im√≥vel;
  + o produto entre as coordenadas geogr√°ficas e o pre√ßo m√©dio do aluguel no bairro;
  + a quantidade total de c√¥modos do im√≥vel, obtida pela soma do n√∫mero de quartos e banheiros, acrescida de dois, considerando a presen√ßa de sala e cozinha.


## Tunagem dos modelos

- Random Forest
  + Foram ajustados os hiperpar√¢metros referentes √† quantidade de √°rvores e √† profundidade m√°xima das √°rvores. Al√©m disso, foram consideradas $m = \sqrt{p}$‚Äã covari√°veis como candidatas para as divis√µes.

- Gradient Boosting
  +  A tunagem incluiu a quantidade de √°rvores, a profundidade m√°xima e a taxa de aprendizado. Foram consideradas $m= \sqrt{p}$ covari√°veis como candidatas para as divis√µes.

- Light Gradient Boosting Machine (LightGBM)
  + Foram ajustados os hiperpar√¢metros de quantidade de √°rvores, profundidade m√°xima, taxa de aprendizado e n√∫mero m√°ximo de folhas por √°rvore.

- Extreme Gradient Boosting (XGBoost)
  + Os hiperpar√¢metros tunados foram: quantidade de √°rvores, profundidade m√°xima das √°rvores e taxa de aprendizado.

::: {#tbl-tuning .smaller}

| Tentativa |   RMSE  | n\_estimators | max\_depth | learning\_rate | num\_leaves |           Modelo          |
| :-------: | :-----: | :-----------: | :--------: | :------------: | :---------: | :-----------------------: |
|     62    | 0,29084 |      653      |     21     |        ‚Äì       |      ‚Äì      |       Random Forest       |
|     57    | 0,28512 |      1404     |      7     |     0,09806    |      ‚Äì      |     Gradient Boosting     |
|     31    | 0,28546 |      1988     |     379    |     0,00985    |     202     |            LGBM           |
|     81    | 0,28528 |      788      |      8     |     0,07119    |      ‚Äì      | XGBoost |

: Combina√ß√£o de hiperpar√¢metros encontrado pelo TPE.

:::

## Tunagem dos modelos

<br>

::: {layout-ncol="2"}
![Resultados da tunagem da Random Forest.](tunagem_rf.png){#fig-tuning_rf}

![Resultados da tunagem do Gradient Boosting.](tunagem_gbdt.png){#fig-tuning_gbdt}
:::

## Tunagem dos modelos

<br>

::: {layout-ncol="2"}
![Resultados da tunagem do LGBM.](tunagem_lgbm.png){#fig-tuning_lgbm}

![Resultados da tunagem do XGBoost.](tunagem_xgb.png){#fig-tuning_xgb}
:::


## Resultado dos modelos

<br>

::: {#tbl-metrics_models}

|           Algoritmo          |   RMSE  |   $R^2$   |   MAPE  |
|:-------------------------:|:-------:|:---------:|:-------:|
|     Gradient Boosting     | 0,28297 | $87,32712\%$ | 0,01333 |
|          Stacking         | 0,28382 | $87,26007\%$ | 0,01340 |
|  Light Gradient Boosting  | 0,28385 | $87,25794\%$ | 0,01340 |
| Extreme Gradient Boosting | 0,28400 | $87,24599\%$ | 0,01325 |
|       Random Forest       | 0,28578 | $87,10419\%$ | 0,01346 |

: M√©tricas obtidas de cada algoritmo, ap√≥s o ajuste dos hiperpar√¢metros.

:::

- Melhor desempenho geral ‚Äî Gradient Boosting:
  + Obteve o menor RMSE (0,28297), indicando menor erro quadr√°tico m√©dio nas previs√µes.

  + Tamb√©m apresentou o maior R2R2 (87,33%), o que sugere maior capacidade explicativa em rela√ß√£o √† variabilidade dos dados.

  + Teve o menor MAPE (0,01333), o que refor√ßa sua superioridade na acur√°cia percentual das previs√µes.

  +  Todos os modelos obtiveram desempenho semelhante, mas o Gradient Boosting se destacou como o mais eficaz, ainda que por margens pequenas.

## Resultado dos modelos

![Valores previstos em fun√ß√£o dos observados do algoritmo **Random Forest** e
**Gradient Boosting**, respectivamente.](gbdt_rf.png){#fig-gbdt_rf fig-align="center"}

## Resultado dos modelos

![Valores previstos em fun√ß√£o dos observados do algoritmo **Light Gradient
Boosting** e **Extreme Gradient Boosting**, respectivamente.](lgbm_xgb.png){#fig-lgbm_xgb fig-align="center"}

## Resultado dos modelos

![Valores previstos em fun√ß√£o dos observados do algoritmo Stacking.](preds_stacking.png){#fig-preds_stacking}

## Impacto e import√¢ncia das vari√°veis na predi√ß√£o

![Gr√°fico de ICE e PDP.](pdp_ice.png){#fig-pdp_ice fig-align="center"}

## Impacto e import√¢ncia das vari√°veis na predi√ß√£o

<br>

![Impacto e import√¢ncia das vari√°veis na predi√ß√£o a partir do m√©todo SHAP.](summary_vars.png){#fig-import fig-align="center"}

## Impacto e import√¢ncia das vari√°veis na predi√ß√£o

![Gr√°fico de depend√™ncia de valores SHAP para vari√°veis bin√°rias.](depe_plots.png){#fig-dep_plot fig-align="center"}

# Aplica√ß√£o Web

# Refer√™ncias

::: {#refs}
:::
