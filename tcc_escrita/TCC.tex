% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
  a4paper,
]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\usepackage[left=3cm,,right=2cm,,top=3cm,,bottom=2cm]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{\underline{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{\textbf{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.44,0.26,0.76}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.14,0.16,0.18}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.14,0.16,0.18}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.44,0.26,0.76}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.84,0.23,0.29}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.42,0.45,0.49}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.36,0.77}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.89,0.38,0.04}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.01,0.18,0.38}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{1.00,0.33,0.33}{#1}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{dirtree}
\usepackage{amsmath}
\usepackage[explicit]{titlesec}
\usepackage{setspace}
\usepackage{caption}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{etoolbox}
\usepackage[shortlabels]{enumitem}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{pdflscape}
\usepackage{fancyhdr}

\newcommand{\blandscape}{\begin{landscape}}
\newcommand{\elandscape}{\end{landscape}}
\renewcommand{\footnotesize}{\scriptsize}
\renewcommand{\footnoterule}{\noindent\rule{5cm}{0.4pt}\vspace{0.2cm}}
\setlength{\footnotesep}{0.5em}

\setlength{\parskip}{0.0em}
\AtBeginEnvironment{quote}{\setstretch{1.0}}

\titleformat{\section}{\normalfont\large\bfseries}{}{0pt}{\thesection\quad#1}[]
\titleformat{\subsection}{\normalfont\normalsize\bfseries}{}{0pt}{\thesubsection\quad#1}[]
\titleformat{\subsubsection}{\normalfont\normalsize\itshape}{}{0pt}{#1}[]
\titleformat{\paragraph}[runin]{\normalfont\normalsize\bfseries}{}{0pt}{#1}[]
\titleformat{\subparagraph}[runin]{\normalfont\normalsize\itshape}{}{0pt}{#1}[]

\titlespacing*{\section}{0pt}{20pt}{10pt}
\titlespacing*{\subsection}{0pt}{15pt}{10pt}
\titlespacing*{\subsubsection}{0pt}{10pt}{10pt}
\titlespacing*{\paragraph}{0pt}{10pt}{10pt}
\titlespacing*{\subparagraph}{0pt}{10pt}{10pt}
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{algo}{h}{loalgo}}{\newfloat{algo}{h}{loalgo}[chapter]}
\floatname{algo}{Algoritmo}
\newcommand*\listofalgos{\listof{algo}{List of Algoritmos}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Índice}
\else
  \newcommand\contentsname{Índice}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{Lista de Figuras}
\else
  \newcommand\listfigurename{Lista de Figuras}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{Lista de Tabelas}
\else
  \newcommand\listtablename{Lista de Tabelas}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figura}
\else
  \newcommand\figurename{Figura}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Tabela}
\else
  \newcommand\tablename{Tabela}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listagem}
\newcommand*\listoflistings{\listof{codelisting}{Lista de Listagens}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}{}
\makeatother
\makeatletter
\@ifundefined{codebgcolor}{\definecolor{codebgcolor}{HTML}{F0F2F4}}{}
\makeatother
\makeatletter
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[breakable, boxrule=0pt, colback={codebgcolor}, frame hidden, sharp corners, enhanced]}{\end{tcolorbox}}\fi
\makeatother
\makeatletter
\@ifpackageloaded{algorithm}{}{\usepackage{algorithm}}
\makeatother
\makeatletter
\@ifpackageloaded{algpseudocode}{}{\usepackage{algpseudocode}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\makeatother

\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{brazilian}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Uso de Aprendizagem de Máquina na Modelagem Preditiva de Valores Imobiliários no Mercado Imobiliário da Cidade de João Pessoa},
  pdfauthor={Gabriel de Jesus Pereira},
  pdflang={pt-br},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Uso de Aprendizagem de Máquina na Modelagem Preditiva de Valores
Imobiliários no Mercado Imobiliário da Cidade de João Pessoa}
\author{Gabriel de Jesus Pereira}
\date{março, 2025}

\begin{document}
\cleardoublepage
\thispagestyle{empty}
{\centering
\noindent\rule{\textwidth}{0.5pt}

\vspace{2ex}

{\Large\bfseries Universidade Federal da Paraíba \par}
\vspace{1ex}
{\Large\bfseries Centro de Ciências Exatas e da Natureza \par}
\vspace{1ex}
{\Large\bfseries Departamento de Estatística \par}

\vfill

{\large\bfseries Uso de Aprendizagem de Máquina na Modelagem Preditiva
de Valores Imobiliários no Mercado Imobiliário da Cidade de João
Pessoa \par}

\vfill

{\large Gabriel de Jesus Pereira \par}
\vfill
{\normalsize março, 2025 \par}


\noindent\rule{\textwidth}{0.5pt}

\newpage
\thispagestyle{empty}

{\normalsize\bfseries Gabriel de Jesus Pereira \par}

\vfill

{\large\bfseries Uso de Aprendizagem de Máquina na Modelagem Preditiva
de Valores Imobiliários no Mercado Imobiliário da Cidade de João
Pessoa \par}

\vfill

\hfill \parbox{8cm}{\normalsize{Trabalho de conclusão de curso apresentado ao Curso de Bacharelado em Estatística, do Departamento de Estatística, do Centro de Ciências Exatas e da Natureza, da Universidade Federal da Paraíba, como requisito para obtenção do título de Bacharel em Estatística.}}

\vfill

{\normalsize Orientador: Prof. Dr. Pedro Rafael Diniz Marinho}

\vfill

{\normalsize\bfseries João Pessoa \par}
{\normalsize\bfseries março, 2025 \par}

}
\numberwithin{algorithm}{chapter}
\algrenewcommand{\algorithmiccomment}[1]{\hskip3em$\rightarrow$ #1}

\floatname{algorithm}{Algoritmo}

\renewcommand*\contentsname{\centering Sumário \thispagestyle{empty}}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\pagenumbering{arabic}
\pagestyle{fancy}

\fancyhf{}
\fancyhead[RO, LE]{\thepage}
\fancyhead[LO]{\leftmark}
\fancyhead[RE]{\thepage}

\fancypagestyle{plain}{
  \pagestyle{fancy}
  \fancyhf{}
  \fancyhead[RO, LE]{\thepage}
  \fancyhead[RE]{\thepage}
  \renewcommand{\headrulewidth}{0pt}
}

\thispagestyle{empty}

\newpage

\bgroup
\hypersetup{linkcolor = black}

\cleardoublepage
\renewcommand{\listfigurename}{\centering{Lista de Figuras}}
\listoffigures
\thispagestyle{empty}

\cleardoublepage
\renewcommand{\listtablename}{\centering{Lista de Tabelas}}
\listoftables
\thispagestyle{empty}

\cleardoublepage
\renewcommand{\listalgorithmname}{\centering{Lista de Algoritmos}}
\listofalgorithms
\thispagestyle{empty}

\egroup

\chapter*{\centering Agradecimentos}
\thispagestyle{empty}

\chapter*{\centering Resumo}
\thispagestyle{empty}

~~~A história do Brasil, assim como a de muitos outros países, foi
marcada por uma predominância rural durante grande parte de sua
trajetória, com apenas 31\% da população vivendo em áreas urbanas em
1940. Contudo, esse cenário mudou rapidamente e, em 1970, mais da metade
da população já residia em cidades (WAGNER; WARD, 1980). O processo de
urbanização acelerado e desordenado resultou no crescimento
desorganizado das grandes metrópoles, agravando problemas urbanos como o
déficit habitacional. Com o aumento das áreas urbanizadas, a demanda por
habitação tornou-se cada vez maior, levando à criação de medidas para
mitigar esse déficit. Nesse contexto, a Lei nº 4.380, de 21 de agosto de
1964, instituiu pela primeira vez no país um mecanismo de crédito
habitacional, sendo fundamental para articular a oferta e a demanda de
recursos destinados ao setor habitacional. Essa legislação resultou na
criação do Sistema Financeiro de Habitação (SFH), que trouxe inovações
como a correção monetária, o Banco Nacional da Habitação (BNH) e as
Sociedades de Crédito Imobiliário (SCI). Apesar dos avanços do SFH, o
aumento da inflação na década de 1980 impactou seu funcionamento,
agravado por medidas governamentais para conter a inflação. Como
evolução desse sistema, foi criado o Sistema de Financiamento
Imobiliário (SFI), que não substituiu o SFH, mas introduziu um modelo
moderno de financiamento habitacional, fundamentado na securitização de
créditos imobiliários, maior segurança jurídica dos contratos e captação
de recursos diretamente no mercado por meio de operações realizadas por
entidades autorizadas. A dinâmica do tempo e a modernização do mercado
imobiliário trouxe impacto significativo para diversas cidades
brasileiras, incluindo João Pessoa, capital da Paraíba, que se tornou um
dos destinos mais procurados para viver e visitar. A cidade destacou-se
como uma das capitais com maior valorização acumulada de imóveis
residenciais nos últimos 12 meses, até novembro de 2024, apresentando
uma variação de 16,13\%. O crescimento do mercado imobiliário em João
Pessoa tem gerado novas demandas, incluindo a necessidade de métodos
mais precisos para precificação de imóveis. Dessa forma, este trabalho
teve como objetivo desenvolver uma modelagem estatística para
precificação de valores imobiliários na cidade de João Pessoa,
utilizando dados coletados por meio de raspagem de sites especializados.
Além da modelagem, buscou-se também analisar o impacto das variáveis
utilizadas na estimativa dos preços dos imóveis, empregando técnicas de
aprendizagem de máquina como base metodológica. O modelo final foi
construído utilizando o algoritmo Stacking e apresentou resultados
satisfatórios, com um \(R^2\) de 87,19\%, erro percentual absoluto médio
(MAPE) de 1,357\% e raiz do erro quadrático médio (RMSE) de 0,28473. A
análise das variáveis revelou que a área do imóvel foi o fator mais
determinante na precificação, seguida pela quantidade de vagas de
garagem, valor médio de aluguel do bairro, coordenadas geográficas,
número de quartos e banheiros, e área média de aluguel do bairro. Por
fim, foi desenvolvida uma aplicação prática para facilitar a
precificação e a análise de imóveis em João Pessoa, contribuindo para
maior precisão no processo de avaliação imobiliária.

\begin{flushleft}
\textbf{Palavras-chave}: Aprendizagem de máquina, mercado imobiliário, modelagem estatística, João Pessoa.
\end{flushleft}

\chapter{Introdução}\label{introduuxe7uxe3o}

~~~Assim como em todos os países, a maior parte da história do Brasil
foi marcada por uma predominância rural. Em 1940, apenas 31\% da
população vivia em áreas urbanas. No entanto, esse número cresceu
rapidamente, ultrapassando 50\% em 1970 (WAGNER; WARD, 1980). Essa
tendência se manteve nas décadas seguintes, até que, em 2014, 85,1\% da
população brasileira já residia em regiões urbanas.

\vspace{12pt}

No Brasil, o processo de urbanização ocorreu de forma tardia e
desordenada, quando comparado com os países pioneiros da Revolução
Industrial, que tiveram todo um planejamento para suportar a grande
transição demográfica que a população estava passando naquele momento.
Um dos principais fatores que impulsionaram essa rápida urbanização, no
Brasil, foram os chamados fatores de repulsão, característicos de nações
em desenvolvimento. Esses fatores estão ligados às precárias condições
de vida no meio rural, decorrentes da estrutura fundiária concentrada,
dos baixos salários, desemprego e mecanização do campo. Como
consequência, ocorreu o êxodo rural\footnote{Processo de transferência
  em larga escala da população do campo para as cidades.}, que resultou
no crescimento desordenado das grandes metrópoles e no agravamento de
problemas urbanos, como déficit habitacional, precariedade nos serviços
públicos e aumento das desigualdades sociais.

\vspace{12pt}

O êxodo rural no Brasil foi um processo gradual, mas ocorreu de forma
acelerada quando comparado a outros países, atingindo seu auge entre
1960 e 1980. No entanto, foi durante o governo de Getúlio Vargas, na
década de 1930, com o início do processo de industrialização, que
surgiram as condições específicas para o aumento do êxodo rural, mas
ainda era bastante limitado. Na década de 1950, o processo de
urbanização se intensificou com a industrialização impulsionada pelo
governo de Juscelino Kubitschek. Kubitschek implementou o Plano de
Metas, que tinha como principal objetivo modernizar e desenvolver a
infraestrutura do país. O Plano de Metas incluia investimentos estatais
em setores estratégicos como agricultura, saúde, educação, energia,
transporte, mineração e construção civil. Além dessas estratégias, o
plano também previa a transferência da capital federal do Rio de Janeiro
para Brasília, visando promover a ocupação e o desenvolvimento do
interior do país.

\vspace{12pt}

Com os incentivos gerados pelo processo de industrialização e
urbanização no Brasil, a demanda por habitação aumentou
significativamente. Nesse cenário, surgiu a necessidade de se
estabelecer, ainda que tardiamente, um sistema imobiliário no país. A
criação desse sistema foi iniciada pela Lei nº 4.380, de 21 de agosto de
1964, que instituiu o Sistema Financeiro de Habitação (SFH). O sistema
propiciou diversas inovações, como a correção monetária, a criação do
Banco Nacional da Habitação (BNH) e as Sociedades de Crédito Imobiliário
(SCI). Além disso, a legislação estabeleceu, pela primeira vez, um
mecanismo de crédito habitacional capaz de articular a oferta e a
demanda de recursos necessários para a realização de investimentos
habitacionais.

\vspace{12pt}

O BNH tinha como objetivo oferecer incentivos ao mercado imobiliário,
promovendo a poupança no país, além de atrair o mercado privado e
regulamentar as condições de financiamento do Sistema Financeiro da
Habitação (SFH), como garantias, prazos e taxas praticadas no sistema
(ASSUMPÇÃO FILHO, 2011). As SCIs, subordinadas ao BNH, atuavam como
agentes financeiros e eram restritas a operar exclusivamente no
financiamento para construção, venda ou aquisição de bens destinados à
habitação. Nesse período, foi criado o Fundo de Garantia do Tempo de
Serviço (FGTS), uma forma de poupança compulsória que, junto à caderneta
de poupança, tornou-se a principal fonte de financiamento habitacional
no Brasil. As captações voluntárias realizadas por meio das cadernetas
de poupança compõem o Sistema Brasileiro de Poupança e Empréstimo
(SBPE), que reúne as instituições responsáveis pela captação de recursos
livres, como as Sociedades de Crédito Imobiliário e as Associações de
Poupança e Empréstimo.

\vspace{12pt}

A partir da década de 80 com o aumento da inflação no país, o SFH começa
a ser afetado, principalmente com as ações tomadas pelos governos para
combater a inflação. Em 1985, o reajuste das prestações foi de 112\% e a
inflação acumulada já alcançava os 246\%, porcentual aplicado na
correção dos saldos devedores (ABECIP, 2007a). Acuado pela inflação, os
governos passados deram início a uma série de planos heterodoxos.

\vspace{12pt}

O Plano Cruzado, implementado em 1986, converteu o valor das prestações
pela média dos 12 meses anteriores e, em seguida, congelou os reajustes
pelos 12 meses seguintes. Essa medida atingiu a totalidade dos contratos
e resultou em uma redução de cerca de 40\% no valor das prestações. No
entanto, a longo prazo, essa política afetou os contratos e os deixou
mais caros. Nos anos seguintes, novas medidas foram implementadas. Em
1987 e 1989, as prestações foram congeladas temporariamente pelo Plano
Bresser e pelo Plano Verão, respectivamente.

\vspace{12pt}

No Governo Collor, a medida mais prejudicial ao SFH foi o Plano Collor
I, de 1990, que bloqueou todos os ativos financeiros e 60\% do saldo das
cadernetas de poupança, a principal fonte de arrecadação para o setor. O
saldo da poupança na época correspondia a US\$ 30 bilhões. Dos 40\%
restantes, cerca de metate foi retirado pelos depositantes, uma vez que
a população ficou sem dinheiro disponível para arcar com despesas
correntes (ABECIP, 2015). Como consequência, o saldo das cadernetas de
poupança foi drasticamente reduzido, atingindo aproximadamente US\$ 7 a
US\$ 8 bilhões. Essa medida agravou intensamente a situação das
instituições financeiras, que, de repente, ficaram sem passivo e ficaram
com o ativo integral.

\vspace{12pt}

De acordo com a ABECIP (2007b), durante o período de auge do SFH, entre
1978 e 1982, o investimento habitacional por habitante manteve-se em
torno de R\$ 500. Contudo, com as políticas econômicas adotadas pelos
governos para combater a inflação, o investimento em habitação recuou e
retornou ao patamar de R\$ 300. Com a criação do Plano Real, em 1994,
observou-se uma pequena recuperação, embora ele permanecesse abaixo dos
R\$ 500 no auge.

\vspace{12pt}

Como consequência da crise econômica que marcou o período entre 1980 e
1990, o arrocho salarial, a queda do poder aquisitivo, as altas taxas de
juros e a inflação contribuíram significativamente para o aumento da
inadimplência no SFH. Em 1994, a taxa de inadimplência estava próxima de
9\%, enquanto em 2005 já havia se aproximado de 30\% (COSTA FARIAS,
2010). Diante desse cenário, surgiram novos esforços e iniciativas para
contornar a crise e reformular o modelo de financiamento habitacional no
Brasil.

\vspace{12pt}

Após a experiência acumulada com o SFH, a principal medida para
reformular o modelo de financiamento habitacional no Brasil foi a
criação do Sistema de Financiamento Imobiliário (SFI), que não
significou o fim do SFH. O SFI foi instituído em 1997, pela Lei nº
9.514, como um complemento ao SFH.

\vspace{12pt}

Os principais fundamentos do SFI são a securitização dos créditos
imobiliários e a maior segurança jurídica dos contratos. Diferentemente
do SFH, o novo sistema capta recursos diretamente no mercado por meio de
operações realizadas por entidades autorizadas, como caixas econômicas,
bancos comerciais, bancos de investimento, bancos com carteira de
crédito imobiliário, sociedades de crédito imobiliário, associações de
poupança e empréstimo e companhias hipotecárias. Essas entidades podem
aplicar os recursos utilizando instrumentos financeiros introduzidos com
o SFI, tais como o Certificado de Recebíveis Imobiliários (CRI), a Letra
de Crédito Imobiliário (LCI) e a Cédula de Crédito Imobiliário (CCI). A
segurança jurídica dos contratos passou a ser garantida pela introdução
da alienação fiduciária, um mecanismo que trouxe maior confiança e
eficiência ao processo de financiamento imobiliário. Assim, o SFI
representa a efetiva modernização do mercado imobiliário no País.

\vspace{12pt}

O mercado imobiliário tem demonstrado grande potencial em diversas
capitais brasileiras, com destaque para João Pessoa, capital da Paraíba,
que se tornou um dos destinos mais procurados para se viver devido à sua
elevada qualidade de vida. João Pessoa foi fundada no atual bairro do
Varadouro, às margens do Rio Sanhauá, por Martim Leitão e colonos vindos
de Pernambuco, no dia 4 de novembro de 1585. Ao longo de sua história, a
cidade recebeu três nomes antes de adotar sua nomenclatura atual, Nossa
Senhora das Neves (1585), Frederickstadt (1634-1654) e Parahyba, nome
que permaneceu até 1930. Em julho desse ano, a cidade foi renomeada como
João Pessoa, em homenagem ao governador do estado da época. Durante o
período colonial, a cidade era dividida em Cidade Alta e Cidade Baixa,
interligadas por ladeiras (CAMPOS, 2014).

\vspace{12pt}

Atualmente, a cidade é detentora de um território de 211.475 \(km^2\) e
possui uma população de 833.932 habitantes, de acordo com o censo do
IBGE 2022. Segundo o FIPE -- FUNDAÇÃO INSTITUTO DE PESQUISAS ECONÔMICAS
(2024), João Pessoa apresentou a maior valorização acumulada nos imóveis
residenciais nos últimos 12 meses, até novembro de 2024, com uma
variação de 16,13\%. No mesmo período, considerando apenas o ano de
2024, a valorização acumulada foi de 15,15\%, com o valor médio do metro
quadrado residencial atingindo R\$ 6.867 no mês de novembro.

\vspace{12pt}

O crescimento do mercado imobiliário de João Pessoa trás consigo
inúmeras necessidades de melhor precificação de imóveis. Avaliar
corretamente o valor de um imóvel é crucial para uma série de
finalidades, como a negociação justa entre compradores e vendedores e a
determinação de valores tributários, como, por exemplo, o Imposto de
Transmissão de Bens Imóveis (ITBI). Assim, a determinação do valor de um
imóvel continua sendo um desafio, muitas vezes dependente de avaliações
subjetivas ou métodos tradicionais que nem sempre refletem a
complexidade dos fatores envolvidos.

\vspace{12pt}

Diante do cenário e dos desafios apresentados, este trabalho tem como
objetivo analisar e modelar os valores de diversos tipos de imóveis na
cidade de João Pessoa. Além disso, por meio do estudo e dos dados
utilizados em seu desenvolvimento, busca-se identificar os principais
fatores que influenciam os preços dos imóveis e criar modelos preditivos
capazes de auxiliar no processo de avaliação imobiliária, com potencial
aplicação em atividades como o cálculo do ITBI. Além disso, o trabalho
propõe analisar e descrever diferentes algoritmos de aprendizado de
máquina, explorando suas características e desempenho no contexto
proposto. Por fim, será desenvolvida uma aplicação computacional com
múltiplas finalidades, visando servir como ferramenta para avaliação de
imóveis e outros usos relacionados ao mercado imobiliário.

\section{Objetivos}\label{objetivos}

\subsection{Objetivo Geral}\label{objetivo-geral}

~~~Realizar uma análise e modelagem do valor de imóveis na cidade de
João Pessoa, utilizando ferramentas computacionais e técnicas de
aprendizagem de máquina, com o intuito de compreender os fatores que
influenciam e os quais mais impactam na predição desses valores.

\subsection{Objetivos Específicos}\label{objetivos-especuxedficos}

\begin{itemize}
\item
  Definir um modelo para a predição de imóveis da cidade de João Pessoa
  para ajudar na tomada de decisão de avaliação de imóveis.
\item
  Desenvolver uma aplicação prática e interativa que permita a entrada
  de características dos imóveis e forneça estimativas de seus valores
  com base nos modelos construídos.
\item
  Criar visualizações e relatórios que facilitem a interpretação dos
  resultados e apoiem a tomada de decisões no mercado imobiliário.
\end{itemize}

\section{Organização do Trabalho}\label{organizauxe7uxe3o-do-trabalho}

Este trabalho é composto por seis capítulos, incluindo este capítulo
introdutório, que apresenta um panorama histórico do mercado
imobiliário, a região de estudo e os objetivos finais da pesquisa. No
Capítulo 2, são detalhadas as ferramentas computacionais utilizadas, as
linguagens de programação empregadas, as bibliotecas adotadas e o
processo de coleta de dados realizado por meio da técnica de raspagem de
dados.

\vspace{12pt}

O Capítulo 3 descreve teoricamente os algoritmos utilizados na modelagem
dos valores de imóveis, como Random Forest, Boosting, Gradient Boosting
e, por fim, o algoritmo de Stacking, que combina os algoritmos
anteriores para sua construção.

\vspace{12pt}

No Capítulo 4, é detalhado o processo de obtenção dos dados e a
descrição das variáveis. Além disso, são explicadas as etapas de
construção dos modelos, as transformações aplicadas às variáveis, e o
processo de otimização dos hiperparâmetros por meio de validação cruzada
e técnicas de otimização. Por fim, são descritas as técnicas de análise
do comportamento das predições, incluindo Individual Conditional
Expectation (ICE), Local Interpretable Model-Agnostic Explanations
(LIME) e Shapley Additive Explanations (SHAP).

\vspace{12pt}

O Capítulo 5 apresenta os resultados da análise exploratória, o ajuste
dos modelos, bem como os efeitos e a importância das variáveis na
predição. Por fim, o Capítulo 6 discute os resultados obtidos ao longo
do trabalho, destacando o desempenho dos modelos e as relações e
impactos das variáveis na previsão dos valores dos imóveis. Além disso,
são descritos o funcionamento e as funcionalidades da aplicação final
desenvolvida.

\chapter{Recursos Computacionais}\label{recursos-computacionais}

~~~Nesta seção, serão apresentados os recursos computacionais utilizados
no desenvolvimento deste trabalho. As ferramentas selecionadas incluem
linguagens de programação amplamente conhecidas, como \textbf{Python} e
\textbf{R}, e sistemas de publicação técnica, como o \texttt{Quarto}.
Cada uma dessas tecnologias será descrita em relação ao seu papel na
modelagem, coleta e manipulação de dados, bem como na criação da
aplicação final. Portanto, a primeira etapa será descrever as
tecnologias utilizadas para a coleta dos dados.

\section{Recursos utilizados para a coleta de
dados}\label{recursos-utilizados-para-a-coleta-de-dados}

~~~Uma das principais dificuldades no trabalho com dados do mercado
imobiliário é a escassez de informações disponíveis na internet, o que
exige a busca por alternativas para sua obtenção. Uma solução amplamente
utilizada é a extração direta de informações de sites especializados, um
processo conhecido como Web Scraping (raspagem de dados). Essa abordagem
não se restringe ao setor imobiliário, sendo igualmente aplicável a
outras áreas, como a obtenção de dados sobre automóveis, entre outros.

\vspace{12pt}

O Web scraping é uma técnica utilizada para extrair informações de sites
na internet, salvando-as em arquivos ou sistemas de banco de dados para
realizar análise, construção de aplicações ou ter acesso a informações
de difícil disponibilização. Geralmente a raspagem de dados é realizada
utilizando o Hypertext Transfer Protocol (HTTP). O HTTP é o protocolo
responsável por fazer toda a comunicação cliente-servidor contida na
internet com base na definição de oito métodos de requisição: GET, HEAD,
POST, PUT, DELETE, TRACE, OPTIONS e CONNECT. Cada método indica a ação a
ser realizada no recurso especificado.

\begin{itemize}
\item
  GET: O método GET serve para requisitar uma representação do recurso
  especificado. Ou seja, ele serve para visualizar dados de um site.
\item
  HEAD: O HEAD é bastante semelhante o GET, mas ele retorna apenas
  metadados sobre um recurso no servidor, sem que o recurso seja
  retornado. Ele retorna todos os cabeçalhos associados a um recurso em
  uma determinada URL.
\item
  POST: O método POST envia dados para serem processados para o recurso
  especificado. Esses dados podem ser, por exemplo, dados de um
  formulário HTML\footnote{HTML (HyperText Markup Language) é a
    linguagem de marcação utilizada para estruturar e exibir conteúdo na
    web. Ele define elementos como títulos, parágrafos, links, imagens e
    tabelas, sendo a base para o desenvolvimento de páginas da internet.}.
\item
  PUT: O PUT é bastante semelhante ao POST, ele envia os dados de forma
  semelhante. No entanto, caso seja necessário atualizar um usuário
  diversas vezes, o método PUT vai sobrescrever os dados e ficará apenas
  um único registro atualizado. Para o método POST, serão criados
  diversos registros para cada requisição realizada.
\item
  DELETE: Exclui o recurso.
\item
  TRACE: O método TRACE HTTP é usado para diagnóstico, depuração e
  solução de problemas. Ele simplesmente retorna um rastreamento de
  diagnóstico que registra dados do ciclo de forma que o cliente possa
  saber o que os servidores intermediários estão mudando em sua
  requisição.
\item
  OPTIONS: O método OPTIONS retorna uma lista de quais métodos HTTP são
  suportados e permitidos pelo servidor.
\item
  CONNECT: O CONNECT é usado para criar uma conexão com um recurso do
  lado do servidor. O alvo mais comum do método HTTP CONNECT é um
  servidor proxy, que um cliente deve acessar para sair da rede local.
\end{itemize}

Toda raspagem inicia com a composição de uma requisição HTTP para
adquirir recursos de um site. Geralmente, essa requisição é formatada
numa consulta GET ou em uma mensagem HTTP contendo uma consulta POST.
Quando a requisição é recebida e processado com sucesso, o recurso
requisitado é salvo site e enviado de volta em diversos formatos de
arquivos como, HTML, XML, JSON ou dados multímidia. Após o download do
conteúdo do site, o processo de extração continua com a reformatação e
organização dos dados de forma estruturada.

\vspace{12pt}

Para realizar a raspagem de dados, foram utilizadas as linguagens de
programação \textbf{R} (R CORE TEAM, 2024) e \textbf{Python} (VAN
ROSSUM; DRAKE JR, 1995). \textbf{R} é uma linguagem voltada para
computação científica e visualização de dados, desenvolvida pelos
professores Ross Ihaka e Robert Gentleman. Ela foi criada inicialmente
para ensinar introdução à estatística na Universidade de Auckland. A
primeira versão do \textbf{R} foi lançada em 1993, e, em 1997, a
linguagem tornou-se oficialmente parte do Projeto GNU. Em \textbf{R}, há
diversos pacotes disponíveis para a realização de raspagem de dados.
Neste trabalho, foi utilizado o pacote
\href{https://xml2.r-lib.org/}{\textbf{xml2}} (WICKHAM; HESTER; OOMS,
2023) e o \href{https://rvest.tidyverse.org/}{\textbf{rvest}} (WICKHAM,
2024).

\vspace{12pt}

O pacote \href{https://xml2.r-lib.org/}{\textbf{xml2}} foi inspirado no
\href{https://jquery.com/}{\textbf{jQuery}}, uma biblioteca
\textbf{JavaScript} projetada para facilitar tarefas como a manipulação
de documentos HTML. Ele é especialmente útil para lidar com arquivos
HTML e XML, oferecendo ferramentas eficientes para acessar, navegar e
modificar os nós de documentos estruturados. Essas funcionalidades
permitem, por exemplo, a extração de informações específicas de páginas
web ou arquivos XML. Embora o
\href{https://xml2.r-lib.org/}{\textbf{xml2}} não possua suporte direto
para compor requisições HTTP, ele se destaca por sua capacidade de
interpretar a estrutura hierárquica de sites e organizar os dados
extraídos de maneira estruturada e eficiente.

\vspace{12pt}

Embora o pacote \href{https://rvest.tidyverse.org/}{\textbf{rvest}}
compartilhe algumas funcionalidades com o
\href{https://xml2.r-lib.org/}{\textbf{xml2}}, ele oferece diversas
características adicionais que tornam o processo de web scraping ainda
mais simples. O \href{https://rvest.tidyverse.org/}{\textbf{rvest}}
facilita o acesso direto a páginas web e a extração de informações com o
uso de seletores \texttt{CSS} e \texttt{XPath}, permitindo a seleção
precisa de elementos específicos da página. Uma funcionalidade
particularmente interessante é o recurso \texttt{LiveHTML}, que
possibilita a interação direta com a página, abrindo-a em um navegador
para observar, em tempo real, as operações realizadas no site. Isso
torna o processo de coleta de dados mais intuitivo, uma vez que se torna
possível observar as operações que se deseja realizar em tempo real.

\vspace{12pt}

Apesar de os pacotes mencionados anteriormente oferecerem diversas
possibilidades para realizar raspagem de dados, muitas páginas possuem
conteúdos dinâmicos que só podem ser acessados por meio de outros
pacotes, capazes de carregar o restante do conteúdo da página em
questão. Nesse contexto, foi utilizado o pacote
\href{https://cran.r-project.org/web/packages/selenium/index.html}{\textbf{selenium}}
(THORPE, 2024), que realiza requisições HTTP enquanto simula a
experiência de um usuário real. Ele permite automatizar interações com a
página, como clicar em botões, rolar a página automaticamente, lidar com
autenticação, cookies e redirecionamentos, além de simular navegadores
como Google Chrome,
\href{https://www.mozilla.org/pt-BR/firefox/}{\textbf{Firefox}}, entre
outros. Por fim, um dos pacotes que também foi utilizado para fazer
requisições HTTP, mas que não teve o mesmo objetivo do
\href{https://cran.r-project.org/web/packages/selenium/index.html}{\textbf{selenium}},
foi o \href{https://httr.r-lib.org/}{\textbf{httr}} (WICKHAM, 2023).

\vspace{12pt}

Como mencionado anteriormente, \textbf{Python} também foi utilizado para
realizar a raspagem de dados, embora tenha sido empregado em momentos
subsequentes à utilização do \textbf{R}. \textbf{Python} (VAN ROSSUM;
DRAKE JR, 1995) é uma linguagem de programação criada por Guido van
Rossum, que iniciou seu desenvolvimento no final dos anos 1980 como
sucessora da linguagem \texttt{ABC}. A primeira versão oficial do
\textbf{Python} foi lançada em 1991. Trata-se de uma linguagem de alto
nível, de propósito geral, que prioriza a simplicidade e a legibilidade
do código, tornando-a amplamente adotada para diversas aplicações,
incluindo análise de dados e automação de tarefas, além de ser
amplamente utilizada para aprendizagem de máquina.

\vspace{12pt}

A linguagem de programação \textbf{Python} possui um ecossistema
diversificado de bibliotecas para a realização de diversas tarefas,
incluindo a raspagem de dados. Especificamente para essa finalidade, foi
utilizada a biblioteca
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}
(KOUZIS-LOUKAS, 2016), um framework voltado para web scraping e crawling
que permite extrair dados de páginas da web de forma automatizada.

\vspace{12pt}

O \href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}} é uma
ferramenta robusta e completa, com suporte nativo para diversos métodos
HTTP. Ele foi projetado para ser rápido e eficiente, facilitando a
criação de spiders --- programas que navegam automaticamente por páginas
web e extraem dados de forma estruturada. Além disso, o framework
permite a exportação dos dados coletados para diversos formatos, como
CSV, JSON ou bancos de dados, tornando-o uma solução ideal para projetos
de coleta de informações em grande escala.

\vspace{12pt}

O framework também possui seletores \texttt{XPath} e \texttt{CSS}, que
facilitam a localização e extração de informações específicas de páginas
HTML. Uma funcionalidade especialmente interessante é sua capacidade de
lidar com redirecionamentos de IP, permitindo superação de restrições
impostas por alguns sites durante o processo de raspagem. Além disso, o
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}} permite a
criação de \texttt{pipelines}, que são etapas de processamento dos dados
em tempo de execução. Essas \texttt{pipelines} são extremamente úteis
para transformar, limpar e organizar as informações antes de serem
salvas, garantindo que os dados extraídos já estejam em um formato
estruturado e pronto para análise ou armazenamento.

\vspace{12pt}

As middlewares no
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}} são
componentes intermediárias que permitem modificar e processar
requisições e respostas em tempo de execução. Elas são fundamentais para
personalizar o comportamento do framework durante o fluxo de raspagem.
Existem dois tipos principais de middlewares no
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}: middlewares
de requisição e middlewares de download.

\vspace{12pt}

As middlewares de requisição são utilizadas, por exemplo, para adicionar
cabeçalhos personalizados às requisições ou alterar o agente do usuário
(\texttt{User-Agent}) para evitar bloqueios por parte dos servidores.
Uma API bastante interessante que possuí integração completa com o
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}} e que foi
empregada neste trabalho foi o uso de uma API gratuita oferecida pelos
desenvolvedores da ScrapeOps, que permite o rotacionamento automático de
cabeçalhos, como o \texttt{User-Agent}. Essa API está disponível em
\url{https://scrapeops.io/}.

\vspace{12pt}

Por outro lado, as middlewares de download têm a função de gerenciar o
processo de download das páginas. Elas podem ser configuradas para
evitar redirecionamentos indesejados, implementar atrasos entre
requisições para evitar sobrecarregar os servidores e diminuir as
chances de ser bloqueado. Essas middlewares tornam o
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}} altamente
flexível, permitindo ajustes específicos para atender às necessidades do
projeto.

\vspace{12pt}

Embora o \href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}
ofereça uma ampla gama de funcionalidades, pode ser necessário recorrer
a ferramentas externas para lidar com conteúdos dinâmicos presentes em
algumas páginas web. O Scrapy possui integração com bibliotecas
especializadas que facilitam o tratamento desse tipo de conteúdo. Uma
dessas bibliotecas, utilizada neste trabalho, é o
\href{https://github.com/scrapy-plugins/scrapy-playwright}{\textbf{scrapy-playwright}},
que integra o poder do
\href{https://playwright.dev/python/}{\textbf{Playwright}} ao
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}.

\vspace{12pt}

O \href{https://playwright.dev/python/}{\textbf{Playwright}} foi
originalmente desenvolvido para realizar testes em aplicações web e
executar tarefas de automação. No entanto, aqui ele foi empregado para
interagir com o conteúdo dinâmico de páginas, como carregar elementos
gerados por \textbf{JavaScript}. Sua utilização neste trabalho teve o
mesmo propósito que o pacote
\href{https://cran.r-project.org/web/packages/selenium/index.html}{\textbf{selenium}}
desempenhou na raspagem de dados feita com \textbf{R}, permitindo
capturar dados que não seriam acessíveis diretamente via requisições
HTTP convencionais. Além disso, ele também permite simular navegadores
para acompanhar em tempo real o que cada requisição está alterando. Por
fim, ao coletar os dados de imóveis, foi necessário realizar também a
geocodificação dos endereços. Para isso, foi utilizado o pacote
\href{https://jessecambon.github.io/tidygeocoder/}{\textbf{tidygeocoder}}
(CAMBON et al., 2021) da linguagem \textbf{R}, que também serviu para os
dados que estavam sendo coletados em \textbf{Python}.

\vspace{12pt}

Na próxima seção, apresenta-se um exemplo de como realizar web scraping
em \textbf{R} e \textbf{Python}. Para isso, são utilizados alguns dos
pacotes mencionados para a linguagem \textbf{R} e, no caso de
\textbf{Python}, dá-se preferência ao
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}, a principal
biblioteca empregada para a extração dos dados. Além disso, esta seção
também abordará a estruturar de um HTML e o funcionamento dos seletores
para a extração de elementos.

\vspace{12pt}

É importante destacar que os códigos desenvolvidos para a raspagem de
dados podem deixar de funcionar ao longo do tempo devido a possíveis
mudanças na estrutura dos sites. Assim, eventuais ajustes no código
poderão ser necessários.

\subsection{Exemplo de web scraping em
Python}\label{exemplo-de-web-scraping-em-python}

~~~Os exemplos de raspagem de dados em \textbf{R} e \textbf{Python}
utilizarão o mesmo site. O objetivo é extrair informações sobre todos os
pacotes de \textbf{R}, ordenados por data de publicação, disponíveis em:
\url{https://cran.r-project.org/web/packages/index.html}. A partir
desses dados, será gerado um arquivo \texttt{.csv} contendo as
informações extraídas.

\vspace{12pt}

Para instalar o
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}} e suas
dependências, pode-se utilizar o
\href{https://pip.pypa.io/en/stable/}{\textbf{pip}}, o gerenciador de
pacotes do \textbf{Python}. Para isso, basta executar o comando
\texttt{pip\ install\ Scrapy} no terminal.

\vspace{12pt}

Após a instalação do
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}, sua
CLI\footnote{CLI (Command Line Interface) é uma interface de linha de
  comando que permite a interação com um sistema operacional ou software
  por meio de comandos digitados em um terminal, sem o uso de interface
  gráfica.} ficará disponível. A CLI disponibiliza diversas
funcionalidades para o
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}, como
inspencionar o conteúdo de uma página para identificar seletores com
mais facilidade e, o mais importante, iniciar um projeto
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}.

\vspace{12pt}

Antes de iniciar a raspagem dos dados com o
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}, é necessário
iniciar um projeto utilizando sua CLI. Para isso, basta executar o
comando
\texttt{scrapy\ startproject\ \textless{}nome\_do\_projeto\textgreater{}}.
Esse comando gerará um diretório com todos os arquivos e configurações
iniciais necessários para a extração de dados com o
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}. Por exemplo,
ao executar o comando \texttt{scrapy\ startproject\ scraping\_examples},
será criado um diretório com a seguinte estrutura:

\begin{verbatim}
scraping_examples/
|-- scraping_examples/
|   |-- __init__.py
|   |-- items.py                          # definição de tabelas
|   |-- middlewares.py                    # middlewares do projeto
|   |-- pipelines.py                      # pipelines do projeto
|   |-- __pycache__/
|   |   |-- __init__.cpython-310.pyc
|   |   |-- settings.cpython-310.pyc
|   |-- settings.py                       # configurações do projeto
|   |-- spiders/
|       |-- __init__.py
|       |-- __pycache__/
|           |-- __init__.cpython-310.pyc
|-- scrapy.cfg                            # parâmetros de configuração
\end{verbatim}

Ao observar a estrutura do diretório, é possível identificar algumas
funcionalidades essenciais já explicadas anteriormente. Por exemplo, o
arquivo \texttt{middlewares.py} é utilizado para modificar como as
requisições são feitas e como o
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}} lida com as
respostas. O arquivo \texttt{settings.py} contém todas as configurações
do projeto, como a ativação de \texttt{pipelines} e
\texttt{middlewares}, além de permitir ajustes como o delay entre
requisições e outras configurações avançadas. O arquivo
\texttt{items.py} é onde são definidas as estruturas dos dados que serão
coletados. O \texttt{pipelines.py}, como mencionado anteriormente, é
responsável pelo processamento dos dados coletados. Por fim, o arquivo
\texttt{scrapy.cfg} é um arquivo de configuração usado para ajustar
parâmetros relacionados ao deploy e outras configurações do projeto.

\vspace{12pt}

Com a estrutura do projeto criada, o próximo passo é criar o primeiro
spider do projeto. Para isso, utiliza-se o comando
\texttt{scrapy\ genspider\ \textless{}nome\_do\_spider\textgreater{}\ \textless{}website\textgreater{}}.
Neste exemplo, serão extraídas informações do site
\url{https://cran.r-project.org/web/packages/index.html}. Ao executar o
comando
\texttt{scrapy\ genspider\ packagescraper\ https://cran.r-project.org/web/packages/index.html},
será gerado um arquivo dentro do diretório spiders com o nome
\texttt{packagescraper.py}, que terá a seguinte estrutura:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ scrapy}


\KeywordTok{class}\NormalTok{ PackagescraperSpider(scrapy.Spider):}
\NormalTok{    name }\OperatorTok{=} \StringTok{"packagescraper"}
\NormalTok{    allowed\_domains }\OperatorTok{=}\NormalTok{ [}\StringTok{"cran.r{-}project.org"}\NormalTok{]}
\NormalTok{    start\_urls }\OperatorTok{=}\NormalTok{ [}\StringTok{"https://cran.r{-}project.org/web/packages/index.html"}\NormalTok{]}

    \KeywordTok{def}\NormalTok{ parse(}\VariableTok{self}\NormalTok{, response):}
        \ControlFlowTok{pass}
\end{Highlighting}
\end{Shaded}

A classe \texttt{PackagescraperSpider} foi criada a partir do comando
executado anteriormente e possui três atributos e um método. O atributo
\texttt{name} define nome do spider e será utilizado posteriormente para
inicializar a raspagem dos dados. O \texttt{allowed\_domains} é o
atributo que indica para o
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}} os domínios
que podem ser acessados, nesse caso, \texttt{cran.r-project.org}. O
atributo \texttt{start\_urls} define a url inicial a ser acessada pelo
spider. Por fim, o método \texttt{parse} é executado assim que uma
resposta é obtida após a requisição ao site alvo, sendo responsável pelo
processamento e extração dos dados desejados.

\vspace{12pt}

Para extrair dados do conteúdo HTML da página, é necessário utilizar os
seletores \texttt{CSS} ou \texttt{XPath}. Nesse caso, será utilizado o
seletor \texttt{CSS}. Para facilitar a seleção do conteúdo desejado,
será utilizado uma das grandes ferramentas do
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}, o
\href{https://docs.scrapy.org/en/latest/topics/shell.html}{\textbf{Scrapy
Shell}}. Essa ferramenta permite testar seletores, inspensionar
elementos da página e até mesmo ajuda a debugar o código.

\vspace{12pt}

Para executar o
\href{https://docs.scrapy.org/en/latest/topics/shell.html}{\textbf{Scrapy
Shell}}, basta utilizar o comando \texttt{scrapy\ shell} no terminal.
Após a sua execução, aparecerão algumas informações de logs e de
configurações iniciais que estão sendo utilizadas no projeto. Agora, com
o shell ativado, é possível carregar o conteúdo HTML da página de
interesse utilizando o comando:
\texttt{fetch("https://cran.r-project.org/web/packages/index.html")}.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{includes/page_content_new.png}}

}

\subcaption{\label{fig-page_contents}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{includes/page_content_devtools_bigger.png}}

}

\subcaption{\label{fig-page_contents_devtools}}

\end{minipage}%

\caption{\label{fig-pages_scrape}A Figura~\ref{fig-page_contents} exibe
a página que está sendo raspada, enquanto a
Figura~\ref{fig-page_contents_devtools} mostra a mesma página com um
exemplo do uso das ferramentas de desenvolvedor do Firefox.}

\end{figure}%

Com a execução do comando anterior, o conteúdo da página mostrada na
Figura~\ref{fig-page_contents} será carregado, permitindo a extração das
informações contidas em seu HTML. Se o comando \texttt{fetch} obter
êxito, a saída incluirá o código de status HTTP \texttt{200}, indicando
que a requisição foi bem sucedida. Abaixo, segue um exemplo desse
resultado:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{In} \PreprocessorTok{[}\SpecialStringTok{1}\PreprocessorTok{]}\NormalTok{: fetch}\ErrorTok{(}\StringTok{"https://cran.r{-}project.org/web/packages/index.html"}\KeywordTok{)}
\ExtensionTok{2025{-}03{-}03}\NormalTok{ 08:04:24 }\PreprocessorTok{[}\SpecialStringTok{scrapy.core.engine}\PreprocessorTok{]}\NormalTok{ INFO: Spider opened}
\ExtensionTok{2025{-}03{-}03}\NormalTok{ 08:04:27 }\PreprocessorTok{[}\SpecialStringTok{scrapy.core.engine}\PreprocessorTok{]}\NormalTok{ DEBUG: Crawled }\ErrorTok{(}\ExtensionTok{200}\KeywordTok{)} \OperatorTok{\textless{}}\NormalTok{GET}
\ExtensionTok{https://cran.r{-}project.org/robots.txt}\OperatorTok{\textgreater{}} \ErrorTok{(}\ExtensionTok{referer:}\NormalTok{ None}\KeywordTok{)}
\ExtensionTok{2025{-}03{-}03}\NormalTok{ 08:04:28 }\PreprocessorTok{[}\SpecialStringTok{scrapy.core.engine}\PreprocessorTok{]}\NormalTok{ DEBUG: Crawled }\ErrorTok{(}\ExtensionTok{200}\KeywordTok{)} \OperatorTok{\textless{}}\NormalTok{GET}
\ExtensionTok{https://cran.r{-}project.org/web/packages/index.html}\OperatorTok{\textgreater{}} \ErrorTok{(}\ExtensionTok{referer:}\NormalTok{ None}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

Após a execução do comando \texttt{fetch}, o conteúdo da página ficará
armazenado no objeto \texttt{response}, que está disponível nativamente
no shell. Por exemplo, para visualizar o HTML da página, basta executar
\texttt{response.body}. A partir dessa resposta, é possível analisar a
estrutura da página e configurar o seletor para extrair apenas os dados
desejados. No entanto, para facilitar ainda mais esse trabalho,
navegadores como
\href{https://www.mozilla.org/pt-BR/firefox/}{\textbf{Firefox}} oferecem
ferramentas de desenvolvedor que permitem inspencionar a estrutura da
página, como pode ser visto na Figura~\ref{fig-page_contents_devtools}.
Para acessar essa ferramenta no
\href{https://www.mozilla.org/pt-BR/firefox/}{\textbf{Firefox}}, basta
utilizar o atalho \texttt{Ctrl+Shift+I}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\textless{}!{-}{-}PARTE DO CONTEÚDO DA PÁGINA{-}{-}\textgreater{}}

\DataTypeTok{\textless{}!DOCTYPE}\NormalTok{ html}\DataTypeTok{\textgreater{}}
\DataTypeTok{\textless{}}\KeywordTok{html}\DataTypeTok{\textgreater{}}
\DataTypeTok{\textless{}}\KeywordTok{head}\DataTypeTok{\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{title}\DataTypeTok{\textgreater{}}\NormalTok{CRAN: Contributed Packages}\DataTypeTok{\textless{}/}\KeywordTok{title}\DataTypeTok{\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{link}\OtherTok{ rel}\OperatorTok{=}\StringTok{"stylesheet"}\OtherTok{ type}\OperatorTok{=}\StringTok{"text/css"}\OtherTok{ href}\OperatorTok{=}\StringTok{"../CRAN\_web.css"}\DataTypeTok{/\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{meta}\OtherTok{ http{-}equiv}\OperatorTok{=}\StringTok{"Content{-}Type"}
\OtherTok{     content}\OperatorTok{=}\StringTok{"text/html; charset=utf{-}8"}\DataTypeTok{/\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{meta}\OtherTok{ name}\OperatorTok{=}\StringTok{"viewport"}\OtherTok{ content}\OperatorTok{=}\StringTok{"width=device{-}width,}
\StringTok{     initial{-}scale=1.0, user{-}scalable=yes"}\DataTypeTok{/\textgreater{}}
\DataTypeTok{\textless{}/}\KeywordTok{head}\DataTypeTok{\textgreater{}}
\DataTypeTok{\textless{}}\KeywordTok{body}\DataTypeTok{\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{div}\OtherTok{ class}\OperatorTok{=}\StringTok{"container"}\DataTypeTok{\textgreater{}}
        \DataTypeTok{\textless{}}\KeywordTok{h1}\DataTypeTok{\textgreater{}}\NormalTok{Contributed Packages}\DataTypeTok{\textless{}/}\KeywordTok{h1}\DataTypeTok{\textgreater{}}

        \DataTypeTok{\textless{}}\KeywordTok{h3}\OtherTok{ id}\OperatorTok{=}\StringTok{"pkgs"}\DataTypeTok{\textgreater{}}\NormalTok{Available Packages}\DataTypeTok{\textless{}/}\KeywordTok{h3}\DataTypeTok{\textgreater{}}
        \DataTypeTok{\textless{}}\KeywordTok{p}\DataTypeTok{\textgreater{}}
\NormalTok{            Currently, the CRAN package repository features}
\NormalTok{            22127 available packages.}
        \DataTypeTok{\textless{}/}\KeywordTok{p}\DataTypeTok{\textgreater{}}
\NormalTok{        ...}
    \DataTypeTok{\textless{}/}\KeywordTok{div}\DataTypeTok{\textgreater{}}
\DataTypeTok{\textless{}/}\KeywordTok{body}\DataTypeTok{\textgreater{}}
\DataTypeTok{\textless{}/}\KeywordTok{html}\DataTypeTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

Após inspencionar a página, observa-se que existem elementos
\texttt{\textless{}a\textgreater{}} no corpo do HTML, cada um com um
atrituto \texttt{href} associado. Esses \texttt{href} serão necessários
para realizar a paginação e coletar os pacotes ordenados por data de
publicação. Para extrair os \texttt{href}, basta executar no shell o
comando
\texttt{response.css("a{[}target=\_top{]}\ ::attr(href)").get()}. Esse
comando utiliza seletores \texttt{CSS} para extrair o primeiro
\texttt{href} de um elemento \texttt{\textless{}a\textgreater{}} que
possui o atributo \texttt{target="\_top"}. Se for necessário extrair
todos os \texttt{href}, basta substituir o método \texttt{.get()} por
\texttt{.getall()}, o que retornará todos os valores correspondentes.

\vspace{12pt}

Com a execução do comando, é extraída uma string que será usada para
realizar a paginação, chamada
\texttt{\textquotesingle{}available\_packages\_by\_date.html\textquotesingle{}}.
O \texttt{\textquotesingle{}index.html\textquotesingle{}} que está na
string do atributo \texttt{start\_urls} deverá ser substituído pela
string obtida, direcionando para a página de pacotes ordenados por data
de publicação. Agora, com esse \texttt{href}, é possível utilizar o
shell e executar o comando \texttt{fetch} na url
\url{https://cran.r-project.org/web/packages/available_packages_by_date.html}
para carregar o conteúdo da página com a tabela de todos os pacotes
ordenados por data.

\vspace{12pt}

A nova página contém uma tabela com todos os pacotes, ordenados por
data, juntamente com suas descrições. Cada data, nome de pacote e
descrição estão contidos dentro do elemento
\texttt{\textless{}tr\textgreater{}} no HTML. Para selecionar todos os
textos presentes nesse elemento, basta executar o comando
\texttt{response.css("tr\ ::text").getall()}. Após a execução, será
gerada uma lista de strings com as informações de todos os pacotes. No
entanto, essa lista pode conter algumas strings vazias, que precisam ser
removidas. O código a seguir apresenta todos os seletores utilizados até
agora no shell, agora integrados ao spider, para dar início ao processo
de raspagem dos dados:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ scrapy}


\KeywordTok{class}\NormalTok{ PackagescraperSpider(scrapy.Spider):}
\NormalTok{    name }\OperatorTok{=} \StringTok{"packagescraper"}
\NormalTok{    allowed\_domains }\OperatorTok{=}\NormalTok{ [}\StringTok{"cran.r{-}project.org"}\NormalTok{]}
\NormalTok{    start\_urls }\OperatorTok{=}\NormalTok{ [}\StringTok{"https://cran.r{-}project.org/web/packages/index.html"}\NormalTok{]}

    \KeywordTok{def}\NormalTok{ parse(}\VariableTok{self}\NormalTok{, response):}
\NormalTok{        href }\OperatorTok{=}\NormalTok{ response.css(}\StringTok{"a[target=\_top] ::attr(href)"}\NormalTok{).get()}
\NormalTok{        follow\_url }\OperatorTok{=}\NormalTok{ (}
          \VariableTok{self}\NormalTok{.start\_urls[}\DecValTok{0}\NormalTok{].replace(}\StringTok{"index.html"}\NormalTok{, }\StringTok{""}\NormalTok{) }\OperatorTok{+}\NormalTok{ href}
\NormalTok{        )}

        \ControlFlowTok{yield}\NormalTok{ response.follow(}
\NormalTok{            follow\_url,}
\NormalTok{            callback}\OperatorTok{=}\VariableTok{self}\NormalTok{.parse\_packages\_info,}
\NormalTok{            dont\_filter}\OperatorTok{=}\VariableTok{True}
\NormalTok{        )}

    \KeywordTok{def}\NormalTok{ parse\_packages\_info(}\VariableTok{self}\NormalTok{, response):}
\NormalTok{        get\_all\_tr }\OperatorTok{=}\NormalTok{ response.css(}\StringTok{"tr ::text"}\NormalTok{).getall()}
\NormalTok{        packages\_element }\OperatorTok{=}\NormalTok{ [}
\NormalTok{          x }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ get\_all\_tr }\ControlFlowTok{if}\NormalTok{ x.strip() }\OperatorTok{!=} \StringTok{""}
\NormalTok{        ][}\DecValTok{3}\NormalTok{:]}

\NormalTok{        pub\_date }\OperatorTok{=}\NormalTok{ packages\_element[::}\DecValTok{3}\NormalTok{]}
\NormalTok{        packs\_name }\OperatorTok{=}\NormalTok{ packages\_element[}\DecValTok{1}\NormalTok{::}\DecValTok{3}\NormalTok{]}
\NormalTok{        desc\_packs }\OperatorTok{=}\NormalTok{ packages\_element[}\DecValTok{2}\NormalTok{::}\DecValTok{3}\NormalTok{]}

        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(pub\_date, packs\_name, desc\_packs):}
            \ControlFlowTok{yield}\NormalTok{ \{}
                \StringTok{"publication\_date"}\NormalTok{: i[}\DecValTok{0}\NormalTok{],}
                \StringTok{"package\_name"}\NormalTok{: i[}\DecValTok{1}\NormalTok{],}
                \StringTok{"description\_packs"}\NormalTok{: i[}\DecValTok{2}\NormalTok{],}
\NormalTok{            \}}
\end{Highlighting}
\end{Shaded}

O código contém a mesma classe mostrada anteriormente no
\texttt{packagescraper.py}, mas agora inclui os seletores analisados no
shell e outras modificações. Por exemplo, o método
\texttt{parse(self,\ response)}, apresentado anteriormente, é
responsável por capturar o link que leva à tabela de pacotes. Esse link
é extraído utilizando o seletor definido anteriomente no shell. Em
seguida, esse link é concatenado à url base para formar a url completa
que será seguida pelo spider. O método \texttt{response.follow()} é
então usado para acessar a nova página e chamar o método
\texttt{parse\_packages\_info}, que continuará o processo de raspagem.

\vspace{12pt}

Dentro do método \texttt{parse\_packages\_info}, todos os textos
contidos nos elementos \texttt{\textless{}tr\textgreater{}} são
coletados com \texttt{response.css("tr\ ::text").getall()}. Como essa
extração pode incluir strings vazias, é aplicado um \texttt{.strip()} em
cada elemento da lista e removidos aqueles que resultam em uma string
vazia. Além disso, os primeiros três elementos são ignorados, pois não
contêm informações relevantes, apenas o cabeçalho da tabela.

\vspace{12pt}

Após a limpeza, os dados são organizados em três listas:
\texttt{pub\_date}, \texttt{packs\_name} e \texttt{desc\_packs}. Cada
uma delas recebe elementos específicos da lista filtrada, onde,
\texttt{pub\_date} contém as datas de publicação dos pacotes,
\texttt{packs\_name} contém os nomes dos pacotes e \texttt{desc\_packs}
contém as descrições dos pacotes.

\vspace{12pt}

Por fim, a função \texttt{zip(pub\_date,\ packs\_name,\ desc\_packs)}
permite iterar sobre as três listas simultaneamente, gerando um
dicionário com as chaves \texttt{publication\_date},
\texttt{package\_name} e \texttt{description\_packs}. Esse dicionário é
então passado para o \texttt{yield}, permitindo que
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}} armazene os
dados no formato desejado.

\vspace{12pt}

Com esse código, os dados coletados podem ser salvos em diferentes
formatos, como \texttt{.json} ou \texttt{.csv}, ao executar o spider.
Por exemplo, para salvar os dados como \texttt{.csv}, basta executar no
terminal o comando
\texttt{scrapy\ crawl\ -o\ packages.csv\ packagescraper}. Ao executar
esse comando, será salvo um arquivo chamado \texttt{packages.csv},
contendo todos os dados coletados do site alvo.

\vspace{12pt}

O exemplo apresentado aqui é bastante simples. No entanto, dependendo
das necessidades do projeto, pode ser necessário utilizar outras
ferramentas do
\href{https://docs.scrapy.org/en/latest}{\textbf{Scrapy}}. O código
completo para a raspagem de dados de imóveis pode ser acessado em
\url{https://github.com/cowvin0/tcc_realestate/tree/main/scrapy_zap}.
Esse código pode servir como material complementar ao que foi
apresentado neste texto, pois foram adicionadas mais funcionalidades
durante a coleta dos dados, atendendo a necessidades específicas do
projeto.

\subsection{Exemplo de web scraping em
R}\label{exemplo-de-web-scraping-em-r}

~~~Agora que a estrutura do site é conhecida e a inspeção dos dados de
interesse foi feita utilizando as ferramentas de desenvolvedor do
\href{https://www.mozilla.org/pt-BR/firefox/}{\textbf{Firefox}}, o
exemplo em R será mais direto. Neste exemplo, utiliza-se o pacote
\href{https://rvest.tidyverse.org/}{\textbf{rvest}},
\href{https://dplyr.tidyverse.org/}{\textbf{dplyr}} para criar a tabela
com as informações coletadas, e
\href{https://purrr.tidyverse.org/}{\textbf{purrr}}, que é um pacote de
\textbf{R} voltado para programação funcional. O código utilizado para a
extração pode ser visualizado abaixo:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(purrr)}

\NormalTok{base\_url }\OtherTok{\textless{}{-}} \StringTok{"https://cran.r{-}project.org/web/packages/"}

\NormalTok{first\_package\_link }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(base\_url, }\StringTok{"index.html"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"a[target=\_top]"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"href"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{first}\NormalTok{()}

\NormalTok{package\_url }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(base\_url, first\_package\_link)}
\NormalTok{package\_page }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(package\_url)}

\NormalTok{packages\_info }\OtherTok{\textless{}{-}}\NormalTok{ package\_page }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"tr"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{html\_text2}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{tail}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{map}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \FunctionTok{strsplit}\NormalTok{(.x, }\StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)[[}\DecValTok{1}\NormalTok{]])}

\NormalTok{packages\_df }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{publication\_date =} \FunctionTok{map\_chr}\NormalTok{(packages\_info, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{package\_name =} \FunctionTok{map\_chr}\NormalTok{(packages\_info, }\DecValTok{2}\NormalTok{),}
    \AttributeTok{description\_packs =} \FunctionTok{map\_chr}\NormalTok{(packages\_info, }\DecValTok{3}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Após carregar os pacotes necessários, o código define a url base do CRAN
e acessa a página inicial, que contém diversas informações sobre os
pacotes de \textbf{R}. Em seguida, o código extrai o primeiro link do
atributo \texttt{href} presente na página e constrói a url completa para
acessar a página com os pacotes ordenados por data de publicação.

\vspace{12pt}

Depois de carregar o conteúdo da página, o código busca as informações
contidas nos elementos \texttt{\textless{}tr\textgreater{}} da tabela
exibida no site. Os textos são extraídos e processados para remover o
cabeçalho. Em seguida, as informações são separadas em colunas distintas
utilizando a função \texttt{strsplit()}, que divide em uma lista os
dados de data de publicação, nome do pacote e descrição do pacote.

\vspace{12pt}

Por fim, os dados são organizados em um \texttt{tibble}, onde cada linha
contém a data de publicação, o nome do pacote e sua descrição. O uso da
função \texttt{map\_chr} do pacote
\href{https://purrr.tidyverse.org/}{\textbf{purrr}} garante que os
valores sejam extraídos retornando vetores de caracteres com cada uma
das informações.

\vspace{12pt}

Assim como no exemplo apresentado em \textbf{Python}, o projeto de
raspagem pode ser adaptado de acordo com as necessidades e as
informações que precisam ser coletadas durante o processo. Ou seja,
embora os exemplos fornecidos sejam simples, dependendo das
características do site, outras funcionalidades podem ser adicionadas,
como clicar em botões, rolar a página automaticamente para carregar
conteúdos dinâmicos, entre outras ações.

\vspace{12pt}

Portanto, chega ao fim descrição das tecnologias utilizadas para a
obtenção dos dados, com exemplos de raspagem em \textbf{R} e
\textbf{Python}. No entanto, a linguagem \textbf{Python}, em particular,
continuou sendo empregada em outras etapas deste trabalho, como na
modelagem dos valores de imóveis. Dessa forma, os próximos recursos
apresentados serão aqueles utilizados especificamente para a modelagem e
visualização dos resultados.

\section{Recursos utilizados para modelagem e visualização dos
resultados}\label{recursos-utilizados-para-modelagem-e-visualizauxe7uxe3o-dos-resultados}

~~~Com os dados coletados, a etapa de modelagem dos valores de imóveis
torna-se essencial para converter essas informações em entendimentos
relevantes e aplicáveis. Essa etapa permite identificar padrões de
comportamento entre as variáveis que influenciam os valores
imobiliários, além de determinar quais fatores exercem maior impacto
sobre esses valores. Para chegar a esses resultados, foram utilizadas
bibliotecas desenvolvidas em \textbf{Python}, como
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}}
(PEDREGOSA, F. et al., 2011),
\href{https://pandas.pydata.org/}{\textbf{pandas}} (TEAM, 2020),
empregada para a manipulação das bases de dados utilizadas neste
trabalho, \href{https://numpy.org/}{\textbf{numpy}} (HARRIS et al.,
2020), voltada para computação numérica, entre outras.

\vspace{12pt}

O \href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}} é uma
das bibliotecas de aprendizado de máquina mais populares em
\textbf{Python}. Ela oferece uma extensa coleção de algoritmos para
tarefas como classificação, regressão, clusterização, além de
ferramentas para pré-processamento de dados, validação cruzada e seleção
de modelos. O projeto teve início no Google Summer of Code como uma
iniciativa do engenheiro francês David Cournapeau. O
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}} foi
criado como uma ferramenta baseada na biblioteca
\href{https://scipy.org/}{\textbf{SciPy}} (VIRTANEN et al., 2020), que é
voltada para computação científica e cálculo numérico em
\textbf{Python}.

\vspace{12pt}

Uma das funcionalidades mais úteis do
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}} é o uso
de pipelines para transformar dados. As pipelines permitem combinar
etapas de pré-processamento e ajuste de modelos em um único fluxo
organizado. Isso não apenas simplifica o código, mas também assegura que
as transformações aplicadas aos dados de treinamento sejam
automaticamente replicadas nos dados de teste ou em novos dados. Além de
serem úteis para tarefas mais simples, como normalização de variáveis,
as pipelines permitem a inclusão de etapas personalizadas para lidar com
cenários mais complexos, como tratamentos avançados de dados ou
integração com ferramentas externas.

\vspace{12pt}

O exemplo abaixo ilustra uma pipeline para pré-processamento e
modelagem. As variáveis numéricas passam por padronização, enquanto as
variáveis categóricas são transformadas em variáveis binárias utilizando
codificação one-hot. Por fim, os dados processados alimentam um
algoritmo de Random Forest para regressão:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\ImportTok{from}\NormalTok{ sklearn.pipeline }\ImportTok{import}\NormalTok{ Pipeline}
\ImportTok{from}\NormalTok{ sklearn.compose }\ImportTok{import}\NormalTok{ ColumnTransformer}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler, OneHotEncoder}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}

\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(}
\NormalTok{    X, y,}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{,}
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{numerical\_features }\OperatorTok{=}\NormalTok{ X.select\_dtypes(include}\OperatorTok{=}\NormalTok{np.number).columns}
\NormalTok{categorical\_features }\OperatorTok{=}\NormalTok{ X.select\_dtypes(include}\OperatorTok{=}\BuiltInTok{object}\NormalTok{).columns}

\NormalTok{pipeline }\OperatorTok{=}\NormalTok{ Pipeline([}
\NormalTok{    (}\StringTok{\textquotesingle{}preprocessor\textquotesingle{}}\NormalTok{, ColumnTransformer([}
\NormalTok{        (}\StringTok{\textquotesingle{}num\textquotesingle{}}\NormalTok{, StandardScaler(), numerical\_features),}
\NormalTok{        (}\StringTok{\textquotesingle{}cat\textquotesingle{}}\NormalTok{, OneHotEncoder(), categorical\_features)}
\NormalTok{    ])),}
\NormalTok{    (}\StringTok{\textquotesingle{}model\textquotesingle{}}\NormalTok{, RandomForestRegressor(}
\NormalTok{        n\_estimators}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{        random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{    ))}
\NormalTok{])}

\NormalTok{pipeline.fit(X\_train, y\_train)}
\end{Highlighting}
\end{Shaded}

Outra etapa fundamental no processo de aprendizado de máquina, além do
pré-processamento dos dados e do ajuste do algoritmo, é a otimização dos
hiperparâmetros. O
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}} oferece
algumas ferramentas para essa tarefa, mas elas possuem funcionalidades
mais básicas e podem ser limitadas para cenários mais complexos. Dessa
forma, de forma complementar, esse trabalho utilizou a biblioteca
\href{https://optuna.org/}{\textbf{Optuna}} (AKIBA et al., 2019), que é
uma biblioteca que contém uma grande quantidade de algoritmos para
otimização, como, por exemplo, métodos de otimização bayesiana.

\vspace{12pt}

Além das funcionalidades de otimização, o
\href{https://optuna.org/}{\textbf{Optuna}} também oferece ferramentas
para analisar o comportamento da função objetivo durante o processo de
otimização e identificar os hiperparâmetros mais relevantes. Uma dessas
ferramentas é a função \texttt{plot\_param\_importances}, que gera um
gráfico destacando a importância relativa de cada hiperparâmetro. Além
disso, como o \texttt{plot\_param\_importances} utiliza a biblioteca
\href{https://matplotlib.org/}{\textbf{Matplotlib}} (HUNTER, 2007), os
gráficos gerados podem ser personalizados pelo usuário.

\vspace{12pt}

Vale destacar que bibliotecas como o
\href{https://matplotlib.org/}{\textbf{Matplotlib}} e o
\href{https://seaborn.pydata.org/}{\textbf{seaborn}} (WASKOM, 2021)
desempenharam um papel fundamental na geração dos gráficos e na
visualização dos dados apresentados neste trabalho. Essas ferramentas
foram indispensáveis para a análise exploratória e a apresentação dos
resultados de forma clara e compreensível. Além dessas bibliotecas,
também foram utilizados recursos voltados para a análise do impacto e
das relações entre as variáveis e as previsões realizadas pelos modelos.
Nesse contexto, destacam-se o
\href{https://shap.readthedocs.io/en/latest}{\textbf{SHAP}} (LUNDBERG;
LEE, 2017), que fornece explicações interpretáveis para os modelos de
aprendizado de máquina, e o próprio
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}}, que foi
essencial para a criação dos gráficos de ICE (Individual Conditional
Expectation). Esses métodos serão detalhados na seção de metodologia
deste trabalho.

\vspace{12pt}

Vale ressaltar que algumas bibliotecas externas, baseadas na API do
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}}, também
foram utilizadas neste trabalho. Entre elas, destacam-se a
\href{https://lightgbm.readthedocs.io/en/stable/}{\textbf{LightGBM}} (KE
et al., 2017) e a
\href{https://xgboost.readthedocs.io/en/stable/}{\textbf{XGBoost}}
(CHEN, T.; GUESTRIN, 2016), empregadas para a implementação de
algoritmos de aprendizado de máquina. Essas bibliotecas fornecem,
respectivamente, os algoritmos de gradient boosting LightGBM e Extreme
Gradient Boosting, reconhecidos por sua eficiência computacional e
desempenho em tarefas de modelagem preditiva.

\vspace{12pt}

Conclui-se, assim, a apresentação das ferramentas empregadas na
modelagem e visualização dos resultados. A seguir, serão descritas as
tecnologias utilizadas no desenvolvimento da aplicação final deste
trabalho. Vale destacar que o
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}} continuou
desempenhando um papel relevante na aplicação, com sua pipeline sendo
serializada no formato \texttt{.pkl} por meio da biblioteca
\href{https://docs.python.org/3/library/pickle.html}{\textbf{pickle}}.
Essa biblioteca permitiu a reutilização da pipeline no back-end da
aplicação, garantindo a consistência, eficiência do processamento dos
dados e predição do modelo para novos dados. Segue, então, a descrição
das ferramentas empregadas na construção da aplicação final.

\section{Recursos utilizados para a criação da aplicação
final}\label{recursos-utilizados-para-a-criauxe7uxe3o-da-aplicauxe7uxe3o-final}

~~~A aplicação final foi desenvolvida com o objetivo de realizar
previsões de valores de imóveis e oferecer uma análise detalhada do
mercado imobiliário da cidade de João Pessoa. Além de integrar a
pipeline de machine learning criada durante a etapa de modelagem, a
aplicação também dispõe de funcionalidades interativas que permitem aos
usuários explorar os dados e visualizar os resultados de forma clara e
intuitiva. Para alcançar esses objetivos, foram empregadas tecnologias
para o desenvolvimento do front-end e do back-end, que foram
desenvolvidos em \textbf{Python}.

\vspace{12pt}

O front-end da aplicação foi desenvolvido utilizando a biblioteca
\href{https://dash.plotly.com/}{\textbf{Dash}} (HOSSAIN, 2019), uma
ferramenta voltada para a criação de aplicativos web em \textbf{Python}.
Desenvolvida pela \href{https://plotly.com/}{\textbf{Plotly}} (INC.,
2015), a \href{https://dash.plotly.com/}{\textbf{Dash}} permite
construir interfaces gráficas completas sem a necessidade de
conhecimentos avançados em desenvolvimento web, integrando tecnologias
como \texttt{HTML}, \texttt{CSS} e \textbf{JavaScript} diretamente no
ambiente \textbf{Python}. Entre suas principais características,
destacam-se a facilidade de criar gráficos interativos, a integração com
bibliotecas populares como \href{https://plotly.com/}{\textbf{Plotly}} e
a capacidade de atualizar componentes de forma dinâmica por meio de
funções suas funções \texttt{callback}. A seguir é apresentado um
exemplo do uso da função \texttt{callback}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ dash }\ImportTok{import}\NormalTok{ Dash, dcc, html, Output, Input}
\ImportTok{import}\NormalTok{ plotly.express }\ImportTok{as}\NormalTok{ px}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{\textquotesingle{}Categoria\textquotesingle{}}\NormalTok{: [}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{],}
    \StringTok{\textquotesingle{}Valores\textquotesingle{}}\NormalTok{: [}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{]}
\NormalTok{\})}

\NormalTok{app }\OperatorTok{=}\NormalTok{ Dash(}\VariableTok{\_\_name\_\_}\NormalTok{)}

\NormalTok{app.layout }\OperatorTok{=}\NormalTok{ html.Div([}
\NormalTok{    dcc.Dropdown(}
        \BuiltInTok{id}\OperatorTok{=}\StringTok{\textquotesingle{}dropdown\textquotesingle{}}\NormalTok{,}
\NormalTok{        options}\OperatorTok{=}\NormalTok{[}
\NormalTok{          \{}\StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{: cat, }\StringTok{\textquotesingle{}value\textquotesingle{}}\NormalTok{: cat\}}
          \ControlFlowTok{for}\NormalTok{ cat }\KeywordTok{in}\NormalTok{ df[}\StringTok{\textquotesingle{}Categoria\textquotesingle{}}\NormalTok{]}
\NormalTok{        ],}
\NormalTok{        value}\OperatorTok{=}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{,}
\NormalTok{        clearable}\OperatorTok{=}\VariableTok{False}
\NormalTok{    ),}
\NormalTok{    dcc.Graph(}\BuiltInTok{id}\OperatorTok{=}\StringTok{\textquotesingle{}graph\textquotesingle{}}\NormalTok{)}
\NormalTok{])}

\AttributeTok{@app.callback}\NormalTok{(}
\NormalTok{    Output(}\StringTok{\textquotesingle{}graph\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}figure\textquotesingle{}}\NormalTok{),}
\NormalTok{    Input(}\StringTok{\textquotesingle{}dropdown\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}value\textquotesingle{}}\NormalTok{)}
\NormalTok{)}
\KeywordTok{def}\NormalTok{ update\_graph(selected\_category):}
\NormalTok{    filtered\_df }\OperatorTok{=}\NormalTok{ df[df[}\StringTok{\textquotesingle{}Categoria\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ selected\_category]}
\NormalTok{    fig }\OperatorTok{=}\NormalTok{ px.bar(}
\NormalTok{      filtered\_df,}
\NormalTok{      x}\OperatorTok{=}\StringTok{\textquotesingle{}Categoria\textquotesingle{}}\NormalTok{,}
\NormalTok{      y}\OperatorTok{=}\StringTok{\textquotesingle{}Valores\textquotesingle{}}\NormalTok{,}
\NormalTok{      title}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}Valores da Categoria }\SpecialCharTok{\{}\NormalTok{selected\_category}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}

    \ControlFlowTok{return}\NormalTok{ fig}

\ControlFlowTok{if} \VariableTok{\_\_name\_\_} \OperatorTok{==} \StringTok{\textquotesingle{}\_\_main\_\_\textquotesingle{}}\NormalTok{:}
\NormalTok{    app.run\_server(debug}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Neste exemplo, o código permite que o usuário selecione um valor em um
menu suspenso, criado a partir da classe \texttt{Dropdown}, e exiba o
valor escolhido em um gráfico. Primeiramente, é criado um pequeno
conjunto de dados utilizando a biblioteca
\href{https://pandas.pydata.org/}{\textbf{pandas}}, contendo categorias
(A, B e C) e valores associados a elas. Em seguida, define-se o layout
da aplicação (\texttt{app.layout}), que consiste em um \texttt{Dropdown}
para selecionar uma categoria e um componente \texttt{Graph} para exibir
o gráfico correspondente. A lógica para atualizar o gráfico conforme a
seleção do usuário é implementada por meio da função \texttt{callback}.
O decorador \texttt{@app.callback} conecta o valor selecionado no
\texttt{Dropdown} (definido pela classe \texttt{Input}) ao gráfico
(definido pela classe \texttt{Output}). A função associada ao decorador
\texttt{callback}, chamada \texttt{update\_graph}, recebe como argumento
o valor escolhido no \texttt{Dropdown}, filtra o DataFrame com base na
categoria selecionada e gera um gráfico de barras utilizando a
biblioteca \href{https://plotly.com/}{\textbf{Plotly}}. Este gráfico é
então retornado para ser exibido no componente \texttt{Graph}.

\vspace{12pt}

Para o desenvolvimento do back-end da aplicação foi utilizado o
framework \href{https://fastapi.tiangolo.com/}{\textbf{FastAPI}}, uma
ferramenta moderna e eficiente para a criação de APIs em
\textbf{Python}. O
\href{https://fastapi.tiangolo.com/}{\textbf{FastAPI}} é conhecido por
sua alta performance, graças ao uso de tipagem estática e a sua
facilidade de suporte assíncrono, além de permitir a criação de APIs de
maneira rápida e simples. Além disso, integra-se facilmente com
bibliotecas como
\href{https://docs.pydantic.dev/latest/}{\textbf{Pydantic}} para
validação de dados e
\href{https://www.sqlalchemy.org/}{\textbf{SQLAlchemy}} (BAYER, 2012)
para interação e conexão com bancos de dados.

\vspace{12pt}

No projeto, o \href{https://www.sqlalchemy.org/}{\textbf{SQLAlchemy}}
foi utilizado para gerenciar a conexão com o banco de dados,
implementado com
\href{https://www.postgresql.org/}{\textbf{PostgresSQL}}, um sistema de
gerenciamento de banco de dados relacional amplamente reconhecido por
sua robustez e alto desempenho. Isso permitiu que os dados fossem
expostos na API e consumidos no front-end. Além disso, a API foi
responsável por expor a pipeline do modelo, possibilitando a realização
de previsões de valores imobiliários diretamente na aplicação. Não
obstante, os mapas da cidade de João Pessoa, criados com a biblioteca
\href{https://python-visualization.github.io/folium/latest/}{\textbf{Folium}}
(PYTHON-VISUALIZATION, 2020), foram servidos pela API, que expôs seus
arquivos HTML para consumo no front-end.

\vspace{12pt}

Com as tecnologias mencionadas anteriormente e o modelo final obtido,
foi possível criar uma aplicação capaz de realizar previsões para os
imóveis da cidade de João Pessoa, além de proporcionar uma análise de
seu setor imobiliário. No entanto, o desenvolvimento de todo o código da
aplicação exigiu o uso de ferramentas que facilitassem sua escrita,
desenvolvimento e organização. Assim, a seguir serão apresentadas as
ferramentas utilizadas tanto para a implementação do código deste
trabalho quanto para a elaboração do documento final de texto que o
descreve.

\section{Recursos utilizados para escrita de código e de
documento}\label{recursos-utilizados-para-escrita-de-cuxf3digo-e-de-documento}

~~~O desenvolvimento deste trabalho foi realizado em um computador
equipado com processador AMD Ryzen 7 5800H (16 núcleos), 8 GB de memória
RAM, placa de vídeo GeForce GTX 1650 e um SSD NVMe de 256 GB, operando
sob o sistema Pop!\_OS 22.04 LTS. Embora a máquina ofereça um desempenho
geral satisfatório, a quantidade limitada de memória RAM apresentou
desafios em tarefas mais intensivas, como a otimização de
hiperparâmetros dos modelos. Essas tarefas frequentemente demandavam até
dois dias para sua conclusão e, em alguns casos, falhavam próximo ao
término devido ao alto consumo computacional. Dessa forma, a escolha das
ferramentas utilizadas para a escrita do código foram as que impactassem
o mínimo possível no desempenho do sistema.

\vspace{12pt}

O \href{https://code.visualstudio.com/}{\textbf{Visual Studio Code
(VSCode)}} foi a principal ferramenta utilizada para a escrita do código
neste trabalho. O \href{https://code.visualstudio.com/}{\textbf{VSCode}}
é um editor bastante leve e oferece suporte a diversas linguagens de
programação e permite integração com inúmeras tecnologias por meio de
suas extensões. Entre as extensões utilizadas, destaca-se o
\href{https://marketplace.visualstudio.com/items?itemName=vscodevim.vim}{\textbf{vscodevim}},
que incorpora os key mappings do editor de texto Vim, originalmente
criado por Bram Moolenaar e lançado em 2 de novembro de 1991. Além
disso, foram empregadas extensões específicas para as linguagens
\textbf{Python} e \textbf{R}, tendo como principal objetivo uma melhor
formatação de código, execução e identificação de possíveis erros.

\vspace{12pt}

Por exemplo, uma das extensões utilizadas para análise estática de
código em \textbf{Python} foi o
\href{https://marketplace.visualstudio.com/items?itemName=ms-pyright.pyright}{\textbf{Pyright}},
desenvolvido pela Microsoft. Essa ferramenta é projetada para detectar
erros durante o desenvolvimento e verificar a tipagem de código em
\textbf{Python}, contribuindo para a melhoria da qualidade e da
manutenção do programa. Outra ferramenta empregada foi o
\href{https://marketplace.visualstudio.com/items?itemName=ms-python.black-formatter}{\textbf{Black
Formatter}}, um formatador de código \textbf{Python}. Ele automatiza a
padronização do estilo de código, garantindo consistência e
legibilidade. Além disso, foi utilizado o
\href{https://marketplace.visualstudio.com/items?itemName=ms-python.flake8}{\textbf{Flake8}},
que analisa o código em busca de erros de sintaxe, problemas de estilo
com base no padrão \href{https://peps.python.org/pep-0008/}{\textbf{PEP
8}}, e outras questões, como redundâncias ou importações desnecessárias.
Por fim, também foi utilizada uma extensão mais geral de suporte à
linguagem \textbf{Python}, que oferece diversas funcionalidades, como
correção de código por meio da extensão
\href{https://marketplace.visualstudio.com/items?itemName=ms-python.debugpy}{\textbf{PythonDebugger}},
que utiliza a biblioteca debugpy para suporte ao processo de debugging.

\vspace{12pt}

Para a elaboração deste documento, foi utilizado o
\href{https://quarto.org/}{\textbf{Quarto}} (ALLAIRE; DERVIEUX, 2024),
uma plataforma de publicação científica desenvolvida pela empresa Posit.
O \href{https://quarto.org/}{\textbf{Quarto}} é uma evolução do
RMarkdown e se destaca por sua capacidade de criar documentos de alta
qualidade que integram texto e código. Compatível com diversas
linguagens de programação, como \textbf{R}, \textbf{Python}, e outras,
essa ferramenta é extremamente versátil para análise de dados e geração
de relatórios. Com o \href{https://quarto.org/}{\textbf{Quarto}}, é
possível produzir relatórios, artigos, livros, apresentações e até
sites. Ele é amplamente adotado na comunidade científica, especialmente
entre usuários de \textbf{R}, e oferece suporte a Markdown e \LaTeX, o
que facilita a inclusão de fórmulas matemáticas, gráficos, tabelas e
outros elementos visuais. Além disso, os documentos gerados podem ser
exportados para diversos formatos, como HTML, PDF, MS Word, entre
outros. O \href{https://quarto.org/}{\textbf{Quarto}} também foi
utilizado através do
\href{https://code.visualstudio.com/}{\textbf{VSCode}}, com a sua
extensão disponível em \url{https://quarto.org/docs/tools/vscode.html}.

\vspace{12pt}

No contexto deste trabalho, o
\href{https://quarto.org/}{\textbf{Quarto}} foi utilizado para a
produção de todo o texto, garantindo conformidade com as normas da ABNT.
Sua capacidade de integrar texto, código e gráficos de maneira
organizada foi essencial para facilitar o desenvolvimento do documento.

\chapter{Algoritmos de Aprendizado de
Máquina}\label{algoritmos-de-aprendizado-de-muxe1quina}

~~~Neste capítulo, serão descritos os algoritmos de aprendizado de
máquina utilizados neste trabalho. Alguns dos métodos utilizados podem
fazer uso de diversos algoritmos ou modelos estatísticos. No entanto, o
foco principal e o mais utilizado foram as árvores de decisão,
especialmente em sua forma particular, as árvores de regressão. Assim,
os algoritmos descritos são métodos baseados em árvores ou que fazem uso
dela.

\vspace{12pt}

Os métodos baseados em árvore envolvem a estratificação ou segmentação
do espaço dos preditores em várias regiões simples. Dessa forma, todos
os algoritmos utilizados neste trabalho partem dessa ideia. Portanto, o
primeiro a ser explicado será o de árvores de decisão, pois fundamenta
todos os outros algoritmos. Depois das árvores de decisão, serão
explicados os métodos ensemble e, por fim, diferentes variações do
método de gradient boosting.

\section{Árvores de decisão}\label{uxe1rvores-de-decisuxe3o}

~~~Árvores de decisão podem ser utilizadas tanto para regressão quanto
para classificação. Elas servem de base para os modelos baseados em
árvores empregados neste trabalho, focando particularmente nas árvores
de regressão\footnote{Uma árvore de regressão é um caso específico da
  árvore de decisão, mas para regressão.}. O processo de construção de
uma árvore se baseia no particionamento recursivo do espaço dos
preditores, onde cada particionamento é chamado de nó e o resultado
final é chamado de folha ou nó terminal. Em cada nó, é definida uma
condição e, caso essa condição seja satisfeita, o resultado será uma das
folhas desse nó. Caso contrário, o processo segue para o próximo nó e
verifica a próxima condição, podendo gerar uma folha ou outro nó. Veja
um exemplo na Figura~\ref{fig-arvore}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{includes/mermaid-tree.png}}

}

\caption{\label{fig-arvore}Exemplo de estrutura de árvore de regressão.
A árvore tem cinco folhas e quatro nós internos.}

\end{figure}%

\vspace{12pt}

O espaço dos preditores é dividido em \(J\) regiões distintas e
disjuntas denotadas por \(R_1, R_2, \dots, R_J\). Essas regiões são
construídas em formato de caixa de forma a minimizar a soma dos
quadrados dos resíduos. Dessa forma, pode-se modelar a variável resposta
como uma constante \(c_j\) em cada região \(R_j\):

\[
f\left(x\right) = \sum^J_{j=1}c_j I\left(x \in R_j \right)\text{.}
\]

O estimador para a constante \(c_j\) é encontrado pelo método de mínimos
quadrados. Assim, deve-se minimizar
\(\sum_{x_i \in R_j} \left[y_i - f\left(x_i\right)\right]^2\). No
entanto, perceba que \(f\left(x_i\right)\) está sendo avaliado somente
em um ponto específico \(x_i\), o que reduzirá \(f\left(x_i\right)\)
para uma constante \(c_j\). É fácil de se chegar ao resultado se for
observada a definição da função indicadora \(I\left(x \in R_j\right)\):

\[
I_{R_j}(x_i) =
\begin{cases}
    1,& \text{se } x_i \in R_j \\
    0,& \text{se } x_i \notin R_j
\end{cases}\text{.}
\]

Como as regiões são disjuntas, \(x_i\) não pode estar simultaneamente em
duas regiões. Assim, para um ponto específico \(x_i\), apenas um dos
casos da função indicadora será diferente de 0. Portanto,
\(f\left(x_i\right) = c_j\). Agora, derivando
\(\sum_{x_i \in R_j}\left(yi - c_j\right)^2\) em relação a \(c_j\)

\begin{equation}\phantomsection\label{eq-partialdev}{
\frac{\partial}{\partial{c_j}}\sum_{x_i \in R_j} \left(y_i - c_j\right)^2 = -2\sum_{x_i \in R_j} \left(y_i - c_j\right)
}\end{equation} e, ao igualar a Equação~\ref{eq-partialdev} a 0, tem-se
a seguinte igualdade:

\[
\sum_{x_i \in R_j} \left(y_i - \hat{c}_j\right) = 0\text{.}
\]

Expandindo o somatório e dividindo pelo número total de pontos \(N_j\)
na região \(R_j\), concluí-se que o estimador de \(c_j\), denotado por
\(\hat{c}_j\), é simplesmente a média dos valores observados \(y_i\)
dentro da região \(R_j\):

\begin{equation}\phantomsection\label{eq-estimacjdev}{
\sum_{x_i \in R_j} y_i - \hat{c}_j N_j = 0 \Rightarrow \hat{c}_j = \frac{1}{N_{j}}\sum_{x_i \in R_j} y_i\text{.}
}\end{equation}

\vspace{12pt}

No entanto, JAMES et al. (2013) caracteriza como inviável considerar
todas as possíveis partições do espaço das variáveis em \(J\) caixas
devido ao alto custo computacional. Dessa forma, a abordagem a ser
adotada é uma divisão binária recursiva. O processo começa no topo da
árvore de regressão, o ponto em que contém todas as observações, e
continua sucessivamente dividindo o espaço dos preditores. As divisões
são indicadas como dois novos ramos na árvore, como pode ser visto na
Figura~\ref{fig-arvore}.

\vspace{12pt}

Para executar a divisão binária recursiva, deve-se primeiramente
selecionar a variável independente \(X_j\) e o ponto de corte \(s\) tal
que a divisão do espaço dos preditores conduza a maior redução possível
na soma dos quadrados dos resíduos. Dessa forma, definimos dois
semi-planos:

\[
R_{1}\left(j, s\right) = \{X | X_j \leq s\} \text{ e } R_{2}\left(j, s\right) = \{X | X_j > s\}\text{,}
\] e procuramos a divisão da variável \(j\) e o ponto de corte \(s\) que
minimizem a seguinte expressão:

\[
\min_{j, s}\left[\min_{c_1} \sum_{x_i \in R_1\left(j, s\right)} \left(y_i - c_{1}\right)^2 + \min_{c_2} \sum_{x_i \in R_2\left(j, s\right)} \left(y_i - c_{2}\right)^2\right]\text{,}
\] em que \(c_1\) e \(c_2\) é a média da variável dependente para as
observações nas regiões \(R_1\left(j, s\right)\) e
\(R_2\left(j, s\right)\), respectivamente. Após determinar a melhor
divisão, os dados são particionados nessas duas regiões, e o processo é
repetido recursivamente para todas as sub-regiões resultantes.

\vspace{12pt}

O tamanho da árvore pode ser considerado um hiperparâmetro para regular
a complexidade do modelo, pois uma árvore muito grande pode causar
sobreajuste aos dados de treinamento, capturando não apenas os padrões
relevantes, mas também o ruído. Como resultado, o modelo pode apresentar
bom desempenho nos dados de treinamento, mas falhar ao lidar com novos
dados devido à sua incapacidade de generalização. Por outro lado, uma
árvore muito pequena pode não captar padrões, relações e estruturas
importantes presentes nos dados. Dessa forma, a estratégia adotada para
selecionar o tamanho da árvore consiste em crescer uma grande árvore
\(T_0\), interrompendo o processo de divisão apenas ao atingir um
tamanho mínimo de nós. Posteriormente, a árvore \(T_0\) é podada
utilizando o critério de custo complexidade, que será definido a seguir.

\vspace{12pt}

Para o processo de poda da árvore, definimos uma árvore qualquer \(T\)
que pode ser obtida através do processo da poda de \(T_0\), de modo que
\(T \subset T_0\). Assim, sendo \(N_j\) a quantidade de pontos na região
\(R_j\), seja

\[
Q_j\left(T\right) = \frac{1}{N_j} \sum_{x_i \in R_j}\left(y_i - \hat{c}_j\right)^2
\] uma medida de impureza do nó pelo erro quadrático médio. Assim,
define-se o critério de custo complexidade:

\[
C_{\alpha}\left(T\right) = \sum_{m = 1}^{|T|}N_jQ_j\left(T\right) + \alpha |T|\text{,}
\] em que \(|T|\) denota a quantidade total de folhas, e
\(\alpha \geq 0\) é um hiperparâmetro que equilibra o tamanho da árvore
e a adequação aos dados. A ideia é encontrar, para cada \(\alpha\), a
árvore \(T_{\alpha} \subset T_0\) que minimiza
\(C_{\alpha}\left(T\right)\). Valores grandes de \(\alpha\) resultam em
árvores menores, enquanto valores menores resultam em árvores maiores, e
\(\alpha = 0\) resulta na própria árvore \(T_0\). A busca por
\(T_{\alpha}\) envolve colapsar sucessivamente o nó interno que provoca
o menor aumento em \(\sum_j N_j Q_j\left(T\right)\), continuando o
processo até produzir uma árvore com um único nó. Esse processo gera uma
sequência de subárvores, na qual existe uma única subárvore menor que,
para cada \(\alpha\), minimiza \(C_{\alpha}\left(T\right)\).

\vspace{12pt}

A estimação de \(\alpha\) pode ser realizada por validação cruzada com
cinco ou dez folds, sendo \(\hat \alpha\) escolhido para minimizar a
soma dos quadrados dos resíduos durante o processo de validação cruzada.
Assim, a árvore final será \(T_{\hat \alpha}\). O
 Algoritmo~\ref{algo-buildtree}  exemplifica o processo de crescimento
de uma árvore de regressão:

\begin{algo}

\centering{

\begin{algorithm}[H]
\caption{Algoritmo para crescer uma árvore de regressão.}
\begin{algorithmic}
\State \textbf{1.} Use a divisão binária recursiva para crescer uma árvore grande $T_0$ nos dados de treinamento, parando apenas quando cada folha tiver menos do que um número mínimo de observações.

\vspace{3.7pt}

\State \textbf{2.} Aplique o critério custo de complexidade à árvore grande \( T_0 \) para obter uma sequência de melhores subárvores \( T_\alpha \), em função de \( \alpha \).

\vspace{3.7pt}

\State \textbf{3.} Use validação cruzada $K\text{-fold}$ para escolher \( \alpha \). Isto é, divida as observações de treinamento em $K$ folds. Para cada \( k = 1, \ldots, K \):
    \State \hspace{1em} (a) Repita os Passos 1 e 2 em todos os folds, exceto no $k\text{-ésimo}$ fold dos dados de
    \State \hspace{1em} treinamento.
    \State \hspace{1em} (b) Avalie o erro quadrático médio da previsão no $k\text{-ésimo}$ fold deixado
    \State \hspace{1em} de fora, em função de \( \alpha \).

    \vspace{3pt}

    \State \hspace{1em} Faça a média dos resultados para cada valor de \( \alpha \) e escolha \( \alpha \) que minimize o erro
    \State \hspace{1em} médio.

\vspace{3.7pt}

\State \textbf{4.} Retorne a subárvore \( T_{\hat{\alpha}} \) do Passo 2 que corresponde ao valor estimado de \( \alpha \).
\end{algorithmic}
\end{algorithm}

}

\caption{\label{algo-buildtree}Fonte: JAMES et al. (2013, p. 337).}

\end{algo}%

\vspace{12pt}

No caso de uma árvore de decisão para classificação, a principal
diferença está no critério de divisão dos nós e na poda da árvore. Para
a classificação, a previsão em um nó \(j\), correspondente a uma região
\(R_j\) com \(N_j\) observações, será simplesmente a classe majoritária.
Assim, tem-se:

\[
\hat{p}_{jk} = \frac{1}{N_j}\sum_{x_i \in R_j} I\left(y_i = k\right)\text{,}
\] como sendo a proporção de observações da classe \(k\) no nó \(j\).
Dessa forma, as observações no nó \(j\) são classificadas na classe
\(k\left(j\right) = \arg \max_{k} \hat{p}_{jk}\), que é a moda no nó
\(j\).

\vspace{12pt}

Para a divisão dos nós no caso da regressão, foi utilizado o erro
quadrático médio como medida de impureza. Para a classificação, algumas
medidas comuns para \(Q_j\left(T\right)\) são o erro de classificação, o
índice de Gini ou a entropia cruzada.

\section{Métodos Ensemble}\label{muxe9todos-ensemble}

~~~As árvores de decisão são conhecidas por sua alta interpretabilidade,
mas geralmente apresentam um desempenho preditivo inferior em comparação
com outros modelos e algoritmos. No entanto, é possível superar essa
limitação construindo um modelo preditivo que combina a força de uma
coleção de estimadores base, um processo conhecido como aprendizado em
conjunto (Ensemble Learning). De acordo com HASTIE et al. (2009), o
aprendizado em conjunto pode ser dividido em duas etapas principais: a
primeira etapa consiste em desenvolver uma população de algoritmos de
aprendizado base a partir dos dados de treinamento, e a segunda etapa
envolve a combinação desses algoritmos para formar um estimador
agregado. Portanto, nesta seção, serão definidos os métodos de
aprendizado em conjunto utilizados neste trabalho.

\subsection{Bagging}\label{bagging}

~~~O algoritmo de Bootstrap Aggregation, ou Bagging, foi introduzido por
BREIMAN (1996). Sua ideia principal é gerar um estimador agregado a
partir de múltiplas versões de um preditor, que são criadas por meio de
amostras bootstrap do conjunto de treinamento, utilizadas como novos
conjuntos de treinamento. O Bagging pode ser empregado para melhorar a
estabilidade e a precisão de modelos ou algoritmos de aprendizado de
máquina, além de reduzir a variância e evitar o sobreajuste. Por
exemplo, o Bagging pode ser utilizado para melhorar o desempenho da
árvore de regressão descrita anteriormente.

\vspace{12pt}

BREIMAN (1996) define formalmente o algoritmo de Bagging, que utiliza um
conjunto de treinamento \(\mathcal{L}\). A partir desse conjunto, são
geradas amostras bootstrap \(\mathcal{L}^{(B)}\) com \(B\) réplicas,
formando uma coleção de modelos \(\{f(x, \mathcal{L}^{(B)})\}\), onde
\(f\) representa um modelo estatístico ou algoritmo treinado nas
amostras bootstrap para prever ou classificar uma variável dependente
\(y\) com base em variáveis independentes \(\mathbf{x}\). Se a variável
dependente \(y\) for numérica, a predição é obtida pela média das
previsões dos modelos:

\[
f_{B}\left(x\right) = \frac{1}{B} \sum_{b = 1}^B f \left(x, \mathcal{L}^{\left(B\right)}\right)\text{,}
\] em que \(f_{B}\) representa a predição agregada. No caso em que \(y\)
prediz uma classe, utiliza-se a votação majoritária. Ou seja, se
estivermos classificando em classes \(j \in {1, \dots, J}\), então
\(N_j = \#\{B; f(x, \mathcal{L}^{(b)}) = j\}\) representa o número de
vezes que a classe \(j\) foi predita pelos estimadores. Assim:

\[
f_{B}\left(x\right) = \arg \max_{j} N_j\text{.}
\] isto é, o \(j\) para o qual \(N_j\) é máximo

\vspace{12pt}

Embora a técnica de Bagging possa melhorar o desempenho de uma árvore de
regressão ou de classificação, isso geralmente vem ao custo de menor
interpretabilidade. Quando o Bagging é aplicado a uma árvore de
regressão, construímos \(B\) árvores de regressão usando \(B\) réplicas
de amostras bootstrap e tomamos a média das predições resultantes (JAMES
et al., 2013). Nesse processo, as árvores de regressão crescem até seu
máximo, sem passar pelo processo de poda, resultando em cada árvore
individual com alta variância e baixo viés. No entanto, ao agregar as
predições das \(B\) árvores, a variância é reduzida.

\vspace{12pt}

Para mitigar a falta de interpretabilidade do método Bagging aplicado a
árvores de regressão, pode-se usar a medida de impureza baseada no erro
quadrático médio como uma métrica de importância das variáveis
independentes. Um valor elevado na redução total média do erro
quadrático médio, calculado com base nas divisões realizadas por um
determinado preditor em todas as \(B\) árvores, indica que o preditor é
importante.

\vspace{12pt}

As árvores construídas pelo algoritmo de árvore de decisão se beneficiam
da proposta de agregação do Bagging, mas esse benefício é limitado
devido à correlação existente entre as árvores. Se as árvores forem
variáveis aleatórias independentes e identicamente distribuídas, cada
uma com variância \(\sigma^2\), a variância da média das previsões das
\(B\) árvores será \(\frac{1}{B} \sigma^2\). No entanto, se as árvores
forem apenas identicamente distribuídas, mas não necessariamente
independentes, e apresentarem uma correlação positiva \(\rho\), a
esperança da média das \(B\) árvores será a mesma que a esperança de uma
árvore individual. Portanto, o viés do agregado das árvores é o mesmo
das árvores individuais, e a melhoria é alcançada apenas pela redução da
variância. A variância da média das previsões será dada por:

\begin{equation}\phantomsection\label{eq-cor}{
\rho \sigma^2 + \frac{1 - \rho}{B}\sigma^2\text{.}
}\end{equation}

Isso significa que, à medida que o número de árvores \(B\) aumenta, o
segundo termo da soma se torna menos significativo. Portanto, os
benefícios da agregação proporcionados pelo algoritmo de Bagging são
limitados pela correlação entre as árvores (HASTIE et al., 2009). Mesmo
com o aumento do número de árvores no Bagging, a correlação entre elas
impede que as previsões individuais sejam completamente independentes,
resultando em menor diminuição da variância da média das previsões do
que seria esperado se as árvores fossem totalmente independentes. Uma
maneira de melhorar o algoritmo de Bagging é por meio do Random Forest,
que será descrito a seguir.

\subsection{Random Forest}\label{random-forest}

\begin{algo}

\centering{

\begin{algorithm}[H]
\caption{Algoritmo de uma Random Forest para regressão ou classificação.}
\begin{algorithmic}
\State \hspace{1em} \textbf{1.} Para b = 1 até B:

\vspace{0.8em}

    \State \hspace{2em} (a) Construa amostras bootstrap $\mathbf{\mathcal{L}}^*$ de tamanho \( N \) dos dados de
    \State \hspace{3.6em} \vspace{0.1em} treinamento.

    \State \hspace{2em} (b) Faça crescer uma árvore de floresta aleatória \( T_b \) para os dados bootstrap,
    \State \hspace{3.6em} repetindo recursivamente os seguintes passos para cada folha da árvore,
    \State \hspace{3.6em} \vspace{0.5em} até que o tamanho mínimo do nó \( n_{min} \) seja atingido:
    \State \hspace{4em} \vspace{0.1em} i. Selecione \( m \) variáveis aleatoriamente entre as \( p \) variáveis.
    \State \hspace{4em} \vspace{0.1em} ii. Escolha a melhor variável entre as \( m \).
    \State \hspace{4em} \vspace{0.1em} iii. Divida o nó em dois subnós.

\vspace{0.8em}

\State \hspace{1em} \textbf{2.} Por fim, o conjunto de árvores \( \{T_b\}^{B}_1\) é construído.

\vspace{1em}

\State \hspace{0.7em} No caso da regressão, para fazer uma predição em um novo ponto \( x \), temos a seguinte função:


$$
\hat{f}^{B}_{rf}\left(x\right) = \frac{1}{B}\sum^{B}_{b = 1} T_{b}\left(x\right)
$$

\vspace{1em}

\State \hspace{0.7em} Para a classificação é utilizado o voto majoritário. Assim, seja $\hat{C}_{b}\left(x\right)$ a previsão da classe da árvore de floresta aleatória $b$. Assim:

$$
\hat{C}^{B}_{rf}\left(x\right) = \arg \max_c \sum^{B}_{b = 1}I\left(\hat{C}_b\left(x\right) = c\right)\text{,}
$$

\State em que $c$ representa as classes possíveis.

\end{algorithmic}
\end{algorithm}

}

\caption{\label{algo-rf}Fonte: HASTIE et al. (2009, p. 588).}

\end{algo}%

~~~O algoritmo Random Forest é uma técnica derivada do método de
Bagging, mas com modificações específicas na construção das árvores. O
objetivo é melhorar a redução da variância ao diminuir a correlação
entre as árvores, sem aumentar significativamente a variabilidade. Isso
é alcançado durante o processo de crescimento das árvores por meio da
seleção aleatória de variáveis independentes.

\vspace{12pt}

No algoritmo Random Forest, ao construir uma árvore a partir de amostras
bootstrap, selecionam-se aleatoriamente \(m \leq p\) das \(p\) variáveis
independentes como candidatas para a divisão, antes de cada ramificação
(com \(m = p\) no caso do Bagging). Dessa forma, diferente do Bagging,
aqui não se considera todas as \(p\) variáveis independentes para
realizar a divisão e minimizar a impureza, mas apenas \(m\) dessas \(p\)
variáveis. A escolha aleatória de apenas \(m\) covariáveis como
candidatas para a divisão ajuda a solucionar um dos principais problemas
do algoritmo de Bagging, que tende a gerar árvores de decisão
semelhantes, resultando em previsões altamente correlacionadas. O Random
Forest busca diminuir esse problema ao criar oportunidades para que
diferentes preditores sejam considerados. Em média, uma fração
\((p -m)/ p\) das divisões nem sequer incluirá o preditor mais forte
como candidato, permitindo que outros preditores tenham a chance de
serem selecionados (JAMES et al., 2013). Esse mecanismo reduz a
correlação entre as árvores, o que, por sua vez, diminui a variabilidade
das predições produzidas pelas árvores.

\vspace{12pt}

A quantidade de variáveis independentes \(m\) selecionadas
aleatoriamente é um hiperparâmetro que pode ser estimado por meio de
validação cruzada. Valores comuns para \(m\) são
\(m=\sqrt{p}\)\hspace{0pt} com tamanho mínimo do nó igual a um para
classificação, e \(m=p/3\)\hspace{0pt} com tamanho mínimo do nó igual a
cinco para regressão (HASTIE et al., 2009). Quando o número de variáveis
é grande, mas poucas são realmente relevantes, o algoritmo Random Forest
pode ter um desempenho inferior com valores pequenos de \(m\), pois isso
reduz as chances de selecionar as variáveis mais importantes. No
entanto, usar um valor pequeno de \(m\) pode ser vantajoso quando há
muitos preditores correlacionados. Além disso, assim como no Bagging, a
Random Forest não sofre de sobreajuste com o aumento da quantidade de
árvores \(B\). Portanto, é suficiente usar um \(B\) grande o bastante
para que a taxa de erro se estabilize (JAMES et al., 2013).

\subsection{Boosting Trees}\label{boosting-trees}

~~~O Boosting, assim como o Bagging, é um método destinado a melhorar o
desempenho de modelos ou algoritmos. No entanto, neste trabalho, o
Boosting foi aplicado apenas às árvores de regressão. Portanto, a
explicação do Boosting será restrito ao caso de Boosting Trees. Seu
algoritmo pode ser observado no  Algoritmo~\ref{algo-boos} .

\vspace{12pt}

\begin{algo}

\centering{

\begin{algorithm}[H]
\caption{Método Boosting aplicado a árvores de regressão.}
\begin{algorithmic}
\State \hspace{1em} \textbf{1.} Defina $\hat{f}\left(x\right) = 0 \text{ e } r_i = y_i$ para todos os $i$ no conjunto de treinamento

\vspace{0.8em}

\State \hspace{1em} \vspace{0.8em} \textbf{2.} Para $b = 1, 2, \dots, B$, repita:

  \State \hspace{2em} (a) Ajuste uma árvore $\hat{f}^b$ com $d$ divisões para os dados de treinamento $\left(X, r\right)$.

  \State \hspace{2em} (b) Atualize $\hat{f}$ adicionando uma versão com o hiperparâmetro $\lambda$ de taxa de
  \State \hspace{3.6em} aprendizado:

$$
\hat{f}\left(x\right) \gets \hat{f}\left(x\right) + \lambda \hat{f}^b\left(x\right)
$$

  \vspace{0.1em}

  \State \hspace{2em} (c) Atualize os resíduos,

$$
r_i \gets r_i - \lambda \hat{f}^b\left(x_{i}\right)
$$

\vspace{1em}

\State \hspace{1em} \textbf{3.} Retorne o modelo de boosting,

$$
\hat{f}\left(x\right) = \sum_{b = 1}^B \lambda \hat{f}^b\left(x\right)
$$


\end{algorithmic}
\end{algorithm}

}

\caption{\label{algo-boos}Fonte: JAMES et al. (2013, p. 349).}

\end{algo}%

No algoritmo de Bagging, cada árvore é construída e ajustada utilizando
amostras bootstrap, e ao final, um estimador agregado
\(\varphi_B\)\hspace{0pt} é formado a partir das \(B\) árvores. O
Boosting Trees funciona de maneira semelhante, mas sem o uso de amostras
bootstrap. A ideia central do Boosting é corrigir os erros das árvores
anteriores, ajustando as novas árvores aos resíduos gerados pelas
predições anteriores, com o objetivo de aprimorar a precisão do modelo.
Assim, as árvores no Boosting são construídas de forma sequencial,
incorporando gradualmente as informações dos erros cometidos pelas
árvores anteriores.

\vspace{12pt}

No caso da regressão, o Boosting combina um grande número de árvores de
decisão \(\hat{f}^1, \dots, \hat{f}^B\). A primeira árvore é construída
utilizando o conjunto de dados original, e seus resíduos são calculados.
Com a primeira árvore ajustada, a segunda árvore é ajustada aos resíduos
resíduos da árvore anterior e, em seguida, é adicionada ao estimador
para atualizar os resíduos. Dessa forma, os resíduos servem como
informação crucial para construir novas árvores e corrigir os erros das
árvores anteriores. Como cada nova árvore depende das árvores já
construídas, árvores menores são suficientes (JAMES et al., 2013).

\vspace{12pt}

O processo de aprendizado no método de Boosting é lenta, o que acaba
gerando melhores resultados. Esse processo de aprendizado pode ser
controlado por um hiperparâmetro \(\lambda\) chamado de shrinkage, ou
taxa de aprendizado, permitindo que mais árvores, com formas diferentes,
corrijam os erros das árvores passadas. No entanto, um valor muito
pequeno para \(\lambda\) requer uma quantidade muito maior \(B\) de
árvores e, diferente do Bagging e Random Forest, o Boosting pode sofrer
de sobreajuste se a quantidade de árvores é muito grande. Além disso, a
quantidade de divisões \(d\) em cada árvore, que controla a complexidade
do boosting, pode ser considerado também um hiperparâmetro. Para
\(d = 1\) é ajustado um modelo aditivo, já que cada termo involve apenas
uma variável. JAMES et al. (2013) define \(d\) como a profundidade de
interação que controla a ondem de interação do modelo boosting, já que
\(d\) divisões podem envolver no máximo \(d\) variáveis.

\subsection{Stacked generalization}\label{stacked-generalization}

~~~A Stacked Generalization, ou Stacking, é um método de ensemble que
consiste em treinar um modelo gerado a partir da combinação da predição
de vários outros modelos, visando melhorar a precisão das predições.
Esse método pode ser aplicado a qualquer modelo estatístico ou algoritmo
de aprendizado de máquina. A ideia principal é atribuir pesos às
predições, de modo a dar maior importância aos modelos que produzem
melhores resultados, ao mesmo tempo em que se evita atribuir altos pesos
a modelos com alta complexidade.

\vspace{12pt}

Matematicamente, o Stacking define predições
\(\hat{f}_m^{-i}\left(x\right)\) em \(x\), utilizando o modelo \(m\),
aplicado ao conjunto de treinamento com a \(i\text{-ésima}\) observação
removida (HASTIE et al., 2009). Os pesos dos modelos são estimados por
meio de uma regressão linear de mínimos quadrados, ajustando \(y_i\) em
relação a \(\hat{f}^{-i}\left(x\right)\), para \(m=1,\dots,M\). A
estimativa dos pesos é obtida pela seguinte expressão:

\[
\hat{w}^{st} = \arg \min_{w} \sum^{N}_{i = 1} \left[y_i - \sum^{M}_{m = 1} w_m f^{-i}_m\left(x_i\right)\right]^2\text{.}
\]

A previsão final é \(\sum_{m} \hat{w}_m^{st} \hat{f}_m\left(x\right)\).
Assim, em vez de escolher um único modelo, o método de Stacking combina
os modelos utilizando pesos estimados, o que melhora a performance
preditiva, mas pode comprometer a interpretabilidade.

\subsection{Gradient Boosting}\label{gradient-boosting}

~~~O algoritmo de Gradient Boosting é semelhante ao de Boosting, mas com
diferenças mínimas. Ele constrói modelos aditivos ajustando
sequencialmente funções bases aos pseudos-resíduos, que correspondem aos
gradientes da função perda do modelo atual (FRIEDMAN, 2002). Esses
gradientes indicam a direção na qual a função perda diminui. Neste
trabalho, foram utilizadas diferentes implementações de Gradient
Boosting. No entanto, todas empregam o Gradient Boosting com árvores de
regressão, com algumas modificações para a construção das árvores ou
para melhorar a eficiência do algoritmo existente. Assim, o algoritmo a
ser explicado será o Gradient Tree Boosting. O
 Algoritmo~\ref{algo-gradboos}  demonstra a sua construção.

\begin{algo}

\centering{

\begin{algorithm}[H]
\caption{Algoritmo de Gradient Tree Boosting.}
\begin{algorithmic}
\State \hspace{1em} \textbf{1.} Inicialize $f_0\left(x\right) = \arg \min_{\gamma} \sum_{i = 1}^N L\left(y_i, \gamma \right)$

\vspace{0.8em}

\State \hspace{1em} \vspace{0.8em} \textbf{2.} Para $m = 1$ até $M$:

  \State \hspace{2em} (a) Para $i = 1, 2, \dots, N$, calcule

$$
{r}_{im} = -\left[\frac{\partial L\left(y_i, f\left(x_i \right)\right)}{\partial f\left(x_i\right)}\right]_{f = f_{m - 1}}
$$

  \State \hspace{2em} (b) Ajuste uma árvore de regressão aos pseudo-resíduos $r_{im}$, obtendo regiões
  \State \hspace{3.6em} terminais $R_{jm}, \ j = 1, 2, \dots, J_m$.


  \State \vspace{0.1em}

  \State \hspace{2em} (c) Para $j = 1, 2, \dots, J_m$, calcule

$$
\gamma_{jm} = \arg \min_{\gamma} \sum_{x_i \in R_{jm}} L\left(y_i, f_{m - 1}\left(x_i\right) + \gamma\right)
$$

  \vspace{0.1em}

  \State \hspace{2em} (d) Atualize $f_m\left(x\right) = f_{m - 1}\left(x\right) + \lambda \sum^{J}_{j = 1} \gamma_{jm} I\left(x \in R_{jm}\right)$

\vspace{1em}

\State \hspace{1em} \textbf{3.} Retorne $\hat{f}\left(x\right) = f_M\left(x\right)$

\end{algorithmic}
\end{algorithm}

}

\caption{\label{algo-gradboos}Fonte: HASTIE et al. (2009, p. 361).}

\end{algo}%

\vspace{12pt}

O Gradient Boosting aplicado para árvores de regressão, tem que cada
função base é uma árvore de regressão com \(J_m\) folhas. Dessa forma,
cada árvore de regressão tem a seguinte forma aditiva:

\begin{equation}\phantomsection\label{eq-treebost}{
h_m\left(x;\{b_j, R_j\}^J_{1}\right) = \sum^{J_m}_{j = 1} b_{jm} I\left(x \in R_{jm}\right)\text{,}
}\end{equation} em que \(\{R_{jm}\}^{J_m}_{1}\) são as regiões disjuntas
que, coletivamente, cobrem o espaço de todos os valores conjuntos das
variáveis preditoras. Essas regiões são representadas pelas folhas de
sua correspondente árvore. Como as regiões são disjuntas, a
Equação~\ref{eq-treebost} se reduz simplesmente a
\(h_m\left(x\right) = b_{jm}\text{ para } x \in  R_{jm}\). Por mínimos
quadrados, \(b_{jm}\) é simplesmente a média dos pseudo-resíduos
\(r_{im}\)

\[
\hat{b}_{jm} = \frac{1}{N_{jm}} \sum_{x_i \in R_{jm}} r_{im}\text{,}
\] que dão a direção de diminuição da função perda \(L\) pela expressão
do gradiente da linha 2(a), no  Algoritmo~\ref{algo-gradboos} . Assim,
cada árvore de regressão é ajustada aos \(r_{im}\) de forma a minimizar
o erro das árvores anteriores. \(N_{jm}\) denota a quantidade de pontos
na região \(R_{jm}\). Por fim, o estimador é separadamente atualizado em
cada região correspondente e é expresso:

\[
f_m\left(x\right) = f_{m - 1}\left(x\right) + \lambda \sum^{J}_{j = 1} \gamma_{jm} I\left(x \in R_{jm}\right)\text{,}
\] em que \(\gamma_{jm}\) representa a atualização da constante ótima
para cada região, baseado na função perda \(L\), dada a aproximação
\(f_{m-1}\left(x\right)\). O \(0 < \lambda \leq 1\), assim como no
algoritmo de boosting, representa o hiperparâmetro shrinkage para
controlar a taxa de aprendizado. Pequenos valores de \(\lambda\)
necessitam maiores quantidades de iterações \(M\) para diminuir o risco
de treinamento.

\vspace{12pt}

As outras implementações de Gradient Boosting aplicadas à árvores de
decisão tem seus próprios motivos de existência. Esses motivos incluem a
busca por maior eficiência computacional, adição de recursos e até mesmo
maior flexibilidade. As duas outras implementações utilizadas foram o
Extreme Gradient Boosting e Light Gradient Boosting.

\vspace{12pt}

O Extreme Gradient Boosting é uma implementação altamente eficiente do
algoritmo e flexível de Gradient Boosting aplicado à árvores de decisão.
Além disso, é adicionado um novo recurso, técnicas de regularização para
diminuir o sobreajuste do modelo. A função objetivo agora passa a ser
definida da seguinte forma:

\[
\mathcal{L}\left(\phi\right) = \sum_{i}l\left(\hat{y}_{i}, y_{i}\right) + \sum_{k}\Omega\left(f_{k}\right)\text{,}
\] em que \(l\) é uma função de perda convexa diferenciável e o segundo
termo \(\Omega\) penaliza a complexidade de cada função de árvore de
decisão. O termo de regularização adicional ajuda a suavizar os pesos
finais para evitar o sobreajuste. \(\Omega\) é definido como:

\[
\Omega\left(f\right) = \gamma T + \frac{1}{2}\lambda\|\omega\|^{2}\text{,}
\] em que \(T\) é a quantidade de folhas na árvore, \(\|\omega\|^{2}\) é
a soma do quadrado dos pesos associados às folhas. \(\gamma\) e
\(\lambda\) são os parâmetros de regularização, em que \(\lambda\)
penaliza os pesos das folhas e \(\gamma\) penaliza a quantidade de
folhas nas árvores. Assim, com a regularização aplicada a função
objetivo, modelos e funções preditivas serão mais comuns pois
simplificarão o modelo. As implementações do algoritmo de Extreme
Gradient Boosting presentes em \textbf{Python} utilizam a regularização
L2 como padrão, mas também permitem o uso da regularização L1.

\vspace{12pt}

Assim como o Gradient Boosting, as implementações existentes de Light
Gradient Boosting também permite utilizar a regularização L1 ou L2, mas
não utiliza nenhuma das duas por padrão. A principal diferença do Light
Gradient Boosting está na forma de crescimento das árvores, primeiro é
dividido a folha com a maior redução na função de erro. Além disso, tem
um grande foco na velocidade e uma melhor eficiência no uso da memória
atráves do algoritmo de Histogram-Based. Esse algoritmo faz com que, em
vez de avaliar cada ponto quando realiza a divisão, ele divide valores
contínuos em intervalos, como em um histograma, o que reduz grandemente
o custo computacional.

\chapter{Metodologia}\label{metodologia}

\section{Obtenção dos dados}\label{obtenuxe7uxe3o-dos-dados}

~~~Os dados foram obtidos por meio de web scraping, que é uma técnica
para coleta automatizada de dados de páginas da internet. Para isso,
foram utilizadas as linguagens de programação \textbf{R} e
\textbf{Python}. No \textbf{R}, os pacotes
\href{https://xml2.r-lib.org/}{\textbf{xml2}} e
\href{https://rvest.tidyverse.org/}{\textbf{rvest}} foram utilizados
para extrair dados de páginas estáticas de forma estruturada. No
\textbf{Python}, as bibliotecas
\href{https://scrapy.org/}{\textbf{Scrapy}} e
\href{https://playwright.dev/python/}{\textbf{Playwright}}, desenvolvida
pela Microsoft, foram empregadas, sendo esta última essencial para a
interação com componentes dinâmicos da página, possibilitando a extração
de informações que exigem interações como cliques ou rolagem de página
para ser gerada. Além dessas ferramentas, foram implementadas técnicas
de rotacionamento de IPs e de modificação das informações do usuário que
acessa o site, a fim de evitar bloqueios durante o processo de coleta de
dados, garantindo assim a continuidade e eficácia da extração. A API
utilizada para gerenciar a rotação de informações dos usuários que
acessam o site foi desenvolvida pela empresa ScrapeOps. Essa ferramenta
é disponibilizada gratuitamente e pode ser acessada através do domínio
\url{https://scrapeops.io/} após a criação de uma conta no site.

\vspace{12pt}

Assim, utilizando as ferramentas e técnicas de web scraping, foram
coletadas as variáveis que poderiam fazer sentido para a modelagem,
priorizando aquelas com menor chance de gerar problemas durante o
tratamento dos dados, como, por exemplo, ter muitos valores ausentes. Ao
todo, foram extraídas 25 variáveis, das quais 10 são quantitativas e 15
qualitativas nominais, sendo 13 de caráter dicotômico. No entanto, nem
todas as variáveis foram obtidas diretamente por web scraping. As
coordenadas geográficas, latitude e longitude, por exemplo, foram
geradas por meio da geocodificação dos endereços de cada imóvel,
utilizando o pacote
\href{https://jessecambon.github.io/tidygeocoder/}{\textbf{tidygeocoder}}
da linguagem \textbf{R}. Dessa forma, tem-se as seguintes variáveis:

\begin{itemize}
\item
  Valor do imóvel: variável dependente que será modelada e constitui o
  principal foco de análise deste trabalho;
\item
  Valor médio do aluguel no bairro: valor médio do aluguel dos imóveis
  no bairro, em \(m^3\);
\item
  Área: área total do imóvel, medida em \(m^2\);
\item
  Área média do aluguel no bairro: área média dos imóveis alugados no
  bairro, em \(m^2\);
\item
  Condomínio: valor mensal pago pelo condomínio do imóvel;
\item
  IPTU: imposto cobrado sobre imóveis urbanos;
\item
  Banheiros: quantidade de banheiros disponíveis na propriedade;
\item
  Vagas de estacionamento: número total de vagas de estacionamento
  disponíveis;
\item
  Quartos: quantidade de quartos no imóvel;
\item
  Latitude: posição horizontal, medida em frações decimais de graus;
\item
  Longitude: posição vertical, também medida em frações decimais de
  graus, assim como a latitude;
\item
  Tipo do imóvel: sete categorias foram consideradas: apartamentos,
  casas, casas comerciais, casas de condomínio, casas de vila,
  coberturas, e lotes comerciais e de condomínio;
\item
  Endereço: nome do endereço onde o imóvel está localizado;
\item
  Variáveis dicotômicas: indicam a presença (1) ou ausência (0) de
  determinadas características no imóvel, como área de serviço,
  academia, elevador, espaço gourmet, piscina, playground, portaria 24
  horas, quadra de esportes, salão de festas, sauna, spa e varanda
  gourmet.
\end{itemize}

\vspace{12pt}

No entanto, com base nas observações realizadas durante o estudo, nem
todas as variáveis coletadas foram utilizadas na modelagem do valor dos
imóveis. Algumas foram excluídas devido a uma quantidade excessiva de
valores ausentes, enquanto outras se mostraram pouco significativas para
explicar o valor do imóvel. Após o processo de coleta e limpeza dos
dados, o banco de dados final conta com 31.782 observações.

\section{Análise exploratória de
dados}\label{anuxe1lise-exploratuxf3ria-de-dados}

~~~A análise exploratória de dados é uma das primeiras etapas de
qualquer estudo que utiliza a estatística como ferramenta principal,
pois permite identificar padrões de comportamento nos dados e descobrir
relações entre as variáveis estudadas. Assim, após a coleta e
organização dos dados, a primeira etapa deste estudo consistiu em uma
análise descritiva. Essa análise possibilitou identificar padrões entre
os diferentes tipos de imóveis e como essas características podem
influenciar o seu valor. Para evidenciar esses comportamentos, foram
criados gráficos e tabelas que permitiram caracterizar as relações entre
as variáveis independentes e a variável dependente.

\vspace{12pt}

Nesta etapa, também foi analisada a correlação entre as variáveis
dependentes utilizadas no modelo, o que possibilitou a criação de novas
variáveis com base na interação entre aquelas que apresentavam alguma
relação. Para isso, utilizou-se a correlação de Spearman (SPEARMAN,
1961).

\vspace{12pt}

A correlação de Spearman é bastante semelhante à correlação de Pearson
(COHEN et al., 2009). No entanto, diferencia-se por ser uma medida não
paramétrica de correlação entre os postos das variáveis. Assim, o
coeficiente de correlação de Spearman é definido como o coeficiente de
correlação de Pearson aplicado às variáveis classificadas em postos:

\[
r_{s} = \rho_{rg_{X}, rg_{Y}} = \frac{cov\left(rg_{X}, rg_{Y}\right)}{\sigma_{rg_X}\sigma_{rg_Y}} \text{,}
\] em que \(\rho\) representa o coeficiente de Pearson aplicado aos
postos das variáveis, \(cov\left(rg_{X}, rg_{Y}\right)\) é a covariância
entre os postos, e \(\sigma_{rg_X}\) e \(\sigma_{rg_Y}\) são os desvios
padrão das variáveis em postos.

\vspace{12pt}

Portanto, essa etapa resultou na criação de três variáveis: a quantidade
total de cômodos do imóvel, o produto entre as coordenadas geográficas e
o preço médio do aluguel no bairro, a razão entre o número de quartos e
a área do imóvel, e o produto entre o número de quartos do imóvel e a
área média do aluguel no bairro. No entanto, a inclusão dessas variáveis
não levou à remoção das variáveis originais utilizadas em sua
construção, pois sua exclusão resultou em uma piora no desempenho do
modelo.

\vspace{12pt}

Com um maior entendimento dos dados, o próximo passo foi a construção do
modelo, incluindo as transformações aplicadas, o método de imputação de
valores ausentes e o tratamento de variáveis nominais. A próxima seção
detalhará os métodos utilizados nesse processo.

\section{Construção do modelo}\label{construuxe7uxe3o-do-modelo}

~~~No conjunto de dados extraído, foram avaliados diferentes modelos
para a previsão do valor do imóvel. O valor do imóvel foi explicado por
variáveis consideradas relevantes para o estudo, como: preço médio do
aluguel no bairro, área, área média do aluguel no bairro, número de
banheiros, vagas de estacionamento, número de quartos, latitude,
longitude, tipo de imóvel, variáveis dicotômicas obtidas durante o
processo de extração e variáveis criadas posteriormente ao processo de
raspagem dos dados. As variáveis relacionadas ao valor do condomínio e
IPTU foram excluídas do modelo devido à alta quantidade de valores
ausentes.

\vspace{12pt}

Após a seleção das variáveis, o conjunto de dados foi dividido em
conjuntos de treino e teste para avaliar o desempenho dos modelos. A
divisão foi realizada de forma estratificada, utilizando a classe
\texttt{sklearn.model\_selection.StratifiedShuffleSplit}, da biblioteca
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}}. Essa
classe permite dividir a base de dados de maneira aleatória, preservando
a proporção das classes definidas.

\vspace{12pt}

A estratificação foi baseada em intervalos criados a partir da variável
que representa o valor dos imóveis. Foram definidos cinco intervalos: o
primeiro abrange valores entre o mínimo e 200.000; o segundo, entre
200.000 e 400.000; o terceiro, entre 400.000 e 600.000; o quarto, entre
600.000 e 800.000; e o último intervalo cobre os valores de 800.000 até
o máximo da variável. Dessa forma, 20\% do conjunto de dados foi
reservado para o teste, enquanto os 80\% restantes foram utilizados para
o treinamento do modelo.

\vspace{12pt}

Para a aplicação das ferramentas de modelagem, foram utilizadas as
bibliotecas
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}},
\href{https://lightgbm.readthedocs.io/en/stable/}{\textbf{LightGBM}} e
\href{https://xgboost.readthedocs.io/en/stable/}{\textbf{XGBoost}}. As
duas últimas foram empregadas especificamente na modelagem, enquanto a
primeira também foi utilizada para criar pipelines de pré-processamento
de dados, que organizam etapas sequenciais de preparação necessárias
para o tratamento adequado dos dados. Assim, os quatro modelos aplicados
na previsão do valor do imóvel foram: Random Forest, Gradient Boosting,
LightGBM e XGBoost. Por fim, foi implementado o algoritmo de Stacking,
combinando os modelos previamente construídos para melhorar a
performance preditiva. No Stacking, foi utilizado como preditor final o
algoritmo de Light Gradient Boosting.

\subsection{Etapas de
pré-processamento}\label{etapas-de-pruxe9-processamento}

~~~Após a organização e limpeza dos dados, foram realizadas
transformações nas variáveis para aprimorar a capacidade preditiva dos
modelos em relação aos valores dos imóveis. Além disso, foi aplicado um
tratamento específico para lidar com valores ausentes em algumas
variáveis do conjunto de dados. Esse tratamento foi restrito às
variáveis com menos de 20\% de valores ausentes, incluindo as variáveis
de número de banheiros, quartos, vagas, valor médio do aluguel e área
média do aluguel. Por outro lado, as variáveis de condomínio e IPTU
apresentaram um elevado percentual de valores ausentes, com a variável
de condomínio tendo quase 60\% de observações ausentes e a variável IPTU
mais de 80\%. Devido a essa alta proporção de valores ausentes, essas
variáveis foram excluídas.

\vspace{12pt}

O método utilizado para a imputação de valores ausentes foi o algoritmo
k-nearest neighbors (KNN). Esse algoritmo estima os valores ausentes de
acordo com a seguinte fórmula:

\[
\hat{y} = \frac{1}{k}\sum_{x_i \in N_k\left(x\right)}y_i\text{,}
\] em que \(N_k(x)\) representa o conjunto de \(k\) vizinhos mais
próximos de \(x\), ou seja, os pontos \(x_i\) no conjunto de dados que
estão mais próximos de \(x\). Essa proximidade é geralmente medida pela
distância Euclidiana, que é a métrica padrão utilizada pela classe
\texttt{KNNImputer} da biblioteca
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}} para
imputação de valores ausentes. No processo de imputação, foi utilizado
um total de 17 vizinhos, definido pelo argumento \texttt{n\_neighbors}
da classe \texttt{KNNImputer}.

\vspace{12pt}

A transformação logarítmica \(\log\left(1 + x\right)\) foi aplicada para
estabilizar a variância e tornar a distribuição dos regressores mais
simétrica. Embora essa transformação tenha sido utilizada na maioria das
variáveis numéricas, algumas exceções foram feitas. Em particular, a
variável que representa o produto entre as coordenadas geográficas e o
valor do aluguel foi transformada usando a técnica de Yeo-Johnson (YEO;
JOHNSON, 2000), que apresenta a vantagem de ser aplicável a dados que
incluem valores negativos e zero, como é o caso dessa variável. Além
disso, as variáveis referentes ao preço e à área do aluguel não foram
transformadas com \(\log\left(1 + x\right)\). A decisão foi tomada com
base nas métricas de avaliação do modelo, que indicaram uma queda de
desempenho do modelo quando essas variáveis eram transformadas.

\vspace{12pt}

A transformação de Yeo-Johnson é definida da seguinte forma:

\[
\psi(\lambda, x) = \begin{cases}
    [(1 + x)^\lambda - 1] / \lambda  &  \lambda \neq 0, \; x \ge 0 \\
    \ln(1 + x)                       &  \lambda = 0, \; x \ge 0 \\
    [(1 - x)^{2 - \lambda} - 1] / (\lambda - 2) \quad & \lambda \neq 2, \; x < 0 \\
    -\ln(1 - x)                     &   \lambda = 2, \; x < 0
\end{cases} \text{,}
\] em que \(\lambda\) é estimado por máxima verossimilhança. Além disso,
percebe-se que a transformação \(\log(1 + x)\) é um caso particular da
transformação de Yeo-Johnson quando \(\lambda = 0\) e \(x \ge 0\).

\vspace{12pt}

Para as variáveis categóricas, utilizou-se a classe
\texttt{OneHotEncoder}, da biblioteca
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}}, que
converte as categorias em variáveis dicotômicas, criando uma nova coluna
para cada categoria. Especificamente, a \texttt{OneHotEncoder} foi
aplicada à variável que representa o tipo de imóvel, transformando cada
categoria em uma variável binária.

\vspace{12pt}

Após a aplicação da transformação logarítmica às variáveis numéricas,
essas variáveis também foram padronizadas utilizando a classe
\texttt{StandardScaler}, disponível no módulo \texttt{preprocessing} da
biblioteca
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}}. Essa
classe ajusta os dados para que fiquem na mesma escala, padronizando-os
de acordo com a fórmula:

\[
z = \frac{x - \mu}{\sigma}\text{,}
\] em que \(\mu\) representa a média e \(\sigma\) o desvio padrão. Essa
padronização é essencial para garantir que os modelos estatísticos e de
aprendizado de máquina tratem as variáveis em escalas consistentes.

\subsection{Validação cruzada}\label{validauxe7uxe3o-cruzada}

~~~A técnica utilizada para otimizar e determinar os hiperparâmetros dos
modelos, além de identificar os vizinhos mais próximos para a imputação
de valores ausentes, foi a validação cruzada. A validação cruzada serve
para estimar um erro de generalização médio da seguinte forma
\(Err = E\left[L\left(Y, \hat{f}\left(X\right)\right)\right]\), em que
\(L\) é uma função perda e \(\hat f\) é um estimador. Existem diversar
técnicas de validação cruzada, a que foi utilizada nesse trabalho é a
validação cruzada K-Fold.

\vspace{12pt}

A validação cruzada K-Fold é uma técnica que utiliza parte dos dados
para ajustar o modelo e outra parte para testá-lo. Nessa abordagem, os
dados são divididos em \(K\) folds. Em cada iteração, um desses folds é
reservado para testar o modelo, enquanto os \(K−1\) folds restantes são
usados para treiná-lo. O modelo é ajustado nos \(K−1\) subconjuntos e
avaliado no subconjunto de teste, permitindo estimar o erro de predição.
Esse processo é repetido \(K\) vezes, alternando o subconjunto de teste
em cada rodada, e ao final, os \(K\) erros de predição são combinados. O
erro de predição estimado pela validação cruzada é dado por:

\[
CV\left(\hat f\right) = \frac{1}{N}\sum_{i = 1}^N L\left(y_i, \hat{f}^{-k\left(i\right)}\left(x_i\right)\right)\text{,}
\] em que \(N\) representa o número total de observações, \(L\) é a
função de perda, \(y_i\) é o valor observado, \(x_i\) é a entrada
correspondente, \(\hat{f}^{-k(i)}\) é o modelo ajustado sem o
\(k\)-ésimo subconjunto, e
\(k: \{1, \dots, N\} \mapsto \{1, \dots, K\}\) indica a partição à qual
a observação \(i\) foi alocada por meio de randomização.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-kfold-output-1.pdf}}

}

\caption{\label{fig-kfold}Visualização de K-Fold com 20 folds.}

\end{figure}%

\vspace{12pt}

A Figura~\ref{fig-kfold} ilustra exatamente o caso da validação cruzada
por K-Fold. As divisões do conjunto de dados para treinamento são
representados pela cor azul, enquanto as laranjas são os conjuntos de
validação, onde o modelo, após ser ajustado, será testado com uma função
perda \(L\).

\vspace{12pt}

Para realizar a validação cruzada com K-Fold nos modelos empregados
neste trabalho, utilizou-se uma função e uma classe da biblioteca
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}}, ambas
disponíveis no módulo \texttt{model\_selection}. A função utilizada foi
a \texttt{cross\_val\_score}, que recebe o modelo por meio do argumento
\texttt{estimator}. A função perda para avaliação é especificado pelo
argumento \texttt{scoring}, enquanto a técnica de validação cruzada é
definida pelo argumento \texttt{cv}. Especificamente, foi empregada a
classe \texttt{KFold(n\_splits=20)} para configurar a validação cruzada
do tipo K-Fold, em que o parâmetro \texttt{n\_splits} define o número de
divisões (folds) a serem realizadas, neste caso, 20. Após a execução da
validação, a média das métricas retornadas pela função
\texttt{cross\_val\_score} para cada fold foi calculada, como forma de
se obter uma estimativa do desempenho do modelo. O código abaixo é um
exemplo de como se utilizar a validação cruzada em \textbf{Python}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ cross\_val\_score, KFold}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestClassifier}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load\_iris}

\NormalTok{X, y }\OperatorTok{=}\NormalTok{ load\_iris(return\_X\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{model }\OperatorTok{=}\NormalTok{ RandomForestClassifier()}

\NormalTok{kf }\OperatorTok{=}\NormalTok{ KFold(n\_splits}\OperatorTok{=}\DecValTok{20}\NormalTok{)}

\NormalTok{scores }\OperatorTok{=}\NormalTok{ cross\_val\_score(}
\NormalTok{  estimator}\OperatorTok{=}\NormalTok{model,}
\NormalTok{  X}\OperatorTok{=}\NormalTok{X, y}\OperatorTok{=}\NormalTok{y,}
\NormalTok{  scoring}\OperatorTok{=}\StringTok{\textquotesingle{}neg\_mean\_squared\_error\textquotesingle{}}\NormalTok{,}
\NormalTok{  cv}\OperatorTok{=}\NormalTok{kf)}

\NormalTok{mse\_scores }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{ scores}
\NormalTok{mean\_mse }\OperatorTok{=}\NormalTok{ mse\_scores.mean()}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}Média de acurácia: }\SpecialCharTok{\{}\NormalTok{mean\_mse}\SpecialCharTok{:.4f\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\vspace{12pt}

A métrica utilizada para avaliar o desempenho dos modelos durante a
validação cruzada foi a raiz do erro quadrático médio (RMSE). O RMSE
avalia, em média, o quanto os valores estimados pelo modelo se afastam
dos valores observados, sendo que valores menores de RMSE indicam melhor
desempenho do modelo.

\vspace{12pt}

Embora não tenha sido utilizada na validação cruzada, outra métrica
considerada para a análise do desempenho dos modelos foi o MAPE (Erro
Percentual Absoluto Médio). Essa métrica mede, em termos percentuais, o
desvio médio entre os valores estimados e os valores observados,
oferecendo uma interpretação relativa ao erro. Por fim, foi analisado o
coeficiente de determinação (\(R^2\)), que indica a proporção da
variância da variável dependente explicada pelas variáveis
independentes. Neste caso, o \(R^2\) avalia o quão bem as variáveis
utilizadas para a contrução do modelo conseguem representar a variação
no valor do imóvel. As métricas utilizadas são definidas da seguinte
forma:

\begin{figure}

\begin{minipage}{0.33\linewidth}
\[
\text{RMSE} = \sqrt{\dfrac{1}{n} \sum_{i = 0}^n (y_i - \hat y_i)^2}
\]\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\[
R^2 = 1 - \dfrac{SS_{\text{resíduos}}}{SS_{\text{total}}}
\]\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\[
\text{MAPE} = \frac{1}{n} \sum_{i=0}^n \left|1 - \frac{y_i}{\hat y_i}\right|\text{,}
\]\end{minipage}%

\end{figure}%

em que \(SS_{\text{resíduos}}\) e \(SS_{\text{total}}\) representam,
respectivamente, a soma dos quadrados dos resíduos e a soma dos
quadrados totais.

\section{Otimização de
hiperparâmetros}\label{otimizauxe7uxe3o-de-hiperparuxe2metros}

~~~Existem diversas técnicas para a otimização de hiperparâmetros em
aprendizado de máquina. Uma das mais comuns é o Grid Search. Segundo
BISCHL et al. (2023), o Grid Search divide o intervalo contínuo de
valores possíveis de cada hiperparâmetro em um conjunto de valores
discretos, avaliando exaustivamente o desempenho do algoritmo para todas
as combinações possíveis. No entanto, como o número de combinações
cresce exponencialmente com o aumento do número de hiperparâmetros, o
Grid Search apresenta um custo computacional elevado.

\vspace{12pt}

Por essa razão, métodos de otimização mais avançados, como a otimização
bayesiana, têm ganhado destaque, pois oferecem um desempenho superior ao
explorar o espaço de hiperparâmetros de maneira mais eficiente. Neste
trabalho, utilizamos a otimização bayesiana para realizar a otimização
dos modelos e determinar a quantidade ideal de vizinhos mais próximos na
classe \texttt{KNNImputer}, empregada para a imputação de valores
ausentes.

\vspace{12pt}

A otimização bayesiana não se refere a um algoritmo específico, mas sim
a uma abordagem de otimização fundamentada na inferência bayesiana, que
engloba uma ampla família de algoritmos (GARNETT, 2023). Além disso, a
otimização bayesiana tem alcançado benchmarks superiores em comparação
com outros algoritmos em diversos problemas complexos de otimização de
hiperparâmetros (SNOEK; LAROCHELLE; ADAMS, 2012).

\vspace{12pt}

Diferentemente de outros algoritmos de otimização de hiperparâmetros, a
otimização bayesiana ajusta suas tentativas futuras com base nos
resultados obtidos anteriormente (YANG; SHAMI, 2020). Para definir os
pontos de avaliação futuros, utiliza-se uma função probabilística
\(P\left(c | \lambda \right)\), que modela a relação entre os
hiperparâmetros \(\lambda\) e a métrica de desempenho \(c\) (BERGSTRA;
YAMINS; COX, 2013). A partir dessa função, estima-se, para cada conjunto
de hiperparâmetros \(\lambda\), a performance esperada
\(\hat{c}\left(\lambda\right)\) e a incerteza associada à predição
\(\hat{\sigma}\left(\lambda\right)\). Com essas estimativas, a
distribuição preditiva derivada da função probabilística permite a
aplicação de uma função de aquisição. Essa função orienta a escolha dos
próximos pontos a serem avaliados, equilibrando exploitation (explorar
regiões próximas às melhores observações anteriores) e exploration
(investigar áreas ainda não exploradas)\footnote{Exploitation refere-se
  à busca por soluções promissoras com base em dados prévios, enquanto
  exploration visa descobrir novas regiões potencialmente vantajosas}.

\vspace{12pt}

Portanto, os algoritmos de otimização bayesiana são regidos pela relação
\(\lambda \rightarrow c\left(\lambda \right)\) e buscam equilibrar
exploitation e exploration. Isso permite identificar as regiões mais
promissoras no espaço de hiperparâmetros, ao mesmo tempo em que evita
negligenciar possíveis configurações melhores em áreas ainda
inexploradas.

\subsection{Tree-Structured Parzen
Estimator}\label{tree-structured-parzen-estimator}

~~~A função probabilística utilizada para a otimização bayesiana neste
trabalho foi a Tree-Structured Parzen Estimator (TPE). O TPE define duas
funções de densidade, \(l\left(\lambda\right)\) e
\(g\left(\lambda\right)\), que são empregadas para modelar a
distribuição das variáveis no domínio dos hiperparâmetros \(\lambda\)
(YANG; SHAMI, 2020). Essas densidades são utilizadas para estimar a
probabilidade condicional \(p\left(\lambda | y\right)\) de observar uma
combinação de hiperparâmetros \(\lambda\), dado um valor de métrica de
desempenho \(y\). A definição é dada por:

\begin{equation}\phantomsection\label{eq-tpe}{
P(\lambda|y) =
\begin{cases}
    l(\lambda) & \text{if } y < y^* \\
    g(\lambda) & \text{if } y \ge y^*
\end{cases}\text{,}
}\end{equation} em que \(l\left(\lambda\right)\) representa a densidade
associada aos valores de \(y\) menores que o limiar \(y^*\) e
\(g\left(\lambda\right)\) é a densidade associada aos valores de \(y\)
iguais ou superiores a \(y^*\) (BERGSTRA et al., 2011). No algoritmo de
TPE, o valor de \(y^*\) é definido como sendo um quantil \(\gamma\) dos
valores observados de \(y\), de forma que
\(p\left(y < y^*\right) = \gamma\).

\vspace{12pt}

Por padrão, o Tree-Structured Parzen Estimator (TPE) utiliza como função
de aquisição o Expected Improvement (EI). O EI representa a expectativa
de um modelo \(M\), que mapeia \(f:\Lambda \rightarrow \mathbb{R}^N\),
sobre a melhora esperada em relação a um limiar \(y^*\). Formalmente, o
EI é definido como:

\[
EI_{y^*}\left(\lambda\right) = \int_{-\infty}^{\infty} \max\left(y^* - y, 0\right)p_{M}\left(y|\lambda\right)dy\text{.}
\]

Se, para o valor de \(\lambda\), o modelo prevê um \(y\) tal que
\(y > y^∗\), a diferença \(y^∗ − y\) será negativa, e o retorno será 0,
o que significa que não haverá melhora. Por outro lado, se \(y < y^∗\),
a diferença será positiva, indicando que o modelo apresenta um
desempenho superior em relação ao limiar \(y^∗\).

\vspace{12pt}

Portanto, para o cálculo do EI utilizando as definições fornecidas pelo
TPE e assumindo que \(y^∗>y\), adota-se a parametrização de
\(p\left(\lambda,y\right)\) como
\(p\left(y\right)p\left(\lambda∣y\right)\), com o objetivo de
simplificar os cálculos. Assim, a expressão do EI, para o TPE, se reduz
a:

\begin{equation}\phantomsection\label{eq-exp}{
EI_{y^*}\left(\lambda\right) = \int_{-\infty}^{y^*} \left(y^* - y\right)p\left(y | \lambda\right) dy = \int_{-\infty}^{y^*} \left(y^* - y\right)\frac{p\left(\lambda | y\right) p\left(y \right)}{p\left(\lambda\right)} dy\text{.}
}\end{equation}

A partir da Equação~\ref{eq-tpe} e da definição
\(p\left(y < y^*\right) = \gamma\) e
\(p\left(y \geq y^*\right) = 1 - \gamma\), pode-se encontrar a
distribuição marginal \(p\left(\lambda\right)\). Dessa forma, segue-se
que:

\[
\begin{aligned}
p\left(\lambda\right) &= \int_{-\infty}^{\infty} p\left(\lambda, y\right) dy \\
&= \int_{-\infty}^{\infty} p\left(\lambda | y\right) p\left(y\right) dy \\
&= \int_{-\infty}^{y^*} p\left(\lambda | y\right) p\left(y\right) dy + \int_{y^*}^{\infty} p\left(\lambda | y\right) p\left(y\right) dy \\
&= \int_{-\infty}^{y^*} l\left(\lambda\right) p\left(y\right) dy + \int_{y^*}^{\infty} g\left(\lambda\right) p\left(y\right) dy \\
&= \gamma l\left(\lambda\right) + \left(1 - \gamma\right) g\left(\lambda\right)\text{.}
\end{aligned}
\]

Agora, basta calcular a integral
\(\int_{-\infty}^{y^*}\left(y^* - y\right) p\left(\lambda|y\right)p\left(y\right)dy\).
Tem-se, portanto:

\[
\begin{aligned}
\int_{-\infty}^{y^*}\left(y^* - y\right) p\left(\lambda|y\right)p\left(y\right)dy &= l\left(\lambda\right) \int_{-\infty}^{y^*} \left(y^* - y\right) p\left(y\right)dy \\
&= \gamma y^{*} l\left(\lambda\right) - l\left(\lambda\right) \int_{-\infty}^{y^*} y p\left(y\right)dy\text{.}
\end{aligned}
\]

Finalmente, substituindo essa última expressão encontrada e
\(p\left(\lambda\right)\) na Equação~\ref{eq-exp}, chega-se a:

\[
EI_{y^*}\left(\lambda\right) = \frac{\gamma y^* l\left(\lambda\right) - l\left(\lambda\right) \int_{-\infty}^{y^*} yp\left(y\right)dy}{\gamma l\left(\lambda\right) + \left(1 + \gamma\right)g\left(\lambda\right)} \propto \left[\gamma + \frac{g\left(\lambda\right)}{l\left(\lambda\right)} \left(1 - \gamma\right)\right]^{-1}\text{.}
\]

Essa última expressão mostra que, para maximizar o Expected Improvement,
é necessário encontrar valores de \(\lambda\) que apresentem alta
probabilidade em \(l\left(\lambda\right)\) e baixa probabilidade em
\(g\left(\lambda\right)\). Portanto, no TPE, maximizar o EI equivale a
maximizar a razão \(l\left(\lambda\right) / g\left(\lambda\right)\).

\subsection{Otimização de hiperparâmetros com
optuna}\label{otimizauxe7uxe3o-de-hiperparuxe2metros-com-optuna}

~~~Para otimizar os hiperparâmetros dos modelos foi utilizado a
bilbioteca \href{https://optuna.org/}{\textbf{Optuna}} da linguagem de
programação \textbf{Python}. Essa biblioteca implementa diversos métodos
para otimização automatizada de hiperparâmetros. Para a sua utilização,
é preciso definir inicialmente uma função objetivo. Por exemplo, para
otimizar os hiperparâmetros de uma random forest, é necessário definir
uma função objetivo da seguinte forma:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ optuna}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ ensemble}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ cross\_val\_score, KFold}

\KeywordTok{def}\NormalTok{ objective(trial):}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ train\_df[variaveis\_independentes]}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ train\_df.variavel\_dependente}

\NormalTok{    params }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{        n\_estimators}\OperatorTok{=}\NormalTok{trial.suggest\_int(}
\NormalTok{          name}\OperatorTok{=}\StringTok{\textquotesingle{}n\_estimators\textquotesingle{}}\NormalTok{,}
\NormalTok{          low}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{          high}\OperatorTok{=}\DecValTok{1000}\NormalTok{),}
\NormalTok{        max\_depth}\OperatorTok{=}\NormalTok{trial.suggest\_int(}
\NormalTok{          name}\OperatorTok{=}\StringTok{\textquotesingle{}max\_depth\textquotesingle{}}\NormalTok{,}
\NormalTok{          low}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{          high}\OperatorTok{=}\DecValTok{1000}\NormalTok{),}
\NormalTok{        max\_features}\OperatorTok{=}\StringTok{\textquotesingle{}sqrt\textquotesingle{}}\NormalTok{,}
\NormalTok{        random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{    )}

\NormalTok{    model }\OperatorTok{=}\NormalTok{ ensemble.RandomForestRegressor(}
        \OperatorTok{*}\NormalTok{params}
\NormalTok{    )}
\NormalTok{    model.fit(X}\OperatorTok{=}\NormalTok{X, y}\OperatorTok{=}\NormalTok{y)}

\NormalTok{    cv\_scores }\OperatorTok{=}\NormalTok{ np.expm1(np.sqrt(}\OperatorTok{{-}}\NormalTok{cross\_val\_score(}
\NormalTok{        estimator}\OperatorTok{=}\NormalTok{model,}
\NormalTok{        X}\OperatorTok{=}\NormalTok{X,}
\NormalTok{        y}\OperatorTok{=}\NormalTok{y,}
\NormalTok{        scoring}\OperatorTok{=}\StringTok{"neg\_mean\_squared\_error"}\NormalTok{,}
\NormalTok{        n\_jobs}\OperatorTok{=}\DecValTok{3}\NormalTok{,}
\NormalTok{        cv}\OperatorTok{=}\NormalTok{KFold(n\_splits}\OperatorTok{=}\DecValTok{20}\NormalTok{))))}

    \ControlFlowTok{return}\NormalTok{ np.mean(cv\_scores)}

\NormalTok{study }\OperatorTok{=}\NormalTok{ optuna.create\_study()}
\NormalTok{study.optimize(objective, n\_trials}\OperatorTok{=}\DecValTok{100}\NormalTok{, n\_jobs}\OperatorTok{={-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Primeiro, define-se, em cada tentativa (trial), quais hiperparâmetros
serão otimizados, especificados no objeto \texttt{params} no início da
função. Após definir o espaço de busca para cada hiperparâmetro, o
modelo escolhido é ajustado aos dados de treinamento. Com o modelo
ajustado, realiza-se a validação cruzada em cada trial, utilizando o
método K-Fold com 20 divisões (folds), conforme definido previamente.
Após definir a função objetivo, inicializa-se um estudo com
\texttt{optuna.create\_study} e, em seguida, inicia-se a otimização com
\texttt{study.optimize(objective,\ n\_trials=100,\ n\_jobs=-1)}. Por
fim, para selecionar os melhores hiperparâmetros ao fim do último trial,
basta executar \texttt{study.best\_params}.

\vspace{12pt}

Por padrão, a biblioteca \href{https://optuna.org/}{\textbf{Optuna}}
utiliza o Tree-Structured Parzen Estimator (TPE) para otimizar
hiperparâmetros de um modelo. A técnica de otimização é escolhida por
meio do argumento \texttt{sampler} no método \texttt{create\_study}.
Para selecionar o TPE, basta passar \texttt{optuna.samplers.TPESampler}
como argumento para a criação do estudo. O método TPE é o padrão para
otimização na biblioteca \href{https://optuna.org/}{\textbf{Optuna}}. No
entanto, caso se deseje utilizar outro método de otimização da
biblioteca, basta especificá-lo da mesma forma:
\texttt{optuna.create\_study(sampler=metodo\_otimizacao)}.

\vspace{12pt}

A otimização de hiperparâmetros realizada com o
\href{https://optuna.org/}{\textbf{Optuna}} oferece diversas
possibilidades para analisar o histórico de comportamento do algoritmo e
compreender a importância de cada hiperparâmetro utilizado no processo
de otimização. Neste trabalho, foi empregado o método fANOVA (Funcional
Analysis of Variance) (HUTTER; HOOS; LEYTON-BROWN, 2014) para a análise
da importância dos hiperparâmetros.

\vspace{12pt}

O método fANOVA utiliza uma Random Forest para estimar a função objetivo
com base nos hiperparâmetros avaliados. Ele realiza uma decomposição da
variância explicada pela função objetivo, atribuindo contribuições
individuais a cada hiperparâmetro. Isso permite identificar quais
hiperparâmetros têm maior impacto no desempenho do modelo.

\vspace{12pt}

Matematicamente, o fANOVA decompõe uma função
\(\hat y: \Theta_{1} \times \dots \times \Theta_{n} \rightarrow \mathbb{R}\)
em componentes aditivos que dependem apenas de subconjuntos de suas
instâncias \(N\). Assim, define-se:

\[
\hat{y} = \sum_{U \subseteq N} \hat{f}_{U} \left(\boldsymbol{\theta}_{U}\right)\text{,}
\] em que
\(\boldsymbol{\theta}_U = \left<\theta_{u_{1}}, \dots, \theta_{u_{m}}\right> \text{, com } \theta_{u_i} \in \Theta_{u_i}\),
é o vetor correspondente a uma configuração parcial dos hiperparâmetros
para o subconjunto \(U = \{u_{1}, \dots, u_{m}\}\), que representa um
subconjunto dos índices dos hiperparâmetros. Além disso, as componentes
de \(\hat{f}_{U} \left(\boldsymbol{\theta}_{U}\right)\) são definidas da
seguinte forma:

\[
\hat{f}_{U} \left(\boldsymbol{\theta}_{U}\right)=
\begin{cases}
  \frac{1}{\|\Theta\|} \int \hat{y} \left(\boldsymbol{\theta}\right) d\boldsymbol{\theta}, & \text{se } U= \emptyset \\
  \hat{a}_{U}\left(\boldsymbol{\theta}_U\right) - \sum_{W \subseteq U} \hat{f}_{W}\left(\boldsymbol{\theta_{W}}\right), & \text{caso contrário}
\end{cases}\text{,}
\] em que
\(\hat{a}_{U}\left(\boldsymbol{\theta}_U\right) = \frac{1}{\|\boldsymbol{\Theta}_{T}\|} \int \hat{y}\left(\boldsymbol{\theta}_{N|U}\right)d \boldsymbol{\theta}_{T}\)
é uma estimativa da performance marginal
\(a_U\left(\boldsymbol{\theta}_U\right)\) de um algoritmo \(A\).

\vspace{12pt}

Quando \(U = \emptyset\), tem-se a média da função
\(\hat{f}_{\emptyset}\) em seu domínio. As funções unárias
\(\hat{f}_{\{j\}}\left(\boldsymbol{\theta}_{\{j\}}\right)\) são chamadas
de efeitos principais e capturam o efeito da variação do hiperparâmetro
\(j\), fazendo a média sobre todas as configurações dos outros
hiperparâmetros. Quando \(|U| > 1\), as funções
\(\hat{f}_{U}\left(\boldsymbol{\theta}_U\right)\) capturam exatamente os
efeitos de interação entre todos os hiperparâmetros em \(U\) (excluindo
todos os efeitos principais de ordem inferior e efeitos de interação de
\(W \subsetneq U\)).

\vspace{12pt}

A variância de \(\hat{y}\) em seu domínio \(\boldsymbol{\Theta}\) é
definido por:

\[
\mathbb{V} = \frac{1}{\|\boldsymbol{\Theta}\|} \int\left(\hat{y}\left(\boldsymbol{\theta}\right) - \hat{f}_{\emptyset}\right)^2 d \boldsymbol{\theta}\text{,}
\] e a fANOVA decompõe essa variância em contribuições de todos os
conjuntos de hiperparâmetros:

\[
\mathbb{V} = \sum_{U \subset N}\mathbb{V}_{U} \text{, em que } \mathbb{V}_U = \frac{1}{\|\boldsymbol{\Theta}_{U}\|} \int \hat{f}_U\left(\boldsymbol{\theta}_U\right)^2 d\boldsymbol{\theta}_U\text{.}
\]

\vspace{12pt}

Por fim, a importância de cada efeito principal e efeito de interação
\(\hat{f}_U\) pode então ser quantificado pela fração da variância
explicada:

\[
\mathbb{F}_{U} = \mathbb{V}_{U} / \mathbb{V}\text{.}
\]

\vspace{12pt}

A classe \texttt{FanovaImportanceEvaluator}, do
\href{https://optuna.org/}{\textbf{Optuna}}, disponibiliza esse método.
Além disso, a função
\texttt{optuna.visualization.plot\_param\_importances}, que utiliza o
método fANOVA por padrão, facilita a criação de gráficos que destacam a
importância dos hiperparâmetros.

\vspace{12pt}

Além de analisar a importância dos hiperparâmetros, o
\href{https://optuna.org/}{\textbf{Optuna}} também oferece ferramentas
para explorar o histórico de otimização, as relações entre os
hiperparâmetros e o comportamento da função objetivo em cada trial.
Essas análises podem ser realizadas, respectivamente, pelas funções
\texttt{plot\_optimization\_history}, \texttt{plot\_contour} e
\texttt{plot\_slice}.

\section{Interpretação dos algoritmos de aprendizagem de
máquina}\label{interpretauxe7uxe3o-dos-algoritmos-de-aprendizagem-de-muxe1quina}

~~~Na aplicação de aprendizado de máquina, o foco geralmente está em
obter um modelo com o menor erro de generalização possível, o que muitas
vezes resulta na negligência da interpretação dos resultados e do que
mais influenciou a variável dependente. Isso pode comprometer a
compreensão do que o algoritmo está efetivamente fazendo. Em resposta a
essa limitação, diversas técnicas têm sido desenvolvidas para
interpretar os efeitos das variáveis independentes nas estimativas
geradas pelos algoritmos. Assim, esta seção será dedicada a descrever a
fundamentação teórica e a aplicação das técnicas de interpretação
utilizadas neste trabalho.

\subsection{Individual Conditional Expectation
(ICE)}\label{individual-conditional-expectation-ice}

~~~O método Individual Conditional Expectation (ICE) é uma ferramenta
gráfica que permite visualizar as estimativas de um modelo de forma
detalhada. Esse método traça a relação entre os valores preditos pelo
modelo e uma variável específica, analisando cada observação
individualmente. Dessa forma, o ICE possibilita a análise da variação
dos valores ajustados ao longo do domínio de uma covariável, destacando
tanto a heterogeneidade entre as observações quanto a forma como cada
uma responde individualmente às mudanças na variável em questão. Essa
abordagem facilita a identificação de padrões e variações que poderiam
ser obscurecidos em análises agregadas (GOLDSTEIN et al., 2015).

\vspace{12pt}

Formalmente, o método ICE considera as observações
\(\{x_{S_i}, \mathbf{x}_{C_i}\}_{i=1}^{N}\)\hspace{0pt} e os valores
preditos \(\hat f\)\hspace{0pt}. Para cada uma das \(N\) observações, é
traçada uma curva \(\hat f_S^{\left(i\right)}\)\hspace{0pt} em função
dos valores da variável de interesse \(x_S\)\hspace{0pt}, enquanto as
demais variáveis \(x_C\)\hspace{0pt} permanecem fixas em seus valores
observados. Nesse gráfico, \(x_S\)\hspace{0pt} é representado no eixo
das abscissas, e as predições correspondentes pelo modelo aparecem no
eixo das ordenadas.

\vspace{12pt}

Quando há um grande número de curvas no gráfico ICE, a interpretação
pode se tornar desafiadora devido à sobreposição ou excesso de linhas.
Para simplificar a análise, pode-se aplicar uma técnica de
centralização, conhecida como c-ICE (centered ICE). Essa abordagem
centraliza as curvas em um ponto específico de \(x_S\)\hspace{0pt},
mostrando apenas a diferença nas predições em relação a esse ponto.
Assim, as novas curvas c-ICE são definidas da seguinte forma:

\[
\hat{f}_{cent}^{\left(i\right)} = \hat{f}^{\left(i\right)} - \mathbf{1} \hat{f}\left(x^*, \mathbf x_{C_i}\right)\text{,}
\] em que \(x^*\) é selecionado como o mínimo ou o máximo de
\(x_S\)\hspace{0pt}, \(\hat f\)\hspace{0pt} é o modelo ajustado, e
\(\mathbf 1\) é um vetor de uns.

\vspace{12pt}

Quando \(x^∗\) é escolhido como o valor mínimo de \(x_S\), todas as
curvas c-ICE iniciam em zero, eliminando as diferenças de nível causadas
pelas variações nos valores de \(x_C^{\left(i\right)}\)\hspace{0pt}
entre as observações. Por outro lado, se \(x^∗\) for o valor máximo de
\(x_S\), as curvas centralizadas mostram o efeito cumulativo de
\(x_S\)\hspace{0pt} sobre \(\hat f\)\hspace{0pt} em relação ao ponto de
centralização. Essa abordagem facilita a interpretação do impacto de
\(x_S\).

\vspace{12pt}

O método Individual Conditional Expectation (ICE) é similar ao Partial
Dependence Plot (PDP), mas com a diferença de que o PDP apresenta uma
visão agregada (global) dos efeitos das variáveis independentes nas
predições e, por outro lado, o ICE fornece uma análise mais detalhada,
mostrando uma linha para cada instância do conjunto de dados. O PDP é
obtido como a média das linhas geradas pelo método ICE, oferecendo uma
visão mais generalizada do impacto de uma variável. Matematicamente, o
PDP é definido como:

\[
\hat{f}_{S}\left(x_{S}\right) = E_{\mathbf{X}_{C}}\left[\hat{f}\left(x_{S}, \mathbf{X}_{C}\right)\right]\text{,}
\] em que \(\mathbf{X}_C\) representa o conjunto das variáveis
independentes que são mantidas fixas durante a análise e \(x_S\) é a
variável independente de interesse, cujo efeito sobre a predição se
deseja analisar.

\vspace{12pt}

Para calcular \(\hat{f}_S\left(x_S\right)\) na prática, pode-se utilizar
simulação de Monte Carlo. O PDP pode, então, ser estimado da seguinte
forma:

\[
\hat{f}_S\left(x_S\right) = \frac{1}{N} \sum^{N}_{i=1}\hat{f}\left(x_{S}, \mathbf{X}_{C_i}\right)\text{,}
\] em que \(N\) é o número de amostras utilizadas na simulação e
\(\hat{f}\left(x_{S}, \mathbf{X}_{C_i}\right)\) são as predições do
modelo.

\vspace{12pt}

Em \textbf{Python}, a biblioteca
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}}
disponibiliza a classe \texttt{PartialDependenceDisplay} para a criação
de gráficos de ICE e a inclusão opcional da linha de PDP. Para gerar o
gráfico, é necessário ter um modelo previamente ajustado. O código a
seguir demonstra sua utilização:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.inspection }\ImportTok{import}\NormalTok{ PartialDependenceDisplay}

\NormalTok{PartialDependenceDisplay}\OperatorTok{\textbackslash{}}
\NormalTok{    .from\_estimator(}
\NormalTok{        model,}
\NormalTok{        df,}
\NormalTok{        features,}
\NormalTok{        kind}\OperatorTok{=}\StringTok{"both"}\NormalTok{,}
\NormalTok{        centered}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{        random\_state}\OperatorTok{=}\NormalTok{set\_seed}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

No exemplo acima, o método \texttt{.from\_estimator} é usado para criar
o gráfico diretamente a partir de um modelo ajustado. Ele recebe como
argumentos o modelo (\texttt{model}), a base de dados (\texttt{df}) e as
variáveis de interesse (\texttt{features}). O argumento \texttt{kind}
permite definir o tipo de visualização, podendo exibir apenas o PDP, as
linhas do ICE ou ambos (\texttt{kind="both"}). Já o argumento
\texttt{centered} oferece a opção de centralizar as curvas.

\subsection{Local interpretable model-agnostic explanations
(LIME)}\label{local-interpretable-model-agnostic-explanations-lime}

~~~RIBEIRO, M. T.; SINGH; GUESTRIN (2016) definem o Local Interpretable
Model-Agnostic Explanations (LIME) como um algoritmo capaz de explicar
as previsões realizadas por qualquer modelo de classificação ou
regressão, aproximando-o localmente de um modelo mais interpretável. O
objetivo principal do LIME é identificar um modelo interpretável e gerar
uma representação compreensível que permita traduzir o comportamento de
modelos complexos.

\vspace{12pt}

Formalmente, para a construção da explicação gerada pelo LIME, a
explicação é definida como um modelo \(g \in G\), onde \(G\) representa
uma classe de potenciais modelos interpretáveis, como regressão linear
ou árvores de decisão. O domínio de \(g\) é dado por \({0, 1}^{d^{'}}\),
ou seja, o modelo explicativo \(g\) opera sobre a presença ou ausência
de componentes interpretáveis. Como nem todos os modelos \(g \in G\)
podem ser simples o suficiente para serem interpretados, define-se uma
medida de complexidade \(\Omega\left(g\right)\) para a explicação gerada
por \(g \in G\). Por exemplo, para uma árvore de regressão,
\(\Omega\left(g\right)\) pode ser a profundidade da árvore, enquanto
que, para modelos lineares, \(\Omega\left(g\right)\) pode ser o número
de pesos diferentes de zero.

\vspace{12pt}

Seja \(f: \mathbb{R}^d \rightarrow \mathbb{R}\) o modelo que está sendo
explicado. Define-se, então, uma medida de aproximação
\(\pi_x\left(z\right)\) entre uma instância \(z\) e \(x\), com o
objetivo de estabelecer a localidade ao redor de \(x\). Por fim, seja
\(L\left(f, g, \pi_x\right)\) uma estatística que quantifica o erro de
\(g\) ao tentar aproximar \(f\) na localidade definida por \(\pi_x\).
Para garantir a interpretabilidade e a fidelidade local, é necessário
minimizar \(L(f, g, \pi_x)\), mantendo \(\Omega(g)\) suficientemente
baixo. Dessa forma, obtém-se a explicação gerada pelo LIME:

\[
\xi\left(x\right) = \arg \min_{g \in G} L\left(f, g, \pi_x\right) + \Omega\left(g\right)\text{.}
\]

Podem exister diversas variações para \(L\) e \(\Omega\) com diferentes
famílias de modelos explicativos \(G\). Uma escolha para \(L\), por
exemplo, é o erro quadrático médio.

\vspace{12pt}

Embora o LIME não tenha sido utilizado neste trabalho, ele é um dos
métodos fundamentais para a abordagem que será apresentada a seguir, o
Shapley Additive Explanations (SHAP). Em \textbf{Python}, o LIME está
disponível por meio da biblioteca \texttt{lime}, cuja documentação pode
ser acessada em
\url{https://lime-ml.readthedocs.io/en/latest/lime.html}.

\subsection{Shapley Additive Explanations
(SHAP)}\label{shapley-additive-explanations-shap}

~~~O Shapley Additive Explanations (SHAP) é um método cujo objetivo é
explicar as predições individuais de uma instância \(x\) por meio da
computação da contribuição de cada variável para o resultado da
predição. Esse método é fundamentado nos valores de Shapley, que serão
definidos a seguir.

\vspace{12pt}

Os valores de Shapley foram introduzidos por SHAPLEY (1953) e baseiam-se
nos conceitos da teoria dos jogos de coalizão. Essa teoria foi adaptada
para explicar predições realizadas por modelos, representando a
contribuição média de uma variável para a predição, considerando todas
as possíveis coalizões. Nesse contexto, coalizões referem-se a
diferentes combinações de variáveis. Matematicamente, os valores de
Shapley são definidos como:

\begin{equation}\phantomsection\label{eq-shapley}{
\phi_i\left(x\right) = \sum_{Q \subseteq S | \{i\}} \frac{|Q|!\left(|S| - |Q| - 1\right)!}{|S|!} \left(\Delta_{Q\cup \{i\}}\left(x\right) - \Delta_{Q}\left(x\right)\right)\text{,}
}\end{equation} em que \(Q\) é um subconjunto das covariáveis
consideradas no modelo e \(S\) representa o conjunto completo de todas
as covariáveis. A diferença
\(\Delta_{Q \cup \{i\}}\left(x\right) - \Delta_{Q}\left(x\right)\)
corresponde à contribuição marginal da variável \(i\) ao ser adicionada
ao subconjunto \(Q\).

\vspace{12pt}

No entanto, a Equação~\ref{eq-shapley} apresenta crescimento exponencial
em termos de complexidade computacional à medida que o número de
variáveis aumenta. Para contornar esse problema e reduzir o custo
computacional, os valores de Shapley podem ser estimados de forma
aproximada e eficiente utilizando o método de Monte Carlo, conforme a
seguinte expressão:

\[
\hat{\phi}_{i}\left(x\right) = \frac{1}{n!} \sum_{O \in \pi \left(n \right)} \left( \Delta_{{Pre}^{i} \left(O\right) \cup \{i\}} - \Delta_{{Pre}^{i} \left(O\right)} \right), \ i = 1, \dots, n\text{,}
\] em que \(\pi\left(n\right)\) é o conjunto de todas as permutações
ordenadas dos índices das variáveis \(\{1, 2, \dots, n\}\) e
\({Pre}^{i}\left(O\right)\) representa o conjunto de índices das
variáveis que precedem \(i\) na permutação \(O \in \pi \left(n\right)\)
(ŠTRUMBELJ; KONONENKO, 2014).

\vspace{12pt}

O SHAP estabelece uma conexão entre os valores de Shapley e o método
LIME, previamente definido. O modelo explicativo do SHAP, denotado por
\(g\), é uma função linear de variáveis binárias, construído a partir
dos valores de Shapley, conforme define a expressão a seguir:

\[
g\left(z^{'}\right) = \phi_0 + \sum_{j = 1}^{M} \phi_{j} z_{j}^{'}\text{,}
\] em que \(g\) é o modelo explicativo, \(z^{'} \in \{0, 1\}^{M}\) é o
vetor de coalizão, representando a presença \(\left(z^{'}_j = 1\right)\)
ou ausência \(\left(z^{'}_j = 0\right)\) de cada covariável e \(\phi_j\)
denota os valores Shapley.

\vspace{12pt}

A partir do método SHAP, é possível ter diversas visualizações que
ajudam a entender como as predições do modelo se comportam. Neste
trabalho foi utilizado o gráfico de importância das variáveis, resumo
dos valores shapley e o de dependência. Os métodos serão descritos a
seguir tendo como referência o livro de MOLNAR (2020).

\vspace{12pt}

O gráfico de importância das variáveis é bastante simples, variáveis com
valores absolutos elevados de Shapley são importantes. No entanto, como
se deseja obter a importância global, calcula-se a média dos valores
absolutos de Shapley por variável em todo o conjunto de dados. Assim,
tem-se a seguinte expressão:

\[
I_j = \frac{1}{n}\sum^{n}_{i = 1} |\phi_j^{\left(i\right)}|\text{.}
\]

O gráfico de resumo combina a importância das variáveis com seus
efeitos, representados pelos valores de Shapley e pela variação das
observações de cada variável. No eixo x estão os valores de Shapley, que
tem sua variação representada por pontos, enquanto no eixo y
encontram-se as variáveis, ordenadas de forma decrescente com base em
sua importância. Os pontos, que representam os valores de Shapley, são
coloridos de acordo com os valores altos ou baixos das observações
originais de cada variável. Essa representação facilita a compreensão de
como as predições do modelo estão sendo influenciadas por cada variável,
permitindo uma análise mais detalhada de seus efeitos.

\vspace{12pt}

Por fim, o gráfico de dependência é o mais simples de todos. Nele, os
valores de Shapley de uma variável são plotados em função de suas
respectivas observações. Matematicamente, é definido como: \[
\{\left(x_j^{\left(i\right)}, \phi_j^{\left(i\right)} \right)\}^{n}_{i=1}\text{.}
\]

Esse gráfico permite visualizar diretamente como as observações de uma
variável estão relacionadas aos seus efeitos no modelo, representados
pelos valores de Shapley. Ele foi utilizado somente para analisar como
as variáveis binárias se comportam.

\vspace{12pt}

Para aplicar o método SHAP em \textbf{Python}, foi utilizada a
biblioteca \href{https://shap.readthedocs.io/en/latest/}{\textbf{SHAP}}.
Essa biblioteca calcula os valores de Shapley com base no modelo
ajustado e no algoritmo de explicação escolhido, implementado na classe
\texttt{shap.Explainer}. Esse algoritmo estima os valores de Shapley de
maneira eficiente e adaptada ao modelo em análise. Após obter os valores
de Shapley, é possível criar gráficos como os de resumo, dependência e
importância. O gráfico de dependência pode ser gerado utilizando a
função \texttt{shap.dependence\_plot}, enquanto os gráficos de
importância e resumo são criados com a função
\texttt{shap.summary\_plot}. Abaixo, é apresentado um exemplo de código
que utiliza a biblioteca
\href{https://shap.readthedocs.io/en/latest/}{\textbf{SHAP}} para gerar
esses gráficos baseado no algoritmo Stacking:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ shap}

\NormalTok{X1000 }\OperatorTok{=}\NormalTok{ shap.utils.sample(train\_df, }\DecValTok{1000}\NormalTok{)}
\NormalTok{explainer\_stacking }\OperatorTok{=}\NormalTok{ shap.Explainer(}
\NormalTok{    model}\OperatorTok{=}\NormalTok{stacking.predict,}
\NormalTok{    mask}\OperatorTok{=}\NormalTok{X1000}
\NormalTok{    )}
\NormalTok{shap\_values\_stacking }\OperatorTok{=}\NormalTok{ explainer\_stacking(test\_df)}

\NormalTok{shap.summary\_plot(}
\NormalTok{    shap\_values\_stacking,}
\NormalTok{    test\_df,}
\NormalTok{    )}

\NormalTok{shap.summary\_plot(}
\NormalTok{    shap\_values\_stacking,}
\NormalTok{    test\_df,}
\NormalTok{    plot\_type}\OperatorTok{=}\StringTok{"bar"}\NormalTok{,}
\NormalTok{    )}

\NormalTok{shap.dependence\_plot(}
\NormalTok{    variavel,}
\NormalTok{    shap\_values\_stacking.values,}
\NormalTok{    test\_df.values,}
\NormalTok{    interaction\_index}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

Por padrão, a classe \texttt{shap.Explainer} utiliza o algoritmo de
explicação considerado a melhor escolha com base no modelo passado como
argumento. Neste trabalho, foi selecionado o algoritmo de explicação
utilizando a classe \texttt{Permutation}. Essa escolha foi realizada
automaticamente pela classe \texttt{shap.Explainer(algorithm="auto")}. O
algoritmo Permutation funciona iterando sobre todas as permutações
possíveis das variáveis, tanto na ordem original quanto na ordem
inversa. Em relação ao gráfico de importância, ele é gerado de maneira
semelhante ao gráfico de resumo, com a diferença de que o argumento
\texttt{plot\_type="bar"} é utilizado para criar o gráfico de
importância. Por fim, o método \texttt{shap.utils.sample} é utilizado
para selecionar aleatoriamente observações para a construção de uma
amostra, a fim de estimar os valores de Shapley.

\chapter{Resultados}\label{resultados}

\section{Análise exploratória de
dados}\label{anuxe1lise-exploratuxf3ria-de-dados-1}

~~~A análise exploratória dos dados foi realizada após a divisão entre
os conjuntos de treinamento e teste. Essa abordagem foi adotada para
evitar o sobreajuste do modelo e garantir que o algoritmo não aprenda
com informações indisponíveis no conjunto de teste. Assim, a descritiva
dos dados foi realizada utilizando o conjunto de treinamento.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-miss-output-1.pdf}}

}

\caption{\label{fig-miss}Proporção de valores ausentes por variáveis.}

\end{figure}%

A primeira etapa da análise exploratória de dados foi identificar os
dados faltantes e determinar a melhor forma de tratá-los. A
Figura~\ref{fig-miss} mostra a porcentagem de observações ausentes em
cada variável. As variáveis com a maior quantidade de dados ausentes são
o valor do condomínio e o IPTU, pois essas informações são as menos
preenchidas no site de onde os dados foram coletados. A terceira
variável, com quase 20\% de observações ausentes, é a quantidade de
vagas de estacionamento. As variáveis com mais de 20\% de observações
ausentes foram removidas da base de dados, pois, com essa quantidade de
valores faltantes, nem mesmo métodos de imputação proporcionariam um
tratamento adequado. Dessa forma, apenas as variáveis de valor do
condomínio e IPTU foram removidas, enquanto as demais com valores
ausentes foram tratadas por meio de imputação.

\vspace{12pt}

Uma das dificuldades que podem surgir durante a modelagem é o
desbalanceamento das classes, ou seja, a diferença na quantidade de cada
tipo de imóvel. O tipo de imóvel mais predominante no conjunto de dados
são os apartamentos, que representam 81,36\% do total. Em seguida, vêm
as casas, com 8,91\%, e os flats, com 5,72\%. Por fim, as casas
comerciais são as menos representadas, com apenas 15 ocorrências. Esse
desbalanceamento claro entre as classes pode dificultar o desempenho do
modelo, especialmente na previsão de categorias menos frequentes, como
as casas comerciais, onde o modelo pode ter dificuldade em obter bons
resultados.

\begin{figure}

\centering{

\includegraphics[width=4.6875in,height=2.60417in]{TCC_files/mediabag/includes/map_valor_m2.pdf}

}

\caption{\label{fig-mapa_valor}Variação da média do valor do \(m^2\) dos
imóveis de João Pessoa, área de estudo.}

\end{figure}%

A partir da Figura~\ref{fig-mapa_valor}, tem-se o mapa da região de
estudo, correspondente à cidade de João Pessoa. O mapa apresenta a
variação da média do valor do \(m^2\) dos imóveis, calculada com base
nos bairros da cidade. Vale destacar que alguns bairros não possuíam
dados disponíveis no momento da coleta de informações por raspagem dos
sites de imóveis. Esses bairros estão representados pela cor cinza. Por
outro lado, os bairros com dados disponíveis apresentam variações de
cores que indicam diferentes faixas de valores do \(m^2\). Tonalidades
mais escuras representam bairros com valores médios mais altos para o
\(m^2\), enquanto tonalidades mais claras indicam valores médios
menores.

\vspace{12pt}

O bairro com o maior preço médio do \(m^2\) é Cabo Branco, com um valor
pouco superior a R\$ 10.000,00. Em segundo lugar, está o bairro de
Tambaú, com um valor médio de R\$ 8.951,45 por \(m^2\). Em terceiro,
encontra-se o bairro Jardim Oceania, com um valor médio de R\$ 7.879,60
por \(m^2\), seguido pelo bairro Altiplano Cabo Branco, com um valor
médio de R\$ 7.218,70 por \(m^2\).

\vspace{12pt}

A análise de alguns bairros se torna limitada devido à baixa quantidade
de imóveis disponíveis no momento da raspagem de dados. Por exemplo, o
bairro de Barra de Gramame, que apresenta o menor valor médio de
\(m^2\), tinha apenas um imóvel à venda no momento da coleta de dados.
Da mesma forma, o bairro Jardim Veneza, o segundo com o menor valor
médio por \(m^2\), também possuía poucos registros. Isso indica que os
valores apresentados para esses bairros podem não refletir com precisão
o mercado imobiliário local e podem ser superiores em outras
circunstâncias.

\vspace{12pt}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-violin-output-1.pdf}}

}

\caption{\label{fig-violin}Distribuição das variáveis numéricas.}

\end{figure}%

A distribuição das variáveis foram análisadas em termos do tipo do
imóvel a partir de um gráfico de violino. Pela Figura~\ref{fig-violin},
é fácil perceber que a maioria das distribuições possuem assimetria
negativa. Os apartamentos possuem caudas longas à direita, indicando
presença de valores extremamente altos e que indicam que talvez seja
necessário a aplicação de alguma transformação para a estabilização da
variância.

Para reduzir a assimetria da distribuição dos valores dos imóveis, foi
aplicada uma transformação logarítmica. O gráfico de densidade à
esquerda na Figura~\ref{fig-densitarg} mostra os dados originais da
distribuição do valor dos imóveis. Há uma tendência dos valores ficarem
mais concentrados em uma faixa mais baixa, mas alguns imóveis apresentam
valores excepcionalmente altos, o que acaba gerando uma distribuição
assimétrica positiva. Com a aplicação da transformação logarítmica, a
assimetria é suavizada, comprimindo os valores mais altos. Isso tende a
aproximar a distribuição de uma forma mais simétrica, facilitando a
modelagem e análise estatística.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-densitarg-output-1.pdf}}

}

\caption{\label{fig-densitarg}Comparação entre distribuição dos valores
dos imóveis antes e depois da transformação logarítmica.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-corplot-output-1.pdf}}

}

\caption{\label{fig-corplot}Gráfico de correlação de Spearman das
variáveis independentes.}

\end{figure}%

\vspace{12pt}

A Figura~\ref{fig-corplot} apresenta a matriz de correlação entre as
variáveis numéricas do conjunto de dados. As cores mais escuras indicam
uma correlação mais forte entre as variáveis, enquanto as cores mais
claras indicam o contrário. O valor do imóvel apresenta maior correlação
com as variáveis de área do imóvel e número de vagas de estacionamento.
Além disso, o valor do imóvel tem alta correlação com o valor médio do
aluguel, número de quartos e banheiros, além de ser fortemente
influenciado pela localização dos imóveis.

\vspace{12pt}

A partir do gráfico de correlação, foram criadas novas variáveis com
base nas interações entre as mais correlacionadas. Dessa forma, foram
adicionadas três novas variáveis. A primeira é o produto entre o número
de quartos do imóvel e a área média do aluguel no bairro; a segunda
representa a razão entre o número de quartos e a área do imóvel; e a
terceira resulta do produto entre as coordenadas geográficas e o preço
médio do aluguel no bairro. Além disso, foi considerada uma variável que
representa a quantidade total de cômodos do imóvel. Para isso,
somaram-se o número de quartos e banheiros, adicionando-se dois cômodos
a mais para imóveis do tipo apartamento, casa, casa de condomínio e
flat, assumindo a existência de sala e cozinha.

\section{Tunagem dos modelos}\label{tunagem-dos-modelos}

~~~Como os algoritmos utilizados neste trabalho são baseados em árvores
de decisão ou podem utilizar algoritmos baseados em árvore, como Random
Forest, Gradient Boosting e suas variações, os parâmetros escolhidos
para otimização serão bastantes parecidos. Assim, foram ajustados
hiperparâmetros como o número de árvores e a profundidade das árvores,
de forma a capturar a complexidade das relações presentes nos dados.
Além disso, para os algoritmos baseados em Gradient Boosting, o
hiperparâmetro de taxa de aprendizado também foi ajustado.

\vspace{12pt}

Todos os modelos e bibliotecas usados neste trabalho seguem a API do
\href{https://scikit-learn.org/stable/}{\textbf{scikit-learn}}. Nessa
API, o hiperparâmetro que define a quantidade de árvores é denominado
\texttt{n\_estimators}, o de profundidade das árvores é
\texttt{max\_depth}, e o de taxa de aprendizado é
\texttt{learning\_rate}. Com a configuração da função objetivo na
biblioteca \href{https://optuna.org/}{\textbf{Optuna}}, foi possível
encontrar o ponto ótimo desses hiperparâmetros para cada modelo. As
métricas obtidas para cada algoritmo pode ser visualizado na
Tabela~\ref{tbl-metrics_models}.

\begin{longtable}[]{@{}cccc@{}}
\caption{Métricas obtidas de cada
algoritmo.}\label{tbl-metrics_models}\tabularnewline
\toprule\noalign{}
Algoritmo & RMSE & \(R^2\) & MAPE \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Algoritmo & RMSE & \(R^2\) & MAPE \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Random Forest & 0,28972 & \(86,78792\%\) & 0,01373 \\
Gradient Boosting & 0,28730 & \(86,98259\%\) & 0,01377 \\
Light Gradient Boosting & 0,28385 & \(87,25794\%\) & 0,01340 \\
Extreme Gradient Boosting & 0,28400 & \(87,24599\%\) & 0,01325 \\
Stacking & 0,28473 & \(87,18793\%\) & 0,01357 \\
\end{longtable}

\subsection{Tunagem da Random Forest}\label{tunagem-da-random-forest}

~~~Para o modelo Random Forest, a função objetivo foi definido apenas
para otimizar os hiperparâmetros de quantidade de árvores e de
profundidade da árvore. O espaço de procura para a quantidade de árvores
foi definido entre 1 a 1000 árvores. Já o hiperparâmetro de profundidade
da árvore foi definido entre 20 a 1000. Além disso, foi utilizado
aleatoriamente \(m = \sqrt{p}\) das \(p\) variáveis independentes como
candidatas para a divisão.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-rf_slice-output-1.pdf}}

}

\subcaption{\label{fig-rf_slice}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-rf_import-output-1.pdf}}

}

\subcaption{\label{fig-rf_import}}

\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-rf_history-output-1.pdf}}

}

\subcaption{\label{fig-rf_history}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-rf_contour-output-1.pdf}}

}

\subcaption{\label{fig-rf_contour}}

\end{minipage}%

\caption{\label{fig-rf_param}Resultados da tunagem da Random Forest.}

\end{figure}%

A figura Figura~\ref{fig-rf_slice} mostra a variação da estatística de
erro em função dos valores de cada hiperparâmetro ao longo dos trials. O
gráfico Figura~\ref{fig-rf_import} exibe a importância de cada
hiperparâmetro, calculada pelo método fANOVA. Na figura
Figura~\ref{fig-rf_history}, observa-se a variação do erro para cada
trial, com a linha vermelha representando o menor valor obtido em cada
etapa. Por fim, a figura Figura~\ref{fig-rf_contour} é um gráfico de
área que mostra a interação entre os hiperparâmetros tunados em relação
à estatística de erro.

\vspace{12pt}

A partir de Figura~\ref{fig-rf_slice}, é possível ver que o erro tende a
ser menor para valores menores do hiperparâmetro de profundidade das
árvores (max\_depth). Em contraste, para o hiperparâmetro de número de
árvores (n\_estimators), o erro apresenta uma tendência de redução e
estabilização à medida que o número de árvores aumenta. Esse
comportamento também é observado em Figura~\ref{fig-rf_contour}, onde
valores menores para max\_depth e maiores para n\_estimators resultam em
um modelo com menor erro de generalização.

\vspace{12pt}

Além disso, o gráfico de importância dos hiperparâmetros em
Figura~\ref{fig-rf_import} revela que o hiperparâmetro n\_estimators
possui uma maior contribuição na performance do modelo. A figura
Figura~\ref{fig-rf_history} ilustra o comportamento da métrica de erro
para cada trial, mostrando que o método de otimização alcança um de seus
melhores valores pouco antes do trial 20. Após esse ponto, o otimizador
não consegue encontrar valores de erro significativamente menores.

\begin{longtable}[]{@{}cccc@{}}
\caption{Melhores hiperparâmetros para Random
Forest.}\label{tbl-params_rf}\tabularnewline
\toprule\noalign{}
Tentativa & RMSE & n\_estimators & max\_depth \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Tentativa & RMSE & n\_estimators & max\_depth \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
67 & 1,29682 & 650 & 22 \\
\end{longtable}

\subsection{Tunagem do Gradient
Boosting}\label{tunagem-do-gradient-boosting}

~~~Para o algoritmo de Gradient Boosting, os hiperparâmetros ajustados
foram a taxa de aprendizado, a quantidade de árvores e a profundidade
das árvores. No Optuna, o espaço de busca definido na função objetivo
para a taxa de aprendizado variou de \(1 \cdot 10^{-5}\) a
\(1 \cdot 10^{-1}\); para a profundidade das árvores, de 3 a 500; e para
a quantidade de árvores, de 50 a 1500. Assim como no Random Forest, foi
utilizada \(m = \sqrt{p}\)\hspace{0pt} das \(p\) variáveis independentes
para realizar as divisões nas árvores.

\vspace{12pt}

A análise da importância dos hiperparâmetros, apresentada na
Figura~\ref{fig-gdt_import}, indica que o hiperparâmetro com maior
influência na variação da função objetivo --- e, consequentemente, na
performance do modelo --- é a taxa de aprendizado. Em seguida, a
profundidade das árvores é o segundo mais relevante, enquanto o número
de árvores tem a menor influência.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-gdt_slice-output-1.pdf}}

}

\subcaption{\label{fig-gdt_slice}}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-gdt_import-output-1.pdf}}

}

\subcaption{\label{fig-gdt_import}}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-gdt_history-output-1.pdf}}

}

\subcaption{\label{fig-gdt_history}}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-gdt_contour-output-1.pdf}}

}

\subcaption{\label{fig-gdt_contour}}

\end{minipage}%

\caption{\label{fig-gdt_param}Resultados da tunagem do Gradient
Boosting.}

\end{figure}%

\vspace{12pt}

Diferentemente do modelo de Random Forest, o algoritmo de Gradient
Boosting não apresentou melhorias significativas, como ilustrado na
Figura~\ref{fig-gdt_history}, onde a estatística de erro poucas vezes
ficou abaixo de 1,3. A relação entre os hiperparâmetros é bastante
similar à obtida para o Random Forest. Na Figura~\ref{fig-gdt_contour},
observa-se que o método de otimização TPE tende a selecionar valores
menores para a profundidade das árvores e maiores para o número de
árvores. No entanto, também há uma preferência por uma menor quantidade
de árvores quando a taxa de aprendizado diminui.

\begin{longtable}[]{@{}ccccc@{}}
\caption{Melhores hiperparâmetros para Gradient
Boosting.}\label{tbl-params_gdt}\tabularnewline
\toprule\noalign{}
Tentativa & RMSE & n\_estimators & max\_depth & learning\_rate \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Tentativa & RMSE & n\_estimators & max\_depth & learning\_rate \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
50 & 1,28891 & 1500 & 6 & 0,08730 \\
\end{longtable}

\subsection{Tunagem do LGBM}\label{tunagem-do-lgbm}

~~~Para a otimização do LGBM, foram considerados os mesmos
hiperparâmetros do algoritmo de Gradient Boosting, mas agora também com
a tunagem do número de folhas das árvores. O espaço de busca para a
quantidade de árvores foi definido entre 100 e 2000, para a taxa de
aprendizado no mesmo intervalo usado no Gradient Boosting, para a
profundidade das árvores e número de folhas o intervalo foi definido
entre 100 e 600

\begin{figure}

\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-lgbm_slice-output-1.pdf}}

}

\subcaption{\label{fig-lgbm_slice}}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-lgbm_import-output-1.pdf}}

}

\subcaption{\label{fig-lgbm_import}}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-lgbm_history-output-1.pdf}}

}

\subcaption{\label{fig-lgbm_history}}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-lgbm_contour-output-1.pdf}}

}

\subcaption{\label{fig-lgbm_contour}}

\end{minipage}%

\caption{\label{fig-lgbm_param}Resultados da tunagem do LGBM.}

\end{figure}%

\vspace{12pt}

As trials de otimização do modelo LGBM (Figura~\ref{fig-lgbm_history})
apresentaram um comportamento semelhante ao do Gradient Boosting, porém
com maior estabilidade em cada tentativa e uma convergência mais rápida.
Ao analisar a contribuição de cada hiperparâmetro para o desempenho do
modelo na Figura~\ref{fig-lgbm_import}, observa-se que a quantidade de
árvores é o hiperparâmetro de maior influência, seguido pela taxa de
aprendizado, número de folhas e, por último, a profundidade da árvore.

\vspace{12pt}

Analisando a relação entre o hiperparâmetro de profundidade e o número
de folhas em função da estatística de erro na
Figura~\ref{fig-lgbm_contour}, observa-se que uma maior quantidade de
folhas e uma menor profundidade das árvores resultam em um modelo com
erro menor. Esse mesmo comportamento é observado para os outros
hiperparâmetros, uma quantidade maior de folhas combinada com uma taxa
de aprendizado crescente também produz um modelo com menor erro, assim
como um número maior de árvores.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1549}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1127}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1972}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1690}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1549}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2113}}@{}}
\caption{Melhores hiperparâmetros para Light Gradient
Boosting.}\label{tbl-params_lgbm}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Tentativa
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
RMSE
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
n\_estimators
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
num\_leaves
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
max\_depth
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
learning\_rate
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Tentativa
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
RMSE
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
n\_estimators
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
num\_leaves
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
max\_depth
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
learning\_rate
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
31 & 0,28546 & 1988 & 202 & 379 & 0,00985 \\
\end{longtable}

\subsection{Tunagem do XGBoost}\label{tunagem-do-xgboost}

~~~Para o algoritmo Extreme Gradient Boosting, os hiperparâmetros
selecionados para a tunagem foi a taxa de aprendizagem, profundidade da
árvore e quantidade máxima de árvores. Na configuração da função
objetivo para a tunagem dos hiperparâmetros pelo optuna, a taxa de
aprendizagem variou entre \(1 \cdot 10^{-7}\) e \(0.5\), a profundidade
da árvore variou entre 3 e 50 e a quantidade de árvores variou entre 50
e 1000.

\vspace{12pt}

A partir da Figura~\ref{fig-xgb_import}, o hiperparâmetro que
representou a maior variação na função objetivo, e portanto a maior
importância para a performance do modelo, foi a taxa de aprendizagem. A
profundidade da árvore representou 22\% da variação e por último vem a
quantidade de árvores. Observando a Figura~\ref{fig-xgb_history}, é
possível ver que o algoritmo de XGBoost foi aquele que apresentou a
menor variação entre os trials, com um único trial com erro acima de
2,0.

\vspace{12pt}

O algoritmo de otimização TPE apresentou uma maior frequência de procura
do hiperparâmetro de taxa de aprendizagem para valores menores, como
pode ser visto na Figura~\ref{fig-xgb_slice}. O mesmo acontece para o
hiperparâmetro de profundidade de árvores, há uma repetição maior para
valores menores, indicando que o a estatística de erro tendeu a ter
valores menores para essa região. Para a quantidade de árvores esse
padrão foi diferente, uma quantidade maior de árvores faz com que o
modelo tenha um melhor ajuste. Essas relações também podem ser
observadas a partir da Figura~\ref{fig-xgb_contour}, menores taxas de
aprendizado, combinado com uma menor profundidade de árvore e maior
quantidade de árvores apresentam menor valor na função objetivo.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-xgb_slice-output-1.pdf}}

}

\subcaption{\label{fig-xgb_slice}}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-xgb_import-output-1.pdf}}

}

\subcaption{\label{fig-xgb_import}}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-xgb_history-output-1.pdf}}

}

\subcaption{\label{fig-xgb_history}}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/figure-pdf/fig-xgb_contour-output-1.pdf}}

}

\subcaption{\label{fig-xgb_contour}}

\end{minipage}%

\caption{\label{fig-xgb_param}Resultados da tunagem do XGBoost.}

\end{figure}%

\begin{longtable}[]{@{}ccccc@{}}
\caption{Melhores hiperparâmetros para Extreme Gradient
Boosting.}\label{tbl-params_xgb}\tabularnewline
\toprule\noalign{}
Tentativa & RMSE & n\_estimators & max\_depth & learning\_rate \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Tentativa & RMSE & n\_estimators & max\_depth & learning\_rate \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
81 & 0,28528 & 788 & 8 & 0,07119 \\
\end{longtable}

A tabela Tabela~\ref{tbl-params_xgb} apresenta a tentativa em que a
combinação de hiperparâmetros resultou no menor erro para o algoritmo de
Extreme Gradient Boosting. Nesse caso, a melhor tentativa foi a de
número 81, com uma configuração de 788 árvores, profundidade máxima de 8
e taxa de aprendizado de 0,07119.

\vspace{12pt}

Para o LGBM, a melhor combinação foi encontrada na tentativa 96, com
1.798 árvores, 247 folhas, profundidade máxima de 299 e taxa de
aprendizado de 0,00903, como apresentado na tabela
Tabela~\ref{tbl-params_lgbm}.

\vspace{12pt}

Por fim, para os algoritmos Gradient Boosting e Random Forest, as
melhores tentativas foram as de números 50 e 67, respectivamente. No
caso do Gradient Boosting, a configuração ideal foi composta por 1.500
árvores, profundidade máxima de 6 e taxa de aprendizado de 0,08730. Já
para o Random Forest, a melhor combinação consistiu em 650 árvores e
profundidade máxima de 22. Os resultados estão na
Tabela~\ref{tbl-params_gdt} e Tabela~\ref{tbl-params_rf},
respectivamente.

\section{Resultados dos modelos}\label{resultados-dos-modelos}

~~~Nesta seção, serão apresentados os resultados do ajuste de cada
algoritmo utilizado: Random Forest, Gradient Boosting, LightGBM e
Extreme Gradient Boosting. A análise do ajuste foi realizada com base no
gráfico que compara os valores estimados aos valores observados dos
imóveis. Além disso, foram utilizadas métricas de erro, como MAPE,
\(R^2\) e RMSE, para avaliar o desempenho dos modelos.

\vspace{12pt}

O ajuste do algoritmo Random Forest é apresentado na figura
Figura~\ref{fig-preds_rf}, que exibe a comparação entre os valores
previstos e observados. Em termos da transformação logarítmica
realizada, a raiz do erro quadrático médio (RMSE) foi de 0,28972,
enquanto o coeficiente de determinação \(R^2\) alcançou \(86,79\%\),
indicando que o modelo explica \(86,79\%\) da variação dos dados. Em
relação ao MAPE, o valor foi de 0,01373, assim, em média, as predições
realizadas pelo modelo Random Forest, considerando a transformação
logarítmica, estiveram 1,373\% distantes dos valores reais.

\vspace{12pt}

Em relação ao algoritmo de Gradient Boosting, o seu ajuste consegue
explicar \(86,98\%\) dos dados, bastante semelhante à Random Forest, com
uma diferença não tão significativa. Por outro lado, o RMSE do Gradient
Boosting foi menor que a Random Forest, tendo sido de 0,28730.
Observando o MAPE, não houve também muitas diferenças, embora o MAPE do
Gradient Boosting tenha sido maior. O MAPE indica que as predições
geradas pelo Gradient Boosting estão \(1,377\%\) distantes de seus
valores observados. Seu ajuste pode ser observado na
Figura~\ref{fig-preds_gdt}.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/rf_plot_predict.pdf}}

}

\subcaption{\label{fig-preds_rf}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/gdt_plot_predict.pdf}}

}

\subcaption{\label{fig-preds_gdt}}

\end{minipage}%

\caption{\label{fig-preds1}Valores previstos em função dos observados do
algoritmo Random Forest e Gradient Boosting, respectivamente.}

\end{figure}%

\vspace{12pt}

O Light Gradient Boosting conseguiu melhorias em relação aos resultados
dos últimos dois algoritmos. Em relação ao seu \(R^2\), o modelo
consegue explicar \(87,17\%\) dos dados. O seu RMSE ficou em \(0,28493\)
e o MAPE em \(0,01346\). Dessa forma, as predições geradas pelo Light
Gradient Boosting estão em média \(1,346\%\) distantes de seus valores
reais. A melhora do ajuste do Light Gradient Boosting fica bastante
perceptível observando a figura de seus valores estimados em função dos
observados (Figura~\ref{fig-preds_lgbm}). As predições geradas pelos
Gradient Boosting e Random Forest desviam bastante, principalmente em
relação ao final da distribuição.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/lgbm_plot_predict.pdf}}

}

\subcaption{\label{fig-preds_lgbm}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/xgb_plot_predict.pdf}}

}

\subcaption{\label{fig-preds_xgb}}

\end{minipage}%

\caption{\label{fig-preds2}Valores previstos em função dos observados do
algoritmo Light Gradient Boosting e Extreme Gradient Boosting,
respectivamente.}

\end{figure}%

\vspace{12pt}

Assim como o LGBM, o Extreme Gradient Boosting também obteve resultados
melhores que os algoritmos de Random Forest e Gradient Boosting.
Entretanto, obteve piores em relação às estatísticas de erro de RMSE e
\(R^2\). O RMSE obtido pelo XGBoost foi de \(0,28659\) e o seu \(R^2\)
foi de \(87,03891\%\), indicando que o modelo consegue explicar
\(87,03891\%\) dos dados. No entanto, obteve um MAPE pouco menor que o
LGBM. As predições geradas pelo Extreme Gradient Boosting estão em média
\(1,357\%\) distantes de seus valores reais.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/stacking_plot_predict.pdf}}

}

\caption{\label{fig-preds_stacking}Valores previstos em função dos
observados do algoritmo Stacking.}

\end{figure}%

\vspace{12pt}

Utilizando os últimos modelos com seus hiperparâmetros otimizados e
utilizando o Light Gradient Boosting como estimador final, as predições
foram feitas com o Stacking. As predições geradas pelo Stacking,
analisando as métricas de erro utilizadas, foram melhores até mesmo que
o Light Gradient Boosting. O RMSE obtido para o Stacking foi de 0,28473
e o modelo consegue explicar \(87,18793\%\). A única métrica que não foi
melhor que do algoritmo LGBM foi o MAPE, que foi de \(1,357\%\), mas a
diferença é pouca.

\section{Impacto e importância das variáveis na
predição}\label{impacto-e-importuxe2ncia-das-variuxe1veis-na-prediuxe7uxe3o}

~~~O impacto e a importância das variáveis na predição de modelos são
fundamentais para entender quais fatores exercem maior influência sobre
as previsões, neste caso, para as predições de valores de imóveis. Esta
seção será dedicada à discussão do impacto das predições individuais e
da importância das variáveis utilizadas na construção do modelo. A
análise será feita com o modelo que apresentou os melhores resultados, o
algoritmo de Stacking.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/pdp_ice.pdf}}

}

\caption{\label{fig-ice_pdp}Gráfico de ICE e PDP.}

\end{figure}%

\vspace{12pt}

Para analisar a influência das variáveis na predição dos valores de
imóveis, foi utilizado o gráfico de ICE em conjunto com a linha de PDP.
Esses gráficos estão representados na Figura~\ref{fig-ice_pdp}.

\vspace{12pt}

As previsões mostram um aumento claro com o tamanho da área, com o PDP
apresentando uma tendência crescente consistente, alinhada à maioria das
curvas ICE. Para a quantidade de banheiro, a relação é majoritariamente
plana, com apenas pequenas variações. Um comportamento semelhante é
observado para a quantidade de quartos, indicando uma influência
limitada dessas variáveis no modelo. No caso do valor médio de aluguel,
o PDP revela uma tendência ascendente bem definida, e as curvas ICE
reforçam esse padrão, sugerindo que valores de aluguel mais altos estão
fortemente associados a preços de imóveis mais elevados. Já a área média
de aluguel apresenta um padrão mais uniforme, indicando uma contribuição
menos significativa para explicar os preços dos imóveis. As curvas ICE
para as coordenadas geográficas exibem um padrão mais complexo nas
previsões. No entanto, é possível identificar um aumento nos valores
previstos dos imóveis à medida que as coordenadas aumentam. Esse
comportamento sugere que os imóveis localizados mais próximos da região
litorânea de João Pessoa tendem a apresentar valores mais elevados. Por
fim, a quantidade de vagas de garagem demonstra uma relação ascendente
clara nas predições, evidenciando uma influência positiva dessa variável
no valor dos imóveis.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/shap_summary_plot.pdf}}

}

\subcaption{\label{fig-shap_summary}Gráfico de resumo dos valores SHAP.}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/shap_importance.pdf}}

}

\subcaption{\label{fig-shap_importance}Importância das variáveis.}

\end{minipage}%

\caption{\label{fig-teste}Impacto e importância das variáveis na
predição a partir do método SHAP.}

\end{figure}%

\vspace{12pt}

A análise da importância das variáveis foi realizada utilizando o método
SHAP. Diferentemente da importância de variáveis gerada por algoritmos
baseados em árvores, que é calculada com base na redução da impureza ou
no ganho de informação ao longo das divisões no modelo, o SHAP é baseado
na magnitude dos valores de Shapley atribuídos a cada variável. Dessa
forma, como pode ser observado na Figura~\ref{fig-shap_importance}, a
variável com maior impacto na predição do modelo é a área, a qual
altera, em média, mais de 25 pontos percentuais na predição absoluta dos
imóveis. A quantidade de vagas de garagem, o valor médio do aluguel e a
localização geográfica também se destacam entre as variáveis mais
relevantes na predição do valor dos imóveis.

\vspace{12pt}

Com a Figura~\ref{fig-shap_summary}, observa-se um resumo da análise dos
valores Shapley. A variável área se destaca como a de maior impacto no
modelo, especialmente para valores elevados, que aumentam
significativamente as predições. As coordenadas geográficas indicam que
valores mais altos tendem a elevar a predição do valor dos imóveis,
refletindo a valorização de imóveis localizados mais próximos às regiões
litorâneas de João Pessoa. Por outro lado, a variável de área média de
aluguel apresenta um padrão menos linear, com valores baixos ainda
gerando impacto positivo nas predições, evidenciando uma relação mais
complexa com o valor dos imóveis.

\begin{figure}

\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_academia.pdf}}\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_area_servico.pdf}}\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_elevador.pdf}}\end{minipage}%
\newline
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_espaco_gourmet.pdf}}\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_piscina.pdf}}\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_playground.pdf}}\end{minipage}%
\newline
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_portaria_24_horas.pdf}}\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_quadra_de_esporte.pdf}}\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_salao_de_festa.pdf}}\end{minipage}%
\newline
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_sauna.pdf}}\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_spa.pdf}}\end{minipage}%
%
\begin{minipage}{0.33\linewidth}
\pandocbounded{\includegraphics[keepaspectratio]{TCC_files/mediabag/includes/dependence_plot_cat/dep_plot_varanda_gourmet.pdf}}\end{minipage}%

\caption{\label{fig-dependence_plot}Gráfico de dependência de valores
SHAP para variáveis binárias.}

\end{figure}%

\vspace{12pt}

Para analisar as variáveis binárias, foram utilizados gráficos de
dependência para cada uma delas, os quais podem ser visualizados na
Figura~\ref{fig-dependence_plot}. Ao analisar os valores SHAP para os
imóveis com e sem academia, observa-se que essa variável tende a
aumentar a predição para imóveis que possuem academia. O mesmo padrão é
identificado para imóveis com elevador, embora aqueles sem elevador
também apresentem uma tendência de aumento na predição, o que pode ser
explicado pela influência de outras características dos imóveis. Como já
mencionado na análise da importância das variáveis, sauna e spa são
características com baixa importância. No entanto, ao observar seus
gráficos de dependência, é possível perceber que essas variáveis podem
ser úteis na avaliação de imóveis que possuem essa característica e tem
valores mais elevados. O mesmo comportamento é observado para varanda
gourmet, playground e espaço gourmet.

\section{Aplicação final}\label{aplicauxe7uxe3o-final}

\chapter{Conclusão}\label{conclusuxe3o}

\chapter*{\texorpdfstring{\centering Referências}{Referências}}\label{referuxeancias}

\markboth{Referências}{Referências}

\phantomsection\label{refs}
\begin{CSLReferences}{0}{1}
\bibitem[\citeproctext]{ref-abecip_credito2007}
ABECIP. \textbf{O Crédito Imobiliário no Brasil: Caracterização e
Desafios}. {[}S.l.{]}: Associação Brasileira das Entidades de Crédito
Imobiliário e Poupança, 2007b.

\bibitem[\citeproctext]{ref-revolucao_credito_imobiliario}
\_\_\_\_\_\_. \textbf{A Revolução do Crédito Imobiliário: 44 Anos
(1967--2011)}. {[}S.l.{]}: Associação Brasileira das Entidades de
Crédito Imobiliário e Poupança, 2007a.

\bibitem[\citeproctext]{ref-abecip_monografia}
\_\_\_\_\_\_. \textbf{IV Prêmio ABECIP de Monografia em Crédito
Imobiliário e Poupança}. {[}S.l.{]}: Associação Brasileira das Entidades
de Crédito Imobiliário e Poupança, 2015.

\bibitem[\citeproctext]{ref-optuna_2019}
AKIBA, T. et al. Optuna: A Next-generation Hyperparameter Optimization
Framework. {[}S.l.{]}: {[}s.n.{]}, 2019.

\bibitem[\citeproctext]{ref-quarto}
ALLAIRE, J.; DERVIEUX, C.
\textbf{\href{https://CRAN.R-project.org/package=quarto}{quarto: R
Interface to 'Quarto' Markdown Publishing System}}. {[}S.l.{]}:
{[}s.n.{]}, 2024.

\bibitem[\citeproctext]{ref-assumpccao2011credito}
ASSUMPÇÃO FILHO, C. A. B. O cr{é}dito imobili{á}rio no Brasil. 2011.

\bibitem[\citeproctext]{ref-sqlalchemy}
BAYER, M. \href{http://aosabook.org/en/sqlalchemy.html}{SQLAlchemy}.
\emph{Em}: BROWN, A.; WILSON, G. (Org.). \textbf{The Architecture of
Open Source Applications Volume II: Structure, Scale, and a Few More
Fearless Hacks}. {[}S.l.{]}: aosabook.org, 2012.

\bibitem[\citeproctext]{ref-bergstra2011algorithms}
BERGSTRA, J. et al. Algorithms for hyper-parameter optimization.
\textbf{Advances in neural information processing systems}, 2011. v. 24.

\bibitem[\citeproctext]{ref-bergstra2013making}
\_\_\_\_\_\_; YAMINS, D.; COX, D. Making a science of model search:
Hyperparameter optimization in hundreds of dimensions for vision
architectures. {[}S.l.{]}: PMLR, 2013. p. 115--123.

\bibitem[\citeproctext]{ref-bischl2023hyperparameter}
BISCHL, B. et al. Hyperparameter optimization: Foundations, algorithms,
best practices, and open challenges. \textbf{Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery}, 2023. v. 13, n. 2, p.
e1484.

\bibitem[\citeproctext]{ref-breiman1996bagging}
BREIMAN, L. Bagging predictors. \textbf{Machine learning}, 1996. v. 24,
p. 123--140.

\bibitem[\citeproctext]{ref-geocoder}
CAMBON, J. et al. tidygeocoder: An R package for geocoding.
\textbf{Journal of Open Source Software}, 2021. v. 6, n. 65, p. 3544.
Disponível em:
\textless{}\url{https://doi.org/10.21105/joss.03544}\textgreater.

\bibitem[\citeproctext]{ref-campos2014precificaccao}
CAMPOS, S. F. Precifica{ç}{ã}o de im{ó}veis e seus elementos agregadores
de valor sob a vis{ã}o do consumidor: uma an{á}lise do mercado
imobili{á}rio de Jo{ã}o Pessoa-PB. 2014.

\bibitem[\citeproctext]{ref-chen2016xgboost}
CHEN, T.; GUESTRIN, C. Xgboost: A scalable tree boosting system.
{[}S.l.{]}: {[}s.n.{]}, 2016. p. 785--794.

\bibitem[\citeproctext]{ref-cohen2009pearson}
COHEN, I. et al. Pearson correlation coefficient. \textbf{Noise
reduction in speech processing}, 2009. p. 1--4.

\bibitem[\citeproctext]{ref-evolucao_abecip}
COSTA FARIAS, B. M. Da. A evolu{ç}{ã}o do mercado imobili{á}rio
brasileiro e o conceito de Home Equity. 2010.

\bibitem[\citeproctext]{ref-fipe_metodologia}
FIPE -- FUNDAÇÃO INSTITUTO DE PESQUISAS ECONÔMICAS. \textbf{Índice
FipeZap de Preços de Imóveis Anunciados: Notas Metodológicas}. São
Paulo: Fipe, 2024.

\bibitem[\citeproctext]{ref-friedman2002stochastic}
FRIEDMAN, J. H. Stochastic gradient boosting. \textbf{Computational
statistics \& data analysis}, 2002. v. 38, n. 4, p. 367--378.

\bibitem[\citeproctext]{ref-garnett2023bayesian}
GARNETT, R. \textbf{Bayesian optimization}. {[}S.l.{]}: Cambridge
University Press, 2023.

\bibitem[\citeproctext]{ref-goldstein2015peeking}
GOLDSTEIN, A. et al. Peeking inside the black box: Visualizing
statistical learning with plots of individual conditional expectation.
\textbf{journal of Computational and Graphical Statistics}, 2015. v. 24,
n. 1, p. 44--65.

\bibitem[\citeproctext]{ref-harris2020array}
HARRIS, C. R. et al. Array programming with {NumPy}. \textbf{Nature},
set. 2020. v. 585, n. 7825, p. 357--362. Disponível em:
\textless{}\url{https://doi.org/10.1038/s41586-020-2649-2}\textgreater.

\bibitem[\citeproctext]{ref-hastie2009elements}
HASTIE, T. et al. \textbf{The elements of statistical learning: data
mining, inference, and prediction}. {[}S.l.{]}: Springer, 2009. V. 2.

\bibitem[\citeproctext]{ref-shammamah_hossain-proc-scipy-2019}
HOSSAIN, Shammamah.
\href{https://doi.org/10.25080/Majora-7ddc1dd1-012}{{V}isualization of
{B}ioinformatics {D}ata with {D}ash {B}io}. (Chris Calloway et al.,
Org.). {[}S.l.{]}: {[}s.n.{]}, 2019. p. 126--133.

\bibitem[\citeproctext]{ref-Hunter:2007}
HUNTER, J. D. \href{https://doi.org/10.1109/MCSE.2007.55}{Matplotlib: A
2D graphics environment}. \textbf{Computing in Science \& Engineering},
2007. v. 9, n. 3, p. 90--95.

\bibitem[\citeproctext]{ref-pmlr-v32-hutter14}
HUTTER, F.; HOOS, H.; LEYTON-BROWN, K. An Efficient Approach for
Assessing Hyperparameter Importance. (E. P. Xing \& T. Jebara, Org.).
Bejing, China: PMLR, 2014. V. 32, p. 754--762. Disponível em:
\textless{}\url{https://proceedings.mlr.press/v32/hutter14.html}\textgreater.

\bibitem[\citeproctext]{ref-plotly}
INC., P. T. Collaborative data science. 2015. Disponível em:
\textless{}\url{https://plot.ly}\textgreater.

\bibitem[\citeproctext]{ref-james2013introduction}
JAMES, G. et al. \textbf{An introduction to statistical learning}.
{[}S.l.{]}: Springer, 2013. V. 112.

\bibitem[\citeproctext]{ref-ke2017lightgbm}
KE, G. et al. Lightgbm: A highly efficient gradient boosting decision
tree. \textbf{Advances in neural information processing systems}, 2017.
v. 30.

\bibitem[\citeproctext]{ref-kouzis2016learning}
KOUZIS-LOUKAS, D. \textbf{Learning Scrapy}. {[}S.l.{]}: Packt Publishing
Ltd, 2016.

\bibitem[\citeproctext]{ref-NIPS2017_7062}
LUNDBERG, S. M.; LEE, S.-I.
\href{http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}{A
Unified Approach to Interpreting Model Predictions}. \emph{Em}: GUYON,
I. et al. (Org.). \textbf{Advances in Neural Information Processing
Systems 30}. {[}S.l.{]}: Curran Associates, Inc., 2017, p. 4765--4774.

\bibitem[\citeproctext]{ref-molnar2020interpretable}
MOLNAR, C. \textbf{Interpretable machine learning}. {[}S.l.{]}: Lulu.
com, 2020.

\bibitem[\citeproctext]{ref-scikit-learn}
PEDREGOSA, F. et al. Scikit-learn: Machine Learning in {P}ython.
\textbf{Journal of Machine Learning Research}, 2011. v. 12, p.
2825--2830.

\bibitem[\citeproctext]{ref-folium}
PYTHON-VISUALIZATION. \textbf{Folium}. Disponível em:
\textless{}\url{https://python-visualization.github.io/folium/}\textgreater.

\bibitem[\citeproctext]{ref-r_language}
R CORE TEAM. \textbf{\href{https://www.R-project.org/}{R: A Language and
Environment for Statistical Computing}}. Vienna, Austria: R Foundation
for Statistical Computing, 2024.

\bibitem[\citeproctext]{ref-ribeiro2016should}
RIBEIRO, M. T.; SINGH, S.; GUESTRIN, C. " Why should i trust you?"
Explaining the predictions of any classifier. {[}S.l.{]}: {[}s.n.{]},
2016. p. 1135--1144.

\bibitem[\citeproctext]{ref-shapley1953value}
SHAPLEY, L. S. A value for n-person games. \textbf{Contribution to the
Theory of Games}, 1953. v. 2.

\bibitem[\citeproctext]{ref-snoek2012practical}
SNOEK, J.; LAROCHELLE, H.; ADAMS, R. P. Practical bayesian optimization
of machine learning algorithms. \textbf{Advances in neural information
processing systems}, 2012. v. 25.

\bibitem[\citeproctext]{ref-spearman1961proof}
SPEARMAN, C. The proof and measurement of association between two
things. 1961.

\bibitem[\citeproctext]{ref-vstrumbelj2014explaining}
ŠTRUMBELJ, E.; KONONENKO, I. Explaining prediction models and individual
predictions with feature contributions. \textbf{Knowledge and
information systems}, 2014. v. 41, p. 647--665.

\bibitem[\citeproctext]{ref-reback2020pandas}
TEAM, T. Pandas Development. \textbf{pandas-dev/pandas: Pandas}. Zenodo.
Disponível em:
\textless{}\url{https://doi.org/10.5281/zenodo.3509134}\textgreater.

\bibitem[\citeproctext]{ref-selenium}
THORPE, A.
\textbf{\href{https://CRAN.R-project.org/package=selenium}{selenium:
Low-Level Browser Automation Interface}}. {[}S.l.{]}: {[}s.n.{]}, 2024.

\bibitem[\citeproctext]{ref-van1995python}
VAN ROSSUM, G.; DRAKE JR, F. L. \textbf{Python reference manual}.
{[}S.l.{]}: Centrum voor Wiskunde en Informatica Amsterdam, 1995.

\bibitem[\citeproctext]{ref-2020SciPy-NMeth}
VIRTANEN, P. et al.
\href{https://doi.org/10.1038/s41592-019-0686-2}{{{SciPy} 1.0:
Fundamental Algorithms for Scientific Computing in Python}}.
\textbf{Nature Methods}, 2020. v. 17, p. 261--272.

\bibitem[\citeproctext]{ref-wagner1980urbanization}
WAGNER, F. E.; WARD, J. O. Urbanization and Migration in Brazil.
\textbf{The American Journal of Economics and Sociology}, 1980. v. 39,
n. 3, p. 249--259.

\bibitem[\citeproctext]{ref-Waskom2021}
WASKOM, M. L. seaborn: statistical data visualization. \textbf{Journal
of Open Source Software}, 2021. v. 6, n. 60, p. 3021. Disponível em:
\textless{}\url{https://doi.org/10.21105/joss.03021}\textgreater.

\bibitem[\citeproctext]{ref-httr}
WICKHAM, H. \textbf{\href{https://CRAN.R-project.org/package=httr}{httr:
Tools for Working with URLs and HTTP}}. {[}S.l.{]}: {[}s.n.{]}, 2023.

\bibitem[\citeproctext]{ref-rvest}
\_\_\_\_\_\_. \textbf{\href{https://rvest.tidyverse.org/}{rvest: Easily
Harvest (Scrape) Web Pages}}. {[}S.l.{]}: {[}s.n.{]}, 2024.

\bibitem[\citeproctext]{ref-xml2}
\_\_\_\_\_\_; HESTER, J.; OOMS, J.
\textbf{\href{https://xml2.r-lib.org/}{xml2: Parse XML}}. {[}S.l.{]}:
{[}s.n.{]}, 2023.

\bibitem[\citeproctext]{ref-yang2020hyperparameter}
YANG, L.; SHAMI, A. On hyperparameter optimization of machine learning
algorithms: Theory and practice. \textbf{Neurocomputing}, 2020. v. 415,
p. 295--316.

\bibitem[\citeproctext]{ref-yeo}
YEO, I.-K.; JOHNSON, R. A. A New Family of Power Transformations to
Improve Normality or Symmetry. \textbf{Biometrika}, 2000. v. 87, n. 4,
p. 954--959. Disponível em:
\textless{}\url{http://www.jstor.org/stable/2673623}\textgreater. Acesso
em: 25 out. 2023.

\end{CSLReferences}




\end{document}
