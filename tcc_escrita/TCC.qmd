---
nocite: |
  @bischl2023hyperparameter
  @garnett2023bayesian
format:
  pdf:
    include-before-body:
      text: |
        \numberwithin{algorithm}{chapter}
        \algrenewcommand{\algorithmiccomment}[1]{\hskip3em$\rightarrow$ #1}
crossref:
  custom:
    - kind: float
      key: algo
      reference-prefix: "Algoritmo"
      caption-prefix: "Algoritmo"
      latex-env: algo
      latex-list-of-description: Algoritmo
filters:
  - pseudocode
pseudocode:
  caption-prefix: "Algoritmo"
  reference-prefix: "Algoritmo"
  caption-number: true
number-sections: true
indent: true
documentclass: scrreprt
whitespace: small
lang: pt-br
bibliography: includes/bib.bib
csl: includes/ufpe-abnt.csl
urlcolor: Goldenrod
# linkcolor: Goldenrod
toc: true
title: |
  ![](includes/ufpb.png){width=1in}

  Escrever título (escolher no final)
subtitle: Universidade Federal da Paraíba - CCEN
author: Gabriel de Jesus Pereira
mermaid:
  theme: forest
date: today
date-format: long
highlight-style: github
fontsize: 12pt
interlinespace: 1.5pt
fig-cap-location: bottom
warning: false
echo: false
include-in-header:
  - text: |
      \usepackage{pdflscape}
      \newcommand{\blandscape}{\begin{landscape}}
      \newcommand{\elandscape}{\end{landscape}}
---

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import os

rename = {
  'Area': 'Área',
  'Area servico': 'Área de serviço',
  'Condominio': 'Condomínio',
  'Espaco gourmet': 'Espaço gourmet',
  'Iptu': 'IPTU',
  'Salao de festa': 'Salão de festa',
  'Area aluguel': 'Área de aluguel',
  'Valor aluguel': 'Valor de aluguel'
}

train_df = pd.read_csv('../data/cleaned/train.csv').drop(columns=['qnt_beneficio'])
test_df = pd.read_csv('../data/cleaned/test.csv').drop(columns=['qnt_beneficio'])

train_df.columns = train_df.columns\
  .str.capitalize().str.replace('_', ' ')
test_df.columns = test_df.columns\
  .str.capitalize().str.replace('_', ' ')

train_df = train_df.rename(columns=rename)
test_df = test_df.rename(columns=rename)
```

\renewcommand{\listalgorithmname}{Lista de algoritmos}
\bgroup
\hypersetup{linkcolor = black}
\listofalgorithms
\egroup


\bgroup
\hypersetup{linkcolor = black}
\listoffigures
\egroup

# Resumo

# Introdução

## Objetivos

### Objetivo Geral

### Objetivos Específicos

## Organização do Trabalho

\newpage

# Recursos Computacionais

### Linguagem de Programação R

### Linguagem de Programação Python

### Quarto

### Linguagem de Programação Python

### Web Scraping

\newpage

# Algoritmos de Aprendizado de Máquina

\ \ \ Neste capítulo, serão descritos os algoritmos de aprendizado de máquina utilizados neste trabalho. Alguns dos métodos utilizados podem fazer uso de diversos algoritmos ou modelos estatísticos. No entanto, o foco principal e o mais utilizado foram as árvores de decisão, especialmente em sua forma particular, as árvores de regressão. Assim, os algoritmos descritos são métodos baseados em árvores.

\vspace{12pt}

\ \ \ Os métodos baseados em árvore envolvem a estratificação ou segmentação do espaço dos preditores^[O espaço dos preditores é o conjunto de todos os valores possíveis para as variáveis independentes $\mathbf{x}$] em várias regiões simples. Dessa forma, todos os algoritmos utilizados neste trabalho partem dessa ideia. Portanto, o primeiro a ser explicado será o de árvores de decisão, pois fundamenta todos os outros algoritmos. Depois das árvores de decisão, serão explicados os métodos ensemble e, por fim, diferentes variações do método de gradient boosting.


## Árvores de decisão

\ \ \ Árvores de decisão podem ser utilizadas tanto para regressão quanto para classificação. Elas servem de base para os modelos baseados em árvores empregados neste trabalho, focando particularmente nas árvores de regressão^[Uma árvore de regressão é um caso específico da árvore de decisão, mas para regressão.]. O processo de construção de uma árvore se baseia no particionamento recursivo do espaço dos preditores, onde cada particionamento é chamado de nó e o resultado final é chamado de folha ou nó terminal. Em cada nó, é definida uma condição e, caso essa condição seja satisfeita, o resultado será uma das folhas desse nó. Caso contrário, o processo segue para o próximo nó e verifica a próxima condição, podendo gerar uma folha ou outro nó. Veja um exemplo na @fig-arvore.

```{mermaid}
%%| label: fig-arvore
%%| fig-cap: Exemplo de estrutura de árvore de regressão. A árvore tem cinco folhas e quatro nós internos.
%%| fig-width: 5.5

graph TD
    A[Condição 1] -->|Condição 1: Sim| B[Condição 2]
    A -->|Condição 1: Não| C[Condição 3]
    B -->|Condição 2: Sim| D[Folha 1]
    B -->|Condição 2: Não| E[Folha 2]
    C -->|Condição 3: Sim| F[Folha 3]
    C -->|Condição 3: Não| G[Condição 4]
    G -->|Condição 4: Sim| H[Folha 4]
    G -->|Condição 4: Não| I[Folha 5]
```

\vspace{12pt}

\ \ \ O espaço dos preditores é dividido em $J$ regiões distintas e disjuntas denotadas por $R_1, R_2, \dots, R_J$. Essas regiões são construídas em formato de caixa de forma a minimizar a soma dos quadrados dos resíduos. Dessa forma, pode-se modelar a variável resposta como uma constante $c_j$ em cada região $R_j$

$$
f\left(x\right) = \sum^J_{j=1}c_j I\left(x \in R_j \right)
$$

\ \ \ O estimador para a constante $c_j$ é encontrado pelo método de mínimos quadrados. Assim, deve-se minimizar $\sum_{x_i \in R_j} \left[y_i - f\left(x_i\right)\right]^2$. No entanto, perceba que $f\left(x_i\right)$ está sendo avaliado somente em um ponto específico $x_i$, o que reduzirá $f\left(x_i\right)$ para uma constante $c_j$. É fácil de se chegar ao resultado se for observada a definição da função indicadora $I\left(x \in R_j\right)$

$$
I_{R_j}(x_i) =
\begin{cases}
    1,& \text{se } x_i \in R_j \\
    0,& \text{se } x_i \notin R_j
\end{cases}
$$
Como as regiões são disjuntas, $x_i$ não pode estar simultaneamente em duas regiões. Assim, para um ponto específico $x_i$, apenas um dos casos da função indicadora será diferente de 0. Portanto, $f\left(x_i\right) = c_j$. Agora, derivando $\sum_{x_i \in R_j}\left(yi - c_j\right)^2$ em relação a $c_j$

$$
\frac{\partial}{\partial{c_j}}\sum_{x_i \in R_j} \left(y_i - c_j\right)^2 = -2\sum_{x_i \in R_j} \left(y_i - c_j\right)
$${#eq-partialdev}
e igualando @eq-partialdev a 0, tem-se a seguinte igualdade

$$
\sum_{x_i \in R_j} \left(y_i - \hat{c}_j\right) = 0
$$
que se abrirmos o somatório e dividirmos pelo número total de pontos $N_j$ na região $R_j$, teremos que o estimador de $c_j$ será simplesmente a média dos $y_i$ na região $R_j$:

$$
\sum_{x_i \in R_j} y_i - \hat{c}_j N_j = 0 \Rightarrow \hat{c}_j = \frac{1}{N_{j}}\sum_{x_i \in R_j} y_i
$${#eq-estimacjdev}

\vspace{12pt}

No entanto, @james2013introduction caracteriza como inviável considerar todas as possíveis partições do espaço das variáveis em $J$ caixas devido ao alto custo computacional. Dessa forma, a abordagem a ser adotada é uma divisão binária recursiva. O processo começa no topo da árvore de regressão, o ponto em que contém todas as observações, e continua sucessivamente dividindo o espaço dos preditores. As divisões são indicadas como dois novos ramos na árvore, como pode ser visto na [@fig-arvore].

\vspace{12pt}

Para executar a divisão binária recursiva, deve-se primeiramente selecionar a variável independente $X_j$ e o ponto de corte $s$ tal que a divisão do espaço dos preditores conduza a maior redução possível na soma dos quadrados dos resíduos. Dessa forma, definimos dois semi-planos

$$
R_{1}\left(j, s\right) = \{X | X_j \leq s\} \text{ e } R_{2}\left(j, s\right) = \{X | X_j > s\}
$$
e procuramos a divisão da variável $j$ e o ponto de corte $s$ que resolve a equação

$$
\min_{j, s}\left[\min_{c_1} \sum_{x_i \in R_1\left(j, s\right)} \left(y_i - c_{1}\right)^2 + \min_{c_2} \sum_{x_i \in R_2\left(j, s\right)} \left(y_i - c_{2}\right)^2\right]
$$
em que $c_1$ e $c_2$ é a média da variável dependente para as observações de treinamento nas regiões $R_1\left(j, s\right)$ e $R_2\left(j, s\right)$, respectivamente. Assim, encontrando a melhor divisão, os dados são particionados nas duas regiões resultantes e o processo de divisão é repetido em todas as outras regiões.

\vspace{12pt}

\ \ \ O tamanho da árvore pode ser considerado um hiperparâmetro para regular a complexidade do modelo, pois uma árvore muito grande pode causar sobreajuste aos dados de treinamento, capturando não apenas os padrões relevantes, mas também o ruído. Como resultado, o modelo pode apresentar bom desempenho nos dados de treinamento, mas falhar ao lidar com novos dados devido à sua incapacidade de generalização. Por outro lado, uma árvore muito pequena pode não captar padrões, relações e estruturas importantes presentes nos dados. Dessa forma, a estratégia adotada para selecionar o tamanho da árvore consiste em crescer uma grande árvore $T_0$, interrompendo o processo de divisão apenas ao atingir um tamanho mínimo de nós. Posteriormente, a árvore $T_0$ é podada utilizando o critério de custo complexidade, que será definido a seguir.

\vspace{12pt}

\ \ \ Para o processo de poda da árvore, definimos uma árvore qualquer $T$ que pode ser obtida através do processo da poda de $T_0$, de modo que $T \subset T_0$. Assim, sendo $N_j$ a quantidade de pontos na região $R_j$, seja

$$
Q_j\left(T\right) = \frac{1}{N_j} \sum_{x_i \in R_j}\left(y_i - \hat{c}_j\right)^2
$$
uma medida de impureza do nó pelo erro quadrático médio. Assim, define-se o critério de custo complexidade

$$
C_{\alpha}\left(T\right) = \sum_{m = 1}^{|T|}N_jQ_j\left(T\right) + \alpha |T|
$$
onde $|T|$ denota a quantidade total de folhas, e $\alpha \geq 0$ é um hiperparâmetro que equilibra o tamanho da árvore e a adequação aos dados. A ideia é encontrar, para cada $\alpha$, a árvore $T_{\alpha} \subset T_0$ que minimiza $C_{\alpha}\left(T\right)$. Valores grandes de $\alpha$ resultam em árvores menores, enquanto valores menores resultam em árvores maiores, e $\alpha = 0$ resulta na própria árvore $T_0$. A busca por $T_{\alpha}$ envolve colapsar sucessivamente o nó interno que provoca o menor aumento em $\sum_j N_j Q_j\left(T\right)$, continuando o processo até produzir uma árvore com um único nó. Esse processo gera uma sequência de subárvores, na qual existe uma única subárvore menor que, para cada $\alpha$, minimiza $C_{\alpha}\left(T\right)$.

\vspace{12pt}

\ \ \ A estimação de $\alpha$ é realizada por validação cruzada com cinco ou dez folds, sendo $\hat \alpha$ escolhido para minimizar a soma dos quadrados dos resíduos durante o processo de validação cruzada. Assim, a árvore final será $T_{\hat \alpha}$. O @algo-buildtree exemplifica o processo de crescimento de uma árvore de regressão:

::: {#algo-buildtree}

```pseudocode
#| pdf-line-number: false

\begin{algorithm}
\caption{Algoritmo para crescer uma árvore de regressão}
\begin{algorithmic}
\State \textbf{1.} Use a divisão binária recursiva para crescer uma árvore grande $T_0$ nos dados de treinamento, parando apenas quando cada folha tiver menos do que um número mínimo de observações.

\vspace{3.7pt}

\State \textbf{2.} Aplique o critério custo de complexidade à árvore grande \( T_0 \) para obter uma sequência de melhores subárvores \( T_\alpha \), em função de \( \alpha \).

\vspace{3.7pt}

\State \textbf{3.} Use validação cruzada $K\text{-fold}$ para escolher \( \alpha \). Isto é, divida as observações de treinamento em $K$ folds. Para cada \( k = 1, \ldots, K \):
    \State \hspace{1em} (a) Repita os Passos 1 e 2 em todos os folds, exceto no $k\text{-ésimo}$ fold dos dados de
    \State \hspace{1em} treinamento.
    \State \hspace{1em} (b) Avalie o erro quadrático médio de previsão nos dados no $k\text{-ésimo}$ fold deixado
    \State \hspace{1em} de fora, em função de \( \alpha \). Faça a média dos resultados para cada valor de \( \alpha \) e
    \State \hspace{1em} escolha \( \alpha \) que minimize o erro médio.

\vspace{3.7pt}

\State \textbf{4.} Retorne a subárvore \( T_{\hat{\alpha}} \) do Passo 2 que corresponde ao valor estimado de \( \alpha \).
\end{algorithmic}
\end{algorithm}
```

Fonte: @james2013introduction [p. 337].

:::

\vspace{12pt}

\ \ \ No caso de uma árvore de decisão para classificação, a principal diferença está no critério de divisão dos nós e na poda da árvore. Para a classificação, a previsão em um nó $j$, correspondente a uma região $R_j$ com $N_j$ observações, será simplesmente a classe majoritária. Assim, tem-se

$$
\hat{p}_{jk} = \frac{1}{N_j}\sum_{x_i \in R_j} I\left(y_i = k\right)
$$
como a proporção de observações da classe $k$ no nó $j$. Dessa forma, as observações no nó $j$ são classificadas na classe $k\left(j\right) = \arg \max_{k} \hat{p}_{jk}$, que é a moda no nó $j$.

\vspace{12pt}

\ \ \ Para a divisão dos nós no caso da regressão, foi utilizado o erro quadrático médio como medida de impureza. Para a classificação, algumas medidas comuns para $Q_j\left(T\right)$ são o erro de classificação, o índice de Gini ou a entropia cruzada.

## Métodos Ensemble

\ \ \ As árvores de decisão são conhecidas por sua alta interpretabilidade, mas geralmente apresentam um desempenho preditivo inferior em comparação com outros modelos e algoritmos. No entanto, é possível superar essa limitação construindo um modelo preditivo que combina a força de uma coleção de estimadores base, um processo conhecido como aprendizado em conjunto (Ensemble Learning). De acordo com @hastie2009elements, o aprendizado em conjunto pode ser dividido em duas etapas principais: a primeira etapa consiste em desenvolver uma população de algoritmos de aprendizado base a partir dos dados de treinamento, e a segunda etapa envolve a combinação desses algoritmos para formar um estimador agregado. Portanto, nesta seção, serão definidos os métodos de aprendizado em conjunto utilizados neste trabalho.

### Bagging

\ \ \ O algoritmo de Bootstrap Aggregation, ou Bagging, foi introduzido por @breiman1996bagging. Sua ideia principal é gerar um estimador agregado a partir de múltiplas versões de um preditor, que são criadas por meio de amostras bootstrap do conjunto de treinamento, utilizadas como novos conjuntos de treinamento. O Bagging pode ser empregado para melhorar a estabilidade e a precisão de modelos ou algoritmos de aprendizado de máquina, além de reduzir a variância e evitar o sobreajuste. Por exemplo, o Bagging pode ser utilizado para melhorar o desempenho da árvore de regressão descrita anteriormente.

\vspace{12pt}

\ \ \ @breiman1996bagging define formalmente o algoritmo de Bagging, que utiliza um conjunto de treinamento $\mathcal{L}$. A partir desse conjunto, são geradas amostras bootstrap $\mathcal{L}^{(B)}$ com $B$ réplicas, formando uma coleção de modelos $\{f(x, \mathcal{L}^{(B)})\}$, onde $f$ representa um modelo estatístico ou algoritmo treinado nas amostras bootstrap para prever ou classificar uma variável dependente $y$ com base em variáveis independentes $\mathbf{x}$. Se a variável dependente $y$ for numérica, a predição é obtida pela média das previsões dos modelos:

$$
f_{B}\left(x\right) = \frac{1}{B} \sum_{b = 1}^B f \left(x, \mathcal{L}^{\left(B\right)}\right)
$$
onde $f_{B}$ representa a predição agregada. No caso em que $y$ prediz uma classe, utiliza-se a votação majoritária. Ou seja, se estivermos classificando em classes $j \in {1, \dots, J}$, então $N_j = \#\{B; f(x, \mathcal{L}^{(b)}) = j\}$ representa o número de vezes que a classe $j$ foi predita pelos estimadores. Assim,

$$
f_{B}\left(x\right) = \arg \max_{j} N_j
$$
isto é, o $j$ para o qual $N_j$ é máximo

\vspace{12pt}

\ \ \ Embora a técnica de Bagging possa melhorar o desempenho de uma árvore de regressão ou de classificação, isso geralmente vem ao custo de menor interpretabilidade. Quando o Bagging é aplicado a uma árvore de regressão, construímos $B$ árvores de regressão usando $B$ réplicas de amostras bootstrap e tomamos a média das predições resultantes [@james2013introduction]. Nesse processo, as árvores de regressão crescem até seu máximo, sem passar pelo processo de poda, resultando em cada árvore individual com alta variância e baixo viés. No entanto, ao agregar as predições das $B$ árvores, a variância é reduzida.

\vspace{12pt}

\ \ \ Para mitigar a falta de interpretabilidade do método Bagging aplicado a árvores de regressão, pode-se usar a medida de impureza baseada no erro quadrático médio, definida anteriormente, como uma métrica de importância das variáveis independentes. Um valor elevado na redução total média do erro quadrático médio, calculado com base nas divisões realizadas por um determinado preditor em todas as $B$ árvores, indica que o preditor é importante.

\vspace{12pt}

\ \ \ As árvores construídas pelo algoritmo de árvore de decisão se beneficiam da proposta de agregação do Bagging, mas esse benefício é limitado devido à correlação positiva existente entre as árvores. Se as árvores forem variáveis aleatórias independentes e identicamente distribuídas, cada uma com variância $\sigma^2$, a variância da média das previsões das $B$ árvores será $\frac{1}{B} \sigma^2$. No entanto, se as árvores forem apenas identicamente distribuídas, mas não necessariamente independentes, e apresentarem uma correlação positiva $\rho$, a esperança da média das $B$ árvores será a mesma que a esperança de uma árvore individual. Portanto, o viés do agregado das árvores é o mesmo das árvores individuais, e a melhoria é alcançada apenas pela redução da variância. A variância da média das previsões será dada por:

$$
\rho \sigma^2 + \frac{1 - \rho}{B}\sigma^2
$${#eq-cor}

\ \ \ Isso significa que, à medida que o número de árvores $B$ aumenta, o segundo termo da soma se torna menos significativo. Portanto, os benefícios da agregação proporcionados pelo algoritmo de Bagging são limitados pela correlação entre as árvores [@hastie2009elements]. Mesmo com o aumento do número de árvores no Bagging, a correlação entre elas impede que as previsões individuais sejam completamente independentes, resultando em menor diminuição da variância da média das previsões do que seria esperado se as árvores fossem totalmente independentes. Uma maneira de melhorar o algoritmo de Bagging é por meio do Random Forest, que será descrito a seguir.

### Random Forest

\ \ \ O algoritmo Random Forest é uma técnica derivada do método de Bagging, mas com modificações específicas na construção das árvores. O objetivo é melhorar a redução da variância ao diminuir a correlação entre as árvores, sem aumentar significativamente a variabilidade. Isso é alcançado durante o processo de crescimento das árvores por meio da seleção aleatória de variáveis independentes.

::: {#algo-rf}

```pseudocode
#| pdf-line-number: false

\begin{algorithm}
\caption{Algoritmo de uma Random Forest para regressão ou classificação}
\begin{algorithmic}
\State \hspace{1em} \textbf{1.} Para b = 1 até B:

\vspace{0.8em}

    \State \hspace{2em} (a) Construa amostras bootstrap $\mathbf{\mathcal{L}}^*$ de tamanho \( N \) dos dados de
    \State \hspace{2em} \vspace{0.1em} treinamento.

    \State \hspace{2em} (b) Faça crescer uma árvore de floresta aleatória \( T_b \) para os dados bootstrap,
    \State \hspace{2em} repetindo recursivamente os seguintes passos para cada folha da árvore, até que
    \State \hspace{2em} \vspace{0.5em} o tamanho mínimo do nó \( n_{min} \) seja atingido.
    \State \hspace{4em} \vspace{0.1em} i. Selecione \( m \) variáveis aleatoriamente entre as \( p \) variáveis.
    \State \hspace{4em} \vspace{0.1em} ii. Escolha a melhor variável entre as \( m \).
    \State \hspace{4em} \vspace{0.1em} iii. Divida o nó em dois subnós.

\vspace{0.8em}

\State \hspace{1em} \textbf{2.} Por fim, o conjunto de árvores \( \{T_b\}^{B}_1\) é construído.

\vspace{1em}

\State \hspace{0.7em} No caso da regressão, para fazer uma predição em um novo ponto \( x \), temos a seguinte função:


$$
\hat{f}^{B}_{rf}\left(x\right) = \frac{1}{B}\sum^{B}_{b = 1} T_{b}\left(x\right)
$$

\vspace{1em}

\State \hspace{0.7em} Para a classificação é utilizado o voto majoritário. Assim, seja $\hat{C}_{b}\left(x\right)$ a previsão da classe da árvore de floresta aleatória $b$. Então,

$$
\hat{C}^{B}_{rf}\left(x\right) = \arg \max_c \sum^{B}_{b = 1}I\left(\hat{C}_b\left(x\right) = c\right)
$$

\State onde $c$ representa as classes possíveis.

\end{algorithmic}
\end{algorithm}
```

Fonte: @hastie2009elements [p. 588].

:::

\vspace{12pt}

\ \ \  No algoritmo Random Forest, ao construir uma árvore a partir de amostras bootstrap, antes de cada divisão, selecionam-se aleatoriamente $m \leq p$ das $p$ variáveis independentes como candidatas para a divisão (com $m = p$ no caso do Bagging). Apenas uma dessas $m$ variáveis é usada para realizar a divisão, com base em critérios como a minimização da impureza. Diferentemente do Bagging, que tende a gerar árvores de decisão semelhantes e, portanto, previsões altamente correlacionadas, o Random Forest visa minimizar esse problema ao proporcionar oportunidades para que outros preditores sejam considerados. Assim, em média, $\left(p - m\right)/p$ das divisões nem sequer considerarão o preditor mais forte, permitindo que outros preditores também tenham a chance de serem usados [@james2013introduction]. Esse processo de redução da correlação entre as árvores resulta em uma média das árvores menos variável e, consequentemente, mais confiável.

\vspace{12pt}

\ \ \ A quantidade de variáveis independentes $m$ selecionadas aleatoriamente é um hiperparâmetro que pode ser estimado por meio de validação cruzada. Valores comuns para $m$ são $m=\sqrt{p}$​ com tamanho mínimo do nó igual a um para classificação, e $m=p/3$​ com tamanho mínimo do nó igual a cinco para regressão [@hastie2009elements]. Quando o número de variáveis é grande, mas poucas são realmente relevantes, o algoritmo Random Forest pode ter um desempenho inferior com valores pequenos de $m$, pois isso reduz as chances de selecionar as variáveis mais importantes. No entanto, usar um valor pequeno de $m$ pode ser vantajoso quando há muitos preditores correlacionados. Além disso, assim como no Bagging, a Random Forest não sofre de sobreajuste com o aumento da quantidade de árvores $B$. Portanto, é suficiente usar um $B$ grande o bastante para que a taxa de erro se estabilize [@james2013introduction].

### Boosting Trees

\ \ \ O Boosting, assim como o Bagging, é um método destinado a melhorar o desempenho de modelos ou algoritmos. No entanto, neste trabalho, o Boosting foi aplicado apenas às árvores de regressão. Portanto, a explicação do Boosting será restrito ao caso de Boosting Trees (@algo-boos).

\vspace{12pt}

::: {#algo-boos}

```pseudocode
#| pdf-line-number: false

\begin{algorithm}
\caption{Método Boosting aplicado a árvores de regressão}
\begin{algorithmic}
\State \hspace{1em} \textbf{1.} Defina $\hat{f}\left(x\right) = 0 \text{ e } r_i = y_i$ para todos os $i$ no conjunto de treinamento

\vspace{0.8em}

\State \hspace{1em} \vspace{0.8em} \textbf{2.} Para $b = 1, 2, \dots, B$, repita:

  \State \hspace{2em} (a) Ajuste uma árvore $\hat{f}^b$ com $d$ divisões para os dados de
  \State \hspace{2em} \vspace{0.1em} treinamento $\left(X, r\right)$.

  \State \hspace{2em} (b) Atualize $\hat{f}$ adicionando uma versão com o hiperparâmetro $\lambda$ de taxa de
  \State \hspace{2em} aprendizado:

$$
\hat{f}\left(x\right) \gets \hat{f}\left(x\right) + \lambda \hat{f}^b\left(x\right)
$$

  \vspace{0.1em}

  \State \hspace{2em} (c) Atualize os resíduos,

$$
r_i \gets r_i - \lambda \hat{f}^b\left(x_{i}\right)
$$

\vspace{1em}

\State \hspace{1em} \textbf{3.} Retorne o modelo de boosting,

$$
\hat{f}\left(x\right) = \sum_{b = 1}^B \lambda \hat{f}^b\left(x\right)
$$


\end{algorithmic}
\end{algorithm}
```

Fonte: @james2013introduction [p. 349].

:::

\ \ \ No algoritmo de Bagging, cada árvore é construída e ajustada utilizando amostras bootstrap, e ao final, um estimador agregado $\varphi_B$​ é formado a partir das $B$ árvores. O Boosting Trees funciona de maneira semelhante, mas sem o uso de amostras bootstrap. A ideia principal é corrigir os erros das árvores anteriores, ajustando as novas árvores aos resíduos das anteriores, visando melhorar suas previsões. Assim, as árvores são construídas de forma sequencial, incorporando as informações das árvores anteriores.

\vspace{12pt}

\ \ \ No caso da regressão, o Boosting combina um grande número de árvores de decisão $\hat{f}^1, \dots, \hat{f}^B$. A primeira árvore é construída utilizando o conjunto de dados original, e seus resíduos são calculados. Com a primeira árvore ajustada, a segunda árvore é ajustada aos da árvore anterior resíduos e, em seguida, é adicionada ao estimador para atualizar os resíduos. Dessa forma, os resíduos servem como informação crucial para construir novas árvores e corrigir os erros das árvores anteriores. Como cada nova árvore depende das árvores já construídas, árvores menores são suficientes [@james2013introduction].

\vspace{12pt}

\ \ \ O processo de aprendizado no método de Boosting é lenta, o que acaba gerando melhores resultados. Esse processo de aprendizado pode ser controlado por um hiperparâmetro $\lambda$ chamado de shrinkage, ou taxa de aprendizado, permitindo que mais árvores, com formas diferentes, corrijam os erros das árvores passadas. No entanto, um valor muito pequeno para $\lambda$ requer uma quantidade muito maior $B$ de árvores e, diferente do Bagging e Random Forest, o Boosting pode sofrer de sobreajuste se a quantidade de árvores é muito grande. Além disso, a quantidade de divisões $d$ em cada árvore, que controla a complexidade do boosting, pode ser considerado também um hiperparâmetro. Para $d = 1$ é ajustado um modelo aditivo, já que cada termo involve apenas uma variável. @james2013introduction define $d$ como a profundidade de interação que controla a ondem de interação do modelo boosting, já que $d$ divisões podem envolver no máximo $d$ variáveis.


### Stacked generalization

\ \ \ A Stacked Generalization, ou Stacking, é um método de ensemble que consiste em treinar um modelo gerado a partir da combinação da predição de vários outros modelos, visando melhorar a precisão das predições. Esse método pode ser aplicado a qualquer modelo estatístico ou algoritmo de aprendizado de máquina. A ideia principal é atribuir pesos às predições, de modo a dar maior importância aos modelos que produzem melhores resultados, ao mesmo tempo em que se evita atribuir altos pesos a modelos com alta complexidade.

\vspace{12pt}

\ \ \ Matematicamente, o Stacking define predições $\hat{f}_m^{-i}\left(x\right)$ em x, utilizando o modelo $m$, aplicado ao conjunto de treinamento com a $i\text{-ésima}$ observação removida [@hastie2009elements]. Assim, os peso são estimados de forma a minimizar o erro de predição combinado, dado pela seguinte expressão:

$$
\hat{w}^{st} = \arg \min_{w} \sum^{N}_{i = 1} \left[y_i - \sum^{M}_{m = 1} w_m f^{-i}_m\left(x_i\right)\right]^2
$$
A previsão final dos modelos empilhados é $\sum_{m} \hat{w}_m^{st} \hat{f}_m\left(x\right)$. Assim, em vez de escolher um único modelo, o método de Stacking combina os modelos utilizando pesos estimados, o que melhora a performance preditiva, mas pode comprometer a interpretabilidade.

### Gradient Boosting

\ \ \ O algoritmo de Gradient Boosting é semelhante ao de Boosting, mas com diferenças mínimas. Ele constrói modelos aditivos ajustando sequencialmente funções bases aos pseudos-resíduos, que correspondem aos gradientes da função perda do modelo atual [@friedman2002stochastic]. Esses gradientes indicam a direção na qual a função perda diminui. Neste trabalho, foram utilizadas diferentes implementações de Gradient Boosting. No entanto, todas empregam o Gradient Boosting com árvores de regressão, com algumas modificações para a construção das árvores ou para melhorar a eficiência do algoritmo existente. Assim, o algoritmo a ser explicado será o Gradient Tree Boosting (@algo-gradboos).

::: {#algo-gradboos}

```pseudocode
#| pdf-line-number: false

\begin{algorithm}
\caption{Gradient Tree Boosting}
\begin{algorithmic}
\State \hspace{1em} \textbf{1.} Inicialize $f_0\left(x\right) = \arg \min_{\gamma} \sum_{i = 1}^N L\left(y_i, \gamma \right)$

\vspace{0.8em}

\State \hspace{1em} \vspace{0.8em} \textbf{2.} Para $m = 1$ até $M$:

  \State \hspace{2em} (a) Para $i = 1, 2, \dots, N$, calcule

$$
{r}_{im} = -\left[\frac{\partial L\left(y_i, f\left(x_i \right)\right)}{\partial f\left(x_i\right)}\right]_{f = f_{m - 1}}
$$

  \State \hspace{2em} (b) Ajuste uma árvore de regressão aos pseudo-resíduos $r_{im}$, obtendo regiões
  \State \hspace{2em} terminais $R_{jm}, \ j = 1, 2, \dots, J$.


  \State \vspace{0.1em}

  \State \hspace{2em} (c) Para $j = 1, 2, \dots, J_m$, calcule

$$
\gamma_{jm} = \arg \min_{\gamma} \sum_{x_i \in R_{jm}} L\left(y_i, f_{m - 1}\left(x_i\right) + \gamma\right)
$$

  \vspace{0.1em}

  \State \hspace{2em} (d) Atualize $f_m\left(x\right) = f_{m - 1}\left(x\right) + \lambda \sum^{J}_{j = 1} \gamma_{jm} I\left(x \in R_{jm}\right)$

\vspace{1em}

\State \hspace{1em} \textbf{3.} Retorne $\hat{f}\left(x\right) = f_M\left(x\right)$

\end{algorithmic}
\end{algorithm}
```

Fonte: @hastie2009elements [p. 361].
:::

\vspace{12pt}

O Gradient Boosting aplicado para árvores de regressão, tem que cada função base é uma árvore de regressão com $J_m$ folhas. Dessa forma, cada árvore de regressão tem a forma aditiva

$$
h_m\left(x;\{b_j, R_j\}^J_{1}\right) = \sum^{J_m}_{j = 1} b_{jm} I\left(x \in R_{jm}\right)
$${#eq-treebost}
em que $\{R_{jm}\}^{J_m}_{1}$ são as regiões disjuntas que, coletivamente, cobrem o espaço de todos os valores conjuntos das variáveis preditoras $\mathbf{x}$. Essas regiões são representadas pelas folhas de sua correspondente árvore. Como as regiões são disjuntas, @eq-treebost se reduz simplesmente a $h_m\left(x\right) = b_{jm}\text{ para } x \in  R_{jm}$. Por mínimos quadrados, $b_{jm}$ é simplesmente a média dos pseudo-resíduos $r_{im}$,


$$
\hat{b}_{jm} = \frac{1}{N_{jm}} \sum_{x_i \in R_{jm}} r_{im}
$$
que dão a direção de diminuição da função perda $L$ pela expressão do gradiente da linha 2(a). Assim, cada árvore de regressão é ajustada aos $r_{im}$ de forma a minimizar o erro das árvores anteriores. $N_{jm}$ denota a quantidade de pontos na região $R_{jm}$. Por fim, o estimador é separadamente atualizado em cada região correspondente e é expresso

$$
f_m\left(x\right) = f_{m - 1}\left(x\right) + \lambda \sum^{J}_{j = 1} \gamma_{jm} I\left(x \in R_{jm}\right)
$$
em que $\gamma_{jm}$ representa a atualização da constante ótima para cada região, baseado na função perda $L$, dada a aproximação $f_{m-1}\left(x\right)$. O $0 < \lambda \leq 1$, assim como no algoritmo de boosting, representa o hiperparâmetro shrinkage para controlar a taxa de aprendizado. Pequenos valores de $\lambda$ necessitam maiores quantidades de iterações $M$ para diminuir o risco de treinamento.

### Diferentes implementações de Gradient Boosting

#### Light Gradient Boosting

#### Extreme Gradient Boosting



\vspace{12pt}

# Metodologia

## Obtenção dos dados

\ \ \ Os dados foram obtidos por meio de web scraping, uma técnica automatizada de extração de informações de páginas web. Para isso, foram utilizadas as linguagens de programação R e Python. No R, os pacotes xml2 [@xml2] e rvest [@rvest] foram utilizados para extrair dados de páginas estáticas de forma estruturada. No Python, as bibliotecas Scrapy [@kouzis2016learning] e Playwright, desenvolvida pela Microsoft, foram empregadas, sendo esta última essencial para a interação com páginas dinâmicas, possibilitando a extração de informações que exigem interações como cliques ou rolagem de página. Além dessas ferramentas, foram implementadas técnicas de rotacionamento de IPs e de modificação das informações do usuário que acessa o site, a fim de evitar bloqueios durante o processo de coleta de dados, garantindo assim a continuidade e eficácia da extração. A ferramenta utilizada para a rotação das informações do usuário que acessa o site é a API criada pela empresa ScrapeOps.

\vspace{12pt}

\ \ \ Assim, utilizando as ferramentas e técnicas de web scraping, foram coletadas as variáveis que faziam sentido para a modelagem, priorizando aquelas com menor probabilidade de gerar problemas durante o tratamento dos dados. Ao todo, foram extraídas 25 variáveis, das quais 10 são quantitativas e 15 qualitativas nominais, sendo 13 de caráter dicotômico. No entanto, nem todas as variáveis foram obtidas diretamente por web scraping. As coordenadas de latitude e longitude, por exemplo, foram geradas por meio da geocodificação dos endereços, utilizando o pacote tidygeocoder [@geocoder] da linguagem R. Dessa forma, tem-se as seguintes variáveis:

- Valor do imóvel: variável dependente que será modelada e constitui o principal foco de análise deste trabalho;

- Valor médio do aluguel no bairro: valor médio do aluguel dos imóveis no bairro, em $m^3$;

- Área: área total do imóvel, medida em $m^2$;

- Área média do aluguel no bairro: área média dos imóveis alugados no bairro, em $m^2$;

- Condomínio: valor mensal pago pelo condomínio do imóvel;

- IPTU: imposto cobrado sobre imóveis urbanos;

- Banheiros: quantidade de banheiros disponíveis na propriedade;

- Vagas de estacionamento: número total de vagas de estacionamento disponíveis;

- Quartos: quantidade de quartos no imóvel;

- Latitude: posição horizontal, medida em frações decimais de graus;

- Longitude: posição vertical, também medida em frações decimais de graus, assim como a latitude;

- Tipo do imóvel: sete categorias foram consideradas: apartamentos, casas, casas comerciais, casas de condomínio, casas de vila, coberturas, e lotes comerciais e de condomínio;

- Endereço: nome do endereço onde o imóvel está localizado;

- Variáveis dicotômicas: indicam a presença (1) ou ausência (0) de determinadas características no imóvel, como área de serviço, academia, elevador, espaço gourmet, piscina, playground, portaria 24 horas, quadra de esportes, salão de festas, sauna, spa e varanda gourmet.

\vspace{12pt}

\ \ \ No entanto, com base nas observações realizadas durante o estudo, nem todas as variáveis coletadas foram utilizadas na modelagem do valor dos imóveis. Algumas foram excluídas devido a uma quantidade excessiva de valores ausentes, enquanto outras se mostraram pouco significativas para explicar o valor do imóvel. Após o processo de coleta e limpeza dos dados, o banco de dados final conta com 31.782 observações.

## Análise exploratória de dados

\ \ \ A análise exploratória de dados é uma das primeiras etapas de qualquer estudo que utiliza a estatística como ferramenta principal, pois permite identificar padrões de comportamento nos dados e descobrir relações entre as variáveis estudadas. Assim, após a coleta e organização dos dados, a primeira etapa deste estudo consistiu em uma análise descritiva. Essa análise possibilitou identificar padrões entre os diferentes tipos de imóveis e como essas características podem influenciar o seu valor. Para evidenciar esses comportamentos, foram criados gráficos e tabelas que permitiram caracterizar as relações entre as variáveis independentes e a variável dependente.

## Construção do modelo

\ \ \ No conjunto de dados extraído, foram avaliados diferentes modelos para a previsão do valor do imóvel. Inicialmente, o valor do imóvel foi explicado por variáveis consideradas relevantes para o estudo, como: valor médio do aluguel no bairro, área, área média do aluguel no bairro, número de banheiros, vagas de estacionamento, número de quartos, latitude, longitude, tipo de imóvel e variáveis dicotômicas obtidas durante o processo de extração. As variáveis relacionadas ao valor do condomínio e IPTU foram excluídas do modelo devido à alta quantidade de valores ausentes.

\vspace{12pt}

\ \ \ Com as variáveis selecionadas, o conjunto de dados foi dividido em treino e teste para avaliar o desempenho dos modelos. A divisão foi realizada de forma estratificada, utilizando a classe StratifiedShuffleSplit, que garante uma amostragem estratificada e aleatória. A estratificação foi baseada na variável "tipo de imóvel", preservando a proporção de amostras para cada categoria. Definiu-se 20% do conjunto de dados para o teste, enquanto os 80% restantes foram reservados para o treinamento.

\vspace{12pt}

\ \ \ Para a aplicação das ferramentas de modelagem, foram utilizadas as bibliotecas **scikit-learn**, **lightgbm** e **xgboost**. As duas últimas foram empregadas especificamente na modelagem, enquanto a primeira também foi utilizada para criar pipelines de pré-processamento de dados, que organizam etapas sequenciais de preparação necessárias para o tratamento adequado dos dados. Assim, os quatro modelos aplicados na previsão do valor do imóvel foram: Random Forest, Gradient Boosting, LightGBM e XGBoost. Por fim, foi implementado o algoritmo de Stacking, combinando os modelos previamente construídos para melhorar a performance preditiva.

### Etapas de pré-processamento

\ \ \ Após toda a organização e limpeza dos dados, foram aplicadas algumas transformações com o objetivo de estabelecer uma relação compreensível das variáveis que mais interferem no valor do imóvel. Além disso, foi aplicado também o tratamento de valores ausentes presentes no conjunto de dados.

\vspace{12pt}

\ \ \ O método utilizado para a imputação de valores ausentes foi o algoritmo k-nearest neighbors (KNN). Esse algoritmo estima os valores ausentes de acordo com a fórmula:

$$
\hat{y} = \frac{1}{k}\sum_{x_i \in N_k\left(x\right)}y_i
$$
onde $N_k(x)$ representa o conjunto de $k$ vizinhos mais próximos de $x$, ou seja, os pontos $x_i$ no conjunto de dados que estão mais próximos de $x$. Essa proximidade é geralmente medida pela distância Euclidiana, que é a métrica padrão utilizada pela classe **KNNImputer** da biblioteca scikit-learn para imputação de valores ausentes. No processo de imputação, foi utilizado um total de 17 vizinhos, definido pelo argumento **n_neighbors** da classe **KNNImputer**.

\vspace{12pt}

\ \ \ A transformação utilizada para estabilizar a variância e aproximar a distribuição dos regressores de uma distribuição normal foi a transformação logarítmica. Para lidar com variáveis categóricas, foi utilizada a classe OneHotEncoder, que aplica a codificação one hot. Essa técnica transforma as categorias das variáveis em variáveis dicotômicas, criando uma nova coluna para cada categoria. Além disso, o OrdinalEncoder foi utilizado para variáveis ordinais, que possuem uma ordem. Dessa forma, o OneHotEncoder foi aplicado à variável de tipo de imóvel, transformando cada categoria de imóvel em variáveis dummy. Já o OrdinalEncoder foi aplicado à variável de tamanho do imóvel, que foi criada posteriormente para classificar os imóveis em diferentes faixas: pequenos, médios e grandes.

\vspace{12pt}

\ \ \ Por fim, as variáveis numéricas foram normalizadas utilizando a classe StandardScaler, que padroniza os dados aplicando a seguinte transformação:

$$
z = \frac{x - \mu}{\sigma}
$$
onde $\mu$ é a média da amostra de treinamento e $\sigma$ é o desvio padrão da amostra de treinamento.


### Validação cruzada

utilizar o livro como referência (no fim colocar quais métricas foram utilizadas para avaliar a performance do modelo)

## Tunagem de hiperparâmetros


<!-- \ \ \ Na aprendizagem de máquina, uma das etapas fundamentais é a tunagem dos hiperparâmetros dos algoritmos de aprendizagem. Essa etapa consiste em encontrar a melhor combinação de hiperparâmetros e, consequentemente, resultando em uma configuração algoritmo que proporciona melhor performance e capacidade de generalização. No entanto, essa configuração não é trivial. -->

\vspace{12pt}

<!-- \ \ \ Os algoritmos de otimização de hiperparâmetros procuram pelo melhor ajuste de hiperparâmetros $\lambda \in \tilde{\Lambda}$ para um algoritmo de aprendizagem $I_{\lambda}$. O espaço de procura $\tilde{\Lambda} \in \Lambda$ contém todas as possíveis configurações de hiperparâmetros consideradas para otimização. Dessa forma, temos que: -->

<!-- $$
\tilde{\Lambda} = \tilde \Lambda_1 \cup \tilde \Lambda_2 \cup ... \cup \tilde \Lambda_l
$${#eq-space}
em que $l$ são todas as possíveis configurações de hiperparâmetros. Além disso, $\tilde \Lambda_i \ \left(i = 1, 2, ..., l\right)$ representam o subconjunto de limites dos domínios do i-ésimo hiperparâmetro $\Lambda_i$, podendo ser contínuo, discreto ou categórico (BISCHL et al., 2023). -->

\vspace{12pt}

A busca pela melhor combinação de hiperparâmetros $\lambda$ é feito de forma iterativa, utilizando métodos de reamostragem para dividir o conjunto de dados entre teste e treinamento. Portanto, para validar e encontrar os hiperparâmetos, o algoritmo de aprendizagem é validado através da divisão do conjunto de dados. Assim, tem-se a seguinte estratégia de separação do conjunto de dados:

$$
\mathcal{J} = \left(\left(J_{treino, 1},\ J_{teste, 1}\right), ..., \left(J_{treino, B},\ J_{test, B}\right)\right)
$${#eq-teste}
onde $J_{treino, i}, \ J_{teste, i}$ representam os vetores de índices $\left(i = 1, \, 2, ..., \ B\right)$ para os conjuntos de treino e teste, respectivamente, e $B$ representa o número total de divisões.

\vspace{12pt}

A motivação para fazer a divisão do conjunto de dados entre treino e teste é ajustar o algoritmo em dados reais e avaliar a sua performance em dados ainda não vistos, processo representado pelo banco de treino e teste, respectivamente. Para fazer a avaliação dessa performance é necessário estimar uma métrica de performance em cada divisão. Assim, para um dado algoritmo $I_{\lambda}$ é calculado uma métrica de performance $\rho$ para cada $J_{teste, i}$ após o ajuste do algoritmo no $J_{treino, i}$. Por fim, pode ser calculado uma média amostral da métrica de cada uma das divisões. Dessa forma, o problema de otimizar os hiperparâmetros pode ser definida da seguinte forma:

$$
\lambda^* \in \underset{\lambda \in \tilde \Lambda}{\operatorname{argmin}} \ c\left(\lambda \right)
$$
onde $\lambda^*$ denota a melhor combinação possível de hiperparâmetros, $c\left(\lambda \right)$ representa a generalização do erro, podendo ser representado também por $\widehat{GE}\left(I, \ \mathcal J, \ \rho, \ \lambda\right)$, quando $I, \ \mathcal{J}, \ \rho$ são fixados e $I$ denota um algoritmo de aprendizagem de máquina. O erro de generalização é estimado e otimizado a fim de evitar um ajuste excessivo (overfitting), podendo prejudicar quando o algoritmo fizer estimativas em dados ainda não vistos. Assim, o processo pode ser visualizado pela figura @fig-hpo:


```{mermaid}
%%| label: fig-hpo
%%| fig-cap: Processo de otimização de hiperparâmetros
%%| fig-width: 4.5
flowchart LR
  A["$$\lambda$$"] --> B(Round edge)
  B --> C{Decision}
```

\vspace{12pt}

### Otimização Bayesiana

\ \ \ Existem diversas técnicas para otimização de hiperparâmetros utilizadas em aprendizagem de máquina. Uma das técnicas mais comuns é o GridSearch. @bischl2023hyperparameter definem GridSearch como um processo que divide o intervalo contínuo de valores possíveis de cada hiperparâmetro em um conjunto de valores específicos e avalia exaustivamente o algoritmo para todas as combinações possíveis. No entanto, como todas as combinações possíveis aumentam exponencialmente com a quantidade necessária para avaliação do algoritmo, o GridSearh tem um custo computacional bastante elevado. Assim, existem algoritmos de otimização mais sofisticados que entregam melhores performances, como a otimização bayesiana, que foi utilizada neste trabalho.

\vspace{12pt}

\ \ \ A otimização bayesiana não se refere a um tipo específico de algoritmo de otimização, mas sim a uma filosofia de otimização baseada em inferência bayesiana, a qual contém uma extensa família de algoritmos de otimização [@garnett2023bayesian]. Não obstante, a otimização bayesiana tem obtido benchmarks melhores que outros algoritmos em inúmeros problemas complexos de otimização de hiperparâmetros [@snoek2012practical].

\vspace{12pt}

\ \ \ Diferente de outros algoritmos de otimização de hiperparâmetros, a otimização bayesiana determina as futuras tentativas de avaliação com base em resultados obtidos previamente [@yang2020hyperparameter]. Para a definição dos pontos futuros, é utilizada uma função probabilística $P\left(\rho |  \lambda \right)$ [@bergstra2013making]. Assim, após o ajuste da função probabilística, tem-se como resultado para cada $\lambda$ uma estimativa da performance $\hat c \left(\lambda \right)$ e da predição da incerteza $\hat \sigma \left(\lambda \right)$, além de obter também a distribuição preditiva da função probabilística. Com a distribuição obtida, uma função de aquisição determina o trade-off entre exploitation e exploration^[exploitation pode ser entendido como procurar próximo a boas observações e ]. Dessa forma, os algoritmos de otimização bayesiana são definidos segundo a lei $\lambda \to c\left(\lambda \right)$ e  procuram um equilíbrio entre o processo de exploitation-exploration para detectar as regiões ótimas mais prováveis e não perder melhores configurações em áreas ainda não exploradas.

### Tree-Structured Parzen Estimator

\ \ \ Existem diversas funções probabilísticas para uso na otimização bayesiana, algumas delas é o Processo Gaussiano, Random Forest ou Tree-Structured Parzen Estimator (TPE). Nesse trabalho, foi utilizado o Tree-Structured Parzen Estimator, utilizando a biblioteca Optuna [@optuna_2019] para sua aplicação.

\vspace{12pt}

\ \ \ O TPE define duas funções, $l\left(x\right) \text{ e } g\left(x\right)$, que são usadas para modelar a distribuição das variáveis do domínio [@yang2020hyperparameter]. Utilizando as duas densidades, o TPE procura modelar a probabilidade de se observar um hiperparâmetro $x$ dado uma métrica de performance $\rho$. Dessa forma, tem-se a seguinte definição:


$$
p(x|y) =
\begin{cases}
    l(x) & \text{if } y < y^* \\
    g(x) & \text{if } y \ge y^*
\end{cases}
$${#eq-tpe}
em que $l\left(x\right)$ é definido como a densidade em que a função perda é menor que um limiar $y^*$ e $g\left(x\right)$ representa é a densidade em que a função perda^[definição de função perda] tem valores acima do limiar $y^*$ [@bergstra2011algorithms]. O limite $y^*$ é escolhido através de um hiperparâmetro $\gamma$, onde $\gamma$ representa o percentil dos valores observados de $y$, de modo que $p\left(y < y^*\right) = \gamma$.

\vspace{12pt}

Por padrão, o tree-structured parzen estimator tem como função de aquisição o Expected Improvement $\left(EI\right)$, que pode ser otimizado para o TPE da seguinte forma:

$$
  EI_{y^*}\left(x\right) = \int_{-\infty}^{y^*} \left(y^* - y\right)p\left(y | x\right) dy
$${#eq-exp}

Ainda, para encontrar a probabilidade marginal de $x$, temos a seguinte integral $p\left(x\right) = \int_{\mathbb{R}} p\left(x | y\right)p\left(y\right)dy$. Particionando o domínio de $y$, chega-se em:

$$
p\left(x\right) = \int_{-\infty}^{y^*} p\left(x | y\right)p\left(y\right)dy + \int_{y^*}^{\infty} p\left(x | y\right)p\left(y\right)dy = \gamma l\left(x\right) + \left(1 - \gamma \right) g\left(x\right)
$$

Assim, utilizando o Teorema de Bayes e fazendo as substituições na integral da @eq-exp:

$$
EI_{y^*}\left(x\right) = \int_{-\infty}^{y^*} \left(y^* - y\right) \frac{p\left(x | y\right)p\left(y\right)}{p\left(x\right)}dy = \left(\gamma y^*  - \int_{-\infty}^{y^*} p\left(y\right)dy\right) \left(\gamma + \left(1 - \gamma\right)\frac{g\left(x\right)}{l\left(x\right)}\right) ^{-1}
$$
em que a segunda expressão do produto mostra que para maximizar o Expected Improvement é necessário pontos de $x$ com maior probabilidade em $l\left(x\right)$ e com baixa probabilidade em $g\left(x\right)$. Não obstante, no TPE, maximizar o $EI$ é equivalente a maximizar a razão entre as duas distribuições $r\left(x\right) = \frac{l\left(x\right)}{g\left(x\right)}$ [@cowen2022hebo].


\newpage

# Resultados

## Análise exploratória de dados

\ \ \ A análise exploratória dos dados foi realizada após a divisão entre os conjuntos de treinamento e teste. Essa abordagem foi adotada para evitar o sobreajuste do modelo e garantir que o algoritmo não aprenda com informações indisponíveis no conjunto de teste. Assim, a descritiva dos dados foi realizada utilizando o conjunto de treinamento.

\vspace{12pt}

\ \ \ A primeira etapa da análise exploratória de dados foi identificar os dados faltantes e determinar a melhor forma de tratá-los. A @fig-miss mostra a porcentagem de observações ausentes em cada variável. As variáveis com a maior quantidade de dados ausentes são o valor do condomínio e o IPTU, pois essas informações são as menos preenchidas no site de onde os dados foram coletados. A terceira variável, com quase 20% de observações ausentes, é a quantidade de vagas de estacionamento. As variáveis com mais de 20% de observações ausentes foram removidas da base de dados, pois, com essa quantidade de valores faltantes, nem mesmo métodos de imputação proporcionariam um tratamento adequado. Dessa forma, apenas as variáveis de valor do condomínio e IPTU foram removidas, enquanto as demais com valores ausentes foram tratadas por meio de imputação.


```{python}
#| label: fig-miss
#| fig-cap: "Quantidade de valores ausentes por variáveis"

g_missing = sns.displot(
    data=train_df.isnull() \
        .melt(value_name="Valores ausentes") \
        .replace([False, True], ["Não é ausente", "Ausente"]) \
        .groupby(["variable", "Valores ausentes"]).size() \
        .reset_index(name="count") \
        .assign(
            proportion=lambda x: x.groupby("variable")["count"].transform(lambda y: y / y.sum())
        ),
    y="variable",
    hue="Valores ausentes",
    weights="proportion",
    multiple="fill",
    height=8,
    aspect=1.1,
    palette='crest'
    #palette={"Não é ausente": "#FAD02C", "Ausente": "black"}
    )

sns.move_legend(obj=g_missing, loc="upper center",
                bbox_to_anchor=(.5, -.0001), ncol=2, title="")
g_missing.set(xlabel="Proporção de valores ausentes (%)", ylabel="")
```

\vspace{12pt}

\ \ \ Uma das dificuldades que podem surgir durante a modelagem é o desbalanceamento das classes, ou seja, a diferença na quantidade de cada tipo de imóvel. O tipo de imóvel mais predominante no conjunto de dados são os apartamentos, que representam 81,36% do total. Em seguida, vêm as casas, com 8,91%, e os flats, com 5,72%. Por fim, as casas comerciais são as menos representadas, com apenas 15 ocorrências. Esse desbalanceamento claro entre as classes pode dificultar o desempenho do modelo, especialmente na previsão de categorias menos frequentes, como as casas comerciais, onde o modelo pode ter dificuldade em obter bons resultados.

\vspace{12pt}

\ \ \ A distribuição das variáveis foram análisadas em termos do tipo do imóvel a partir de um gráfico de violino. Pela @fig-violin, é fácil perceber que a maioria das distribuições possuem assimetria negativa. Os apartamentos possuem caudas longas à direita, indicando presença de valores extremamente altos e que indicam que talvez seja necessário a aplicação de alguma transformação para a estabilização da variância.

```{python}
#| label: fig-violin
#| fig-cap: Distribuição das variáveis numéricas.

fig, axes = plt.subplots(2, 3, figsize=(20, 10))

dists_var = train_df[['Valor', 'Área', 'Área de aluguel',
                      'Valor de aluguel', 'Vaga', 'Quarto',
                      'Banheiro', 'Tipo']]\
                      .set_index('Tipo')\
                      .stack()\
                      .reset_index()\
                      .rename(
                        columns={
                          'level_1': 'Variável',
                           0: 'Valor'
                          }
                      )

sns.violinplot(
  data=train_df,
  x='Vaga',
  hue='Tipo',
  ax=axes[0][0],
  palette='crest'
).legend_.remove()

sns.violinplot(
  data=train_df,
  x='Banheiro',
  hue='Tipo',
  ax=axes[0][1],
  palette='crest'
).legend_.remove()

sns.violinplot(
  data=train_df,
  x='Quarto',
  hue='Tipo',
  ax=axes[0][2],
  palette='crest'
).legend_.remove()

sns.violinplot(
  data=train_df,
  x='Área',
  hue='Tipo',
  ax=axes[1][0],
  palette='crest'
).legend_.remove()

sns.violinplot(
  data=train_df,
  x='Valor de aluguel',
  hue='Tipo',
  ax=axes[1][1],
  palette='crest'
).legend_.remove()

sns.violinplot(
  data=train_df,
  x='Área de aluguel',
  hue='Tipo',
  ax=axes[1][2],
  palette='crest'
).legend_.remove()

l = plt.legend(loc="upper center", bbox_to_anchor=(-.66, -.17), ncol=7)
l.get_texts()[0].set_text('Apartamentos')
l.get_texts()[1].set_text('Casas de condomínio')
l.get_texts()[2].set_text('Casas')
l.get_texts()[3].set_text('Flats')
l.get_texts()[4].set_text('Terrenos, lotes e condomínios')
l.get_texts()[5].set_text('Terrenos e lotes comerciais')
# l.get_texts()[6].set_text('Casas comerciais')
plt.show()
```

\vspace{12pt}

\ \ \ Para reduzir a assimetria da distribuição dos valores dos imóveis, foi aplicada uma transformação logarítmica. O gráfico de densidade à esquerda na @fig-densitarg mostra os dados originais da distribuição do valor dos imóveis. Há uma tendência dos valores ficarem mais concentrados em uma faixa mais baixa, mas alguns imóveis apresentam valores excepcionalmente altos, o que acaba gerando uma distribuição assimétrica positiva. Com a aplicação da transformação logarítmica, a assimetria é suavizada, comprimindo os valores mais altos. Isso tende a aproximar a distribuição de uma forma mais simétrica, facilitando a modelagem e análise estatística.

```{python}
#| label: fig-densitarg
#| fig-cap: Comparação entre distribuição dos valores dos imóveis antes e depois da transformação logarítmica.

fig, axes = plt.subplots(1, 2, figsize=(20, 10))

sns.kdeplot(
  data=train_df,
  x='Valor',
  ax=axes[0],
  fill=True,
  alpha=.5,
  palette='crest'
  )

dens_plot = sns.kdeplot(
  data=train_df.assign(Valor=lambda x: np.log1p(x.Valor)),
  x='Valor',
  ax=axes[1],
  fill=True,
  alpha=.5,
  palette='crest'
  )

axes[0].set_ylabel(ylabel='Densidade')
axes[1].set_ylabel(ylabel='Densidade')
plt.show()
```

\vspace{12pt}

\ \ \ A @fig-corplot apresenta a matriz de correlação entre as variáveis numéricas do conjunto de dados. As cores mais escuras indicam uma correlação mais forte entre as variáveis, enquanto as cores mais claras indicam o contrário. O valor do imóvel apresenta maior correlação com as variáveis de área do imóvel e número de vagas de estacionamento. Além disso, o valor do imóvel tem alta correlação com o valor médio do aluguel, número de quartos e banheiros, além de ser fortemente influenciado pela localização das propriedades. Algumas variáveis apresentam multicolinearidade entre si, mas os algoritmos utilizados selecionam aleatoriamente as variáveis para a modelagem, o que reduz o risco de selecionar variáveis redundantes.

```{python}
#| label: fig-corplot
#| fig-cap: "Gráfico de correlação de Spearman das variáveis independentes."

fig = plt.figure(figsize=(20, 10))

mat_plot = train_df[['Valor', 'Área', 'Área de aluguel',
                      'Valor de aluguel', 'Vaga', 'Quarto',
                      'Banheiro', 'Latitude', 'Longitude']]\
            .assign(Valor=lambda x: np.log1p(x.Valor))\
            .corr(method='spearman')

heatmap = sns.heatmap(
  mat_plot,
  cmap='crest',
  annot=True,
  annot_kws={'size': 17}
  )
plt.tick_params(axis='both', which='major', labelsize=17)
heatmap.figure.axes[-1].tick_params(labelsize=16)
plt.xticks([])
plt.show()
```

**ADICIONAR NO FIM UM GRÁFICO DE ESPACIALIZAÇÃO DOS DADOS (INCLUIR VARIAÇÃO DA COR PELO PREÇO DO IMÓVEL)**

## Construção dos modelos

### Random Forest

### Gradient Boosting

# Conclusão

# Referências

::: {#refs}
:::
